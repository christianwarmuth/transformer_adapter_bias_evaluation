{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "adapter_evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqFl7uUVgEz7"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mthG05Ijv6SZ",
        "outputId": "0e95e870-7b9c-485b-ccda-5f3b4f7a53d6"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Aug  9 12:46:15 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   56C    P0    28W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SyR5EK-hsOv",
        "outputId": "c26e903f-6e2e-439b-c41e-c783dffb2971"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive/\", force_remount=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "julO5QKjnRTA",
        "outputId": "6bffffc5-a945-4f52-b883-61c9d05d345b"
      },
      "source": [
        "!pip install -U git+https://github.com/Adapter-Hub/adapter-transformers.git\n",
        "#!pip install transformers==3.0.2\n",
        "!pip3 install -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/Adapter-Hub/adapter-transformers.git\n",
            "  Cloning https://github.com/Adapter-Hub/adapter-transformers.git to /tmp/pip-req-build-itaomm38\n",
            "  Running command git clone -q https://github.com/Adapter-Hub/adapter-transformers.git /tmp/pip-req-build-itaomm38\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==2.1.2) (3.0.12)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==2.1.2) (0.10.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==2.1.2) (2020.2.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==2.1.2) (4.45.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==2.1.2) (3.13)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==2.1.2) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==2.1.2) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==2.1.2) (21.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.14 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==2.1.2) (0.0.15)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==2.1.2) (1.6.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==2.1.2) (0.0.38)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.14->adapter-transformers==2.1.2) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->adapter-transformers==2.1.2) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->adapter-transformers==2.1.2) (3.1.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers==2.1.2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers==2.1.2) (1.25.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers==2.1.2) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers==2.1.2) (2019.11.28)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->adapter-transformers==2.1.2) (7.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->adapter-transformers==2.1.2) (1.14.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->adapter-transformers==2.1.2) (0.14.1)\n",
            "Requirement already satisfied: blis==0.4.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 1)) (0.4.1)\n",
            "Requirement already satisfied: boto3==1.12.36 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 2)) (1.12.36)\n",
            "Requirement already satisfied: botocore==1.15.36 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 3)) (1.15.36)\n",
            "Requirement already satisfied: catalogue==1.0.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 4)) (1.0.0)\n",
            "Requirement already satisfied: certifi==2019.11.28 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 5)) (2019.11.28)\n",
            "Requirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 6)) (3.0.4)\n",
            "Requirement already satisfied: click==7.1.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 7)) (7.1.1)\n",
            "Requirement already satisfied: colorama==0.4.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 8)) (0.4.3)\n",
            "Requirement already satisfied: cymem==2.0.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 9)) (2.0.3)\n",
            "Requirement already satisfied: docutils==0.15.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 10)) (0.15.2)\n",
            "Requirement already satisfied: en-core-web-sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 11)) (2.2.5)\n",
            "Requirement already satisfied: filelock==3.0.12 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 12)) (3.0.12)\n",
            "Requirement already satisfied: idna==2.9 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 13)) (2.9)\n",
            "Requirement already satisfied: importlib-metadata==1.6.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 14)) (1.6.0)\n",
            "Requirement already satisfied: jmespath==0.9.5 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 15)) (0.9.5)\n",
            "Requirement already satisfied: joblib==0.14.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 16)) (0.14.1)\n",
            "Requirement already satisfied: murmurhash==1.0.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 17)) (1.0.2)\n",
            "Requirement already satisfied: plac==1.1.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 18)) (1.1.3)\n",
            "Requirement already satisfied: preshed==3.0.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 19)) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil==2.8.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 20)) (2.8.1)\n",
            "Requirement already satisfied: pytz==2019.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 21)) (2019.3)\n",
            "Requirement already satisfied: regex==2020.2.20 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 22)) (2020.2.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 23)) (2.23.0)\n",
            "Requirement already satisfied: s3transfer==0.3.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 24)) (0.3.3)\n",
            "Requirement already satisfied: sacremoses==0.0.38 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 25)) (0.0.38)\n",
            "Requirement already satisfied: scikit-learn==0.22.2.post1 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 26)) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 27)) (1.4.1)\n",
            "Requirement already satisfied: sentencepiece==0.1.85 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 28)) (0.1.85)\n",
            "Requirement already satisfied: six==1.14.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 29)) (1.14.0)\n",
            "Requirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 30)) (0.0)\n",
            "Requirement already satisfied: spacy==2.2.4 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 31)) (2.2.4)\n",
            "Requirement already satisfied: srsly==1.0.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 32)) (1.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 33)) (7.4.0)\n",
            "Requirement already satisfied: torch==1.6.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 34)) (1.6.0)\n",
            "Requirement already satisfied: tqdm==4.45.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 35)) (4.45.0)\n",
            "Requirement already satisfied: urllib3==1.25.8 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 36)) (1.25.8)\n",
            "Requirement already satisfied: wasabi==0.6.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 37)) (0.6.0)\n",
            "Requirement already satisfied: zipp==3.1.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 38)) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from blis==0.4.1->-r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4->-r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 31)) (57.2.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0->-r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 34)) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbEF5yRZy4an"
      },
      "source": [
        "import sys\n",
        "from collections import defaultdict \n",
        "from collections import defaultdict \n",
        "from transformers import BertForMaskedLM, BertModel, BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "sys.path.append('/content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/')\n",
        "path = \"/content/gdrive/MyDrive/master_hpi/NLP_Project/code/\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8yDqLezqXLT"
      },
      "source": [
        "#adapter_list = [\"sst\"]\n",
        "#for adapter in adapter_list:\n",
        "#  print(\"!python3 evaluation.py --no-cuda False --output-dir /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/ -- skip_intrasentence False --skip_intersentence False --batch_size 1\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njMAGdTygkwL"
      },
      "source": [
        "## Sentiment Bert\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWdxhEjqgaGX"
      },
      "source": [
        "import os\n",
        "try:\n",
        "    os.makedirs('/content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sentiment_bert/')\n",
        "except OSError as e:\n",
        "        pass"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKR_7gYdgtM1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdad61ac-63ce-4528-9e82-c3ae1f8f8a62"
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/evaluation.py --load-path_model /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/sentiment_bert/SentimentBert.pth --input-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --output-dir /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sentiment_bert/ --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sentiment_bert/predictions.json\n",
        "#!python3 evaluation.py --no-cuda False --output-dir /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/ -- skip_intrasentence False --skip_intersentence False --batch_size 1"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json...\n",
            "---------------------------------------------------------------\n",
            "\u001b[96m                     ARGUMENTS                 \u001b[0m\n",
            "\u001b[96mPretrained Model:\u001b[0m /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/sentiment_bert/SentimentBert.pth\n",
            "\u001b[96mSkip Intrasentence:\u001b[0m False\n",
            "\u001b[96mSkip Intersentence:\u001b[0m False\n",
            "\u001b[96mBatch Size:\u001b[0m 1\n",
            "\u001b[96mMax Seq Length:\u001b[0m None\n",
            "\u001b[96mCUDA:\u001b[0m True\n",
            "---------------------------------------------------------------\n",
            "\n",
            "\u001b[94mEvaluating bias on intersentence tasks...\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2160: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
            "  FutureWarning,\n",
            "Maximum sequence length found: -inf\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "odict_keys(['module.bert.embeddings.word_embeddings.weight', 'module.bert.embeddings.position_embeddings.weight', 'module.bert.embeddings.token_type_embeddings.weight', 'module.bert.embeddings.LayerNorm.weight', 'module.bert.embeddings.LayerNorm.bias', 'module.bert.encoder.layer.0.attention.self.query.weight', 'module.bert.encoder.layer.0.attention.self.query.bias', 'module.bert.encoder.layer.0.attention.self.key.weight', 'module.bert.encoder.layer.0.attention.self.key.bias', 'module.bert.encoder.layer.0.attention.self.value.weight', 'module.bert.encoder.layer.0.attention.self.value.bias', 'module.bert.encoder.layer.0.attention.output.dense.weight', 'module.bert.encoder.layer.0.attention.output.dense.bias', 'module.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'module.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'module.bert.encoder.layer.0.intermediate.dense.weight', 'module.bert.encoder.layer.0.intermediate.dense.bias', 'module.bert.encoder.layer.0.output.dense.weight', 'module.bert.encoder.layer.0.output.dense.bias', 'module.bert.encoder.layer.0.output.LayerNorm.weight', 'module.bert.encoder.layer.0.output.LayerNorm.bias', 'module.bert.encoder.layer.1.attention.self.query.weight', 'module.bert.encoder.layer.1.attention.self.query.bias', 'module.bert.encoder.layer.1.attention.self.key.weight', 'module.bert.encoder.layer.1.attention.self.key.bias', 'module.bert.encoder.layer.1.attention.self.value.weight', 'module.bert.encoder.layer.1.attention.self.value.bias', 'module.bert.encoder.layer.1.attention.output.dense.weight', 'module.bert.encoder.layer.1.attention.output.dense.bias', 'module.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'module.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'module.bert.encoder.layer.1.intermediate.dense.weight', 'module.bert.encoder.layer.1.intermediate.dense.bias', 'module.bert.encoder.layer.1.output.dense.weight', 'module.bert.encoder.layer.1.output.dense.bias', 'module.bert.encoder.layer.1.output.LayerNorm.weight', 'module.bert.encoder.layer.1.output.LayerNorm.bias', 'module.bert.encoder.layer.2.attention.self.query.weight', 'module.bert.encoder.layer.2.attention.self.query.bias', 'module.bert.encoder.layer.2.attention.self.key.weight', 'module.bert.encoder.layer.2.attention.self.key.bias', 'module.bert.encoder.layer.2.attention.self.value.weight', 'module.bert.encoder.layer.2.attention.self.value.bias', 'module.bert.encoder.layer.2.attention.output.dense.weight', 'module.bert.encoder.layer.2.attention.output.dense.bias', 'module.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'module.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'module.bert.encoder.layer.2.intermediate.dense.weight', 'module.bert.encoder.layer.2.intermediate.dense.bias', 'module.bert.encoder.layer.2.output.dense.weight', 'module.bert.encoder.layer.2.output.dense.bias', 'module.bert.encoder.layer.2.output.LayerNorm.weight', 'module.bert.encoder.layer.2.output.LayerNorm.bias', 'module.bert.encoder.layer.3.attention.self.query.weight', 'module.bert.encoder.layer.3.attention.self.query.bias', 'module.bert.encoder.layer.3.attention.self.key.weight', 'module.bert.encoder.layer.3.attention.self.key.bias', 'module.bert.encoder.layer.3.attention.self.value.weight', 'module.bert.encoder.layer.3.attention.self.value.bias', 'module.bert.encoder.layer.3.attention.output.dense.weight', 'module.bert.encoder.layer.3.attention.output.dense.bias', 'module.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'module.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'module.bert.encoder.layer.3.intermediate.dense.weight', 'module.bert.encoder.layer.3.intermediate.dense.bias', 'module.bert.encoder.layer.3.output.dense.weight', 'module.bert.encoder.layer.3.output.dense.bias', 'module.bert.encoder.layer.3.output.LayerNorm.weight', 'module.bert.encoder.layer.3.output.LayerNorm.bias', 'module.bert.encoder.layer.4.attention.self.query.weight', 'module.bert.encoder.layer.4.attention.self.query.bias', 'module.bert.encoder.layer.4.attention.self.key.weight', 'module.bert.encoder.layer.4.attention.self.key.bias', 'module.bert.encoder.layer.4.attention.self.value.weight', 'module.bert.encoder.layer.4.attention.self.value.bias', 'module.bert.encoder.layer.4.attention.output.dense.weight', 'module.bert.encoder.layer.4.attention.output.dense.bias', 'module.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'module.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'module.bert.encoder.layer.4.intermediate.dense.weight', 'module.bert.encoder.layer.4.intermediate.dense.bias', 'module.bert.encoder.layer.4.output.dense.weight', 'module.bert.encoder.layer.4.output.dense.bias', 'module.bert.encoder.layer.4.output.LayerNorm.weight', 'module.bert.encoder.layer.4.output.LayerNorm.bias', 'module.bert.encoder.layer.5.attention.self.query.weight', 'module.bert.encoder.layer.5.attention.self.query.bias', 'module.bert.encoder.layer.5.attention.self.key.weight', 'module.bert.encoder.layer.5.attention.self.key.bias', 'module.bert.encoder.layer.5.attention.self.value.weight', 'module.bert.encoder.layer.5.attention.self.value.bias', 'module.bert.encoder.layer.5.attention.output.dense.weight', 'module.bert.encoder.layer.5.attention.output.dense.bias', 'module.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'module.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'module.bert.encoder.layer.5.intermediate.dense.weight', 'module.bert.encoder.layer.5.intermediate.dense.bias', 'module.bert.encoder.layer.5.output.dense.weight', 'module.bert.encoder.layer.5.output.dense.bias', 'module.bert.encoder.layer.5.output.LayerNorm.weight', 'module.bert.encoder.layer.5.output.LayerNorm.bias', 'module.bert.encoder.layer.6.attention.self.query.weight', 'module.bert.encoder.layer.6.attention.self.query.bias', 'module.bert.encoder.layer.6.attention.self.key.weight', 'module.bert.encoder.layer.6.attention.self.key.bias', 'module.bert.encoder.layer.6.attention.self.value.weight', 'module.bert.encoder.layer.6.attention.self.value.bias', 'module.bert.encoder.layer.6.attention.output.dense.weight', 'module.bert.encoder.layer.6.attention.output.dense.bias', 'module.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'module.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'module.bert.encoder.layer.6.intermediate.dense.weight', 'module.bert.encoder.layer.6.intermediate.dense.bias', 'module.bert.encoder.layer.6.output.dense.weight', 'module.bert.encoder.layer.6.output.dense.bias', 'module.bert.encoder.layer.6.output.LayerNorm.weight', 'module.bert.encoder.layer.6.output.LayerNorm.bias', 'module.bert.encoder.layer.7.attention.self.query.weight', 'module.bert.encoder.layer.7.attention.self.query.bias', 'module.bert.encoder.layer.7.attention.self.key.weight', 'module.bert.encoder.layer.7.attention.self.key.bias', 'module.bert.encoder.layer.7.attention.self.value.weight', 'module.bert.encoder.layer.7.attention.self.value.bias', 'module.bert.encoder.layer.7.attention.output.dense.weight', 'module.bert.encoder.layer.7.attention.output.dense.bias', 'module.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'module.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'module.bert.encoder.layer.7.intermediate.dense.weight', 'module.bert.encoder.layer.7.intermediate.dense.bias', 'module.bert.encoder.layer.7.output.dense.weight', 'module.bert.encoder.layer.7.output.dense.bias', 'module.bert.encoder.layer.7.output.LayerNorm.weight', 'module.bert.encoder.layer.7.output.LayerNorm.bias', 'module.bert.encoder.layer.8.attention.self.query.weight', 'module.bert.encoder.layer.8.attention.self.query.bias', 'module.bert.encoder.layer.8.attention.self.key.weight', 'module.bert.encoder.layer.8.attention.self.key.bias', 'module.bert.encoder.layer.8.attention.self.value.weight', 'module.bert.encoder.layer.8.attention.self.value.bias', 'module.bert.encoder.layer.8.attention.output.dense.weight', 'module.bert.encoder.layer.8.attention.output.dense.bias', 'module.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'module.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'module.bert.encoder.layer.8.intermediate.dense.weight', 'module.bert.encoder.layer.8.intermediate.dense.bias', 'module.bert.encoder.layer.8.output.dense.weight', 'module.bert.encoder.layer.8.output.dense.bias', 'module.bert.encoder.layer.8.output.LayerNorm.weight', 'module.bert.encoder.layer.8.output.LayerNorm.bias', 'module.bert.encoder.layer.9.attention.self.query.weight', 'module.bert.encoder.layer.9.attention.self.query.bias', 'module.bert.encoder.layer.9.attention.self.key.weight', 'module.bert.encoder.layer.9.attention.self.key.bias', 'module.bert.encoder.layer.9.attention.self.value.weight', 'module.bert.encoder.layer.9.attention.self.value.bias', 'module.bert.encoder.layer.9.attention.output.dense.weight', 'module.bert.encoder.layer.9.attention.output.dense.bias', 'module.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'module.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'module.bert.encoder.layer.9.intermediate.dense.weight', 'module.bert.encoder.layer.9.intermediate.dense.bias', 'module.bert.encoder.layer.9.output.dense.weight', 'module.bert.encoder.layer.9.output.dense.bias', 'module.bert.encoder.layer.9.output.LayerNorm.weight', 'module.bert.encoder.layer.9.output.LayerNorm.bias', 'module.bert.encoder.layer.10.attention.self.query.weight', 'module.bert.encoder.layer.10.attention.self.query.bias', 'module.bert.encoder.layer.10.attention.self.key.weight', 'module.bert.encoder.layer.10.attention.self.key.bias', 'module.bert.encoder.layer.10.attention.self.value.weight', 'module.bert.encoder.layer.10.attention.self.value.bias', 'module.bert.encoder.layer.10.attention.output.dense.weight', 'module.bert.encoder.layer.10.attention.output.dense.bias', 'module.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'module.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'module.bert.encoder.layer.10.intermediate.dense.weight', 'module.bert.encoder.layer.10.intermediate.dense.bias', 'module.bert.encoder.layer.10.output.dense.weight', 'module.bert.encoder.layer.10.output.dense.bias', 'module.bert.encoder.layer.10.output.LayerNorm.weight', 'module.bert.encoder.layer.10.output.LayerNorm.bias', 'module.bert.encoder.layer.11.attention.self.query.weight', 'module.bert.encoder.layer.11.attention.self.query.bias', 'module.bert.encoder.layer.11.attention.self.key.weight', 'module.bert.encoder.layer.11.attention.self.key.bias', 'module.bert.encoder.layer.11.attention.self.value.weight', 'module.bert.encoder.layer.11.attention.self.value.bias', 'module.bert.encoder.layer.11.attention.output.dense.weight', 'module.bert.encoder.layer.11.attention.output.dense.bias', 'module.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'module.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'module.bert.encoder.layer.11.intermediate.dense.weight', 'module.bert.encoder.layer.11.intermediate.dense.bias', 'module.bert.encoder.layer.11.output.dense.weight', 'module.bert.encoder.layer.11.output.dense.bias', 'module.bert.encoder.layer.11.output.LayerNorm.weight', 'module.bert.encoder.layer.11.output.LayerNorm.bias', 'module.bert.pooler.dense.weight', 'module.bert.pooler.dense.bias', 'module.classifier.weight', 'module.classifier.bias'])\n",
            "100% 6369/6369 [01:42<00:00, 61.99it/s]\n",
            "\n",
            "\u001b[91mEvaluating bias on intrasentence tasks...\u001b[0m\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "  0% 0/6318 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "100% 6318/6318 [01:45<00:00, 59.81it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeAEkHH3hQYk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19170bde-c282-4fd3-e2c6-259087df3e2a"
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/score_evaluation.py --gold-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --predictions-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sentiment_bert/predictions.json --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sentiment_bert/summary.json"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "intrasentence\n",
            "\tgender\n",
            "\t\tCount: 765.0\n",
            "\t\tLM Score: 46.77033240511501\n",
            "\t\tSS Score: 48.865221252177776\n",
            "\t\tICAT Score: 45.7088528202769\n",
            "\tprofession\n",
            "\t\tCount: 2430.0\n",
            "\t\tLM Score: 54.00482247355584\n",
            "\t\tSS Score: 49.334996261109005\n",
            "\t\tICAT Score: 53.28655429629466\n",
            "\trace\n",
            "\t\tCount: 2886.0\n",
            "\t\tLM Score: 54.46395868877134\n",
            "\t\tSS Score: 38.71398348752011\n",
            "\t\tICAT Score: 42.17033594684142\n",
            "\treligion\n",
            "\t\tCount: 237.0\n",
            "\t\tLM Score: 47.81609195402299\n",
            "\t\tSS Score: 37.37931034482759\n",
            "\t\tICAT Score: 35.74665081252477\n",
            "\toverall\n",
            "\t\tCount: 2106.0\n",
            "\t\tLM Score: 53.06327578374257\n",
            "\t\tSS Score: 43.98156249291462\n",
            "\t\tICAT Score: 46.67611559922874\n",
            "intersentence\n",
            "\tgender\n",
            "\t\tCount: 726.0\n",
            "\t\tLM Score: 89.85632222045265\n",
            "\t\tSS Score: 61.56639857726814\n",
            "\t\tICAT Score: 69.07004147066883\n",
            "\tprofession\n",
            "\t\tCount: 2481.0\n",
            "\t\tLM Score: 86.11345278610756\n",
            "\t\tSS Score: 59.76348576305937\n",
            "\t\tICAT Score: 69.29810338040663\n",
            "\trace\n",
            "\t\tCount: 2928.0\n",
            "\t\tLM Score: 87.11458859384024\n",
            "\t\tSS Score: 57.47634577825531\n",
            "\t\tICAT Score: 74.08861286068013\n",
            "\treligion\n",
            "\t\tCount: 234.0\n",
            "\t\tLM Score: 91.31417624521073\n",
            "\t\tSS Score: 55.05747126436781\n",
            "\t\tICAT Score: 82.0777997974193\n",
            "\toverall\n",
            "\t\tCount: 2123.0\n",
            "\t\tLM Score: 87.24094334052701\n",
            "\t\tSS Score: 58.770752157908326\n",
            "\t\tICAT Score: 71.93756949928931\n",
            "overall\n",
            "\tCount: 4229.0\n",
            "\tLM Score: 70.20149045145398\n",
            "\tSS Score: 51.45293436675148\n",
            "\tICAT Score: 68.16152728997211\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2LRBGYNxgAZ"
      },
      "source": [
        "# Adapter Fusion\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5DQkbAYxjrx"
      },
      "source": [
        "import os\n",
        "try:\n",
        "    os.makedirs('/content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/fusion-sst-imdb/')\n",
        "except OSError as e:\n",
        "        pass"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DudWrrkJxpou",
        "outputId": "da299a60-5350-4609-86d6-25b55ccdb381",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/evaluation.py --load-path_model /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/fusion2/pytorch_model_adapter_fusion.bin --load-path_adapter /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/fusion2/pytorch_model_adapter_fusion.bin --input-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --output-dir /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/fusion-sst-imdb/ --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/fusion-sst-imdb/predictions.json\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json...\n",
            "---------------------------------------------------------------\n",
            "\u001b[96m                     ARGUMENTS                 \u001b[0m\n",
            "\u001b[96mPretrained Model:\u001b[0m /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/fusion2/pytorch_model_adapter_fusion.bin\n",
            "\u001b[96mSkip Intrasentence:\u001b[0m False\n",
            "\u001b[96mSkip Intersentence:\u001b[0m False\n",
            "\u001b[96mBatch Size:\u001b[0m 1\n",
            "\u001b[96mMax Seq Length:\u001b[0m None\n",
            "\u001b[96mCUDA:\u001b[0m True\n",
            "---------------------------------------------------------------\n",
            "\n",
            "\u001b[94mEvaluating bias on intersentence tasks...\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2160: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
            "  FutureWarning,\n",
            "Maximum sequence length found: -inf\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "dict_keys(['bert.encoder.layer.0.output.adapter_fusion_layer.sst-2,imdb.query.weight', 'bert.encoder.layer.0.output.adapter_fusion_layer.sst-2,imdb.query.bias', 'bert.encoder.layer.0.output.adapter_fusion_layer.sst-2,imdb.key.weight', 'bert.encoder.layer.0.output.adapter_fusion_layer.sst-2,imdb.key.bias', 'bert.encoder.layer.0.output.adapter_fusion_layer.sst-2,imdb.value.weight', 'bert.encoder.layer.1.output.adapter_fusion_layer.sst-2,imdb.query.weight', 'bert.encoder.layer.1.output.adapter_fusion_layer.sst-2,imdb.query.bias', 'bert.encoder.layer.1.output.adapter_fusion_layer.sst-2,imdb.key.weight', 'bert.encoder.layer.1.output.adapter_fusion_layer.sst-2,imdb.key.bias', 'bert.encoder.layer.1.output.adapter_fusion_layer.sst-2,imdb.value.weight', 'bert.encoder.layer.2.output.adapter_fusion_layer.sst-2,imdb.query.weight', 'bert.encoder.layer.2.output.adapter_fusion_layer.sst-2,imdb.query.bias', 'bert.encoder.layer.2.output.adapter_fusion_layer.sst-2,imdb.key.weight', 'bert.encoder.layer.2.output.adapter_fusion_layer.sst-2,imdb.key.bias', 'bert.encoder.layer.2.output.adapter_fusion_layer.sst-2,imdb.value.weight', 'bert.encoder.layer.3.output.adapter_fusion_layer.sst-2,imdb.query.weight', 'bert.encoder.layer.3.output.adapter_fusion_layer.sst-2,imdb.query.bias', 'bert.encoder.layer.3.output.adapter_fusion_layer.sst-2,imdb.key.weight', 'bert.encoder.layer.3.output.adapter_fusion_layer.sst-2,imdb.key.bias', 'bert.encoder.layer.3.output.adapter_fusion_layer.sst-2,imdb.value.weight', 'bert.encoder.layer.4.output.adapter_fusion_layer.sst-2,imdb.query.weight', 'bert.encoder.layer.4.output.adapter_fusion_layer.sst-2,imdb.query.bias', 'bert.encoder.layer.4.output.adapter_fusion_layer.sst-2,imdb.key.weight', 'bert.encoder.layer.4.output.adapter_fusion_layer.sst-2,imdb.key.bias', 'bert.encoder.layer.4.output.adapter_fusion_layer.sst-2,imdb.value.weight', 'bert.encoder.layer.5.output.adapter_fusion_layer.sst-2,imdb.query.weight', 'bert.encoder.layer.5.output.adapter_fusion_layer.sst-2,imdb.query.bias', 'bert.encoder.layer.5.output.adapter_fusion_layer.sst-2,imdb.key.weight', 'bert.encoder.layer.5.output.adapter_fusion_layer.sst-2,imdb.key.bias', 'bert.encoder.layer.5.output.adapter_fusion_layer.sst-2,imdb.value.weight', 'bert.encoder.layer.6.output.adapter_fusion_layer.sst-2,imdb.query.weight', 'bert.encoder.layer.6.output.adapter_fusion_layer.sst-2,imdb.query.bias', 'bert.encoder.layer.6.output.adapter_fusion_layer.sst-2,imdb.key.weight', 'bert.encoder.layer.6.output.adapter_fusion_layer.sst-2,imdb.key.bias', 'bert.encoder.layer.6.output.adapter_fusion_layer.sst-2,imdb.value.weight', 'bert.encoder.layer.7.output.adapter_fusion_layer.sst-2,imdb.query.weight', 'bert.encoder.layer.7.output.adapter_fusion_layer.sst-2,imdb.query.bias', 'bert.encoder.layer.7.output.adapter_fusion_layer.sst-2,imdb.key.weight', 'bert.encoder.layer.7.output.adapter_fusion_layer.sst-2,imdb.key.bias', 'bert.encoder.layer.7.output.adapter_fusion_layer.sst-2,imdb.value.weight', 'bert.encoder.layer.8.output.adapter_fusion_layer.sst-2,imdb.query.weight', 'bert.encoder.layer.8.output.adapter_fusion_layer.sst-2,imdb.query.bias', 'bert.encoder.layer.8.output.adapter_fusion_layer.sst-2,imdb.key.weight', 'bert.encoder.layer.8.output.adapter_fusion_layer.sst-2,imdb.key.bias', 'bert.encoder.layer.8.output.adapter_fusion_layer.sst-2,imdb.value.weight', 'bert.encoder.layer.9.output.adapter_fusion_layer.sst-2,imdb.query.weight', 'bert.encoder.layer.9.output.adapter_fusion_layer.sst-2,imdb.query.bias', 'bert.encoder.layer.9.output.adapter_fusion_layer.sst-2,imdb.key.weight', 'bert.encoder.layer.9.output.adapter_fusion_layer.sst-2,imdb.key.bias', 'bert.encoder.layer.9.output.adapter_fusion_layer.sst-2,imdb.value.weight', 'bert.encoder.layer.10.output.adapter_fusion_layer.sst-2,imdb.query.weight', 'bert.encoder.layer.10.output.adapter_fusion_layer.sst-2,imdb.query.bias', 'bert.encoder.layer.10.output.adapter_fusion_layer.sst-2,imdb.key.weight', 'bert.encoder.layer.10.output.adapter_fusion_layer.sst-2,imdb.key.bias', 'bert.encoder.layer.10.output.adapter_fusion_layer.sst-2,imdb.value.weight', 'bert.encoder.layer.11.output.adapter_fusion_layer.sst-2,imdb.query.weight', 'bert.encoder.layer.11.output.adapter_fusion_layer.sst-2,imdb.query.bias', 'bert.encoder.layer.11.output.adapter_fusion_layer.sst-2,imdb.key.weight', 'bert.encoder.layer.11.output.adapter_fusion_layer.sst-2,imdb.key.bias', 'bert.encoder.layer.11.output.adapter_fusion_layer.sst-2,imdb.value.weight'])\n",
            "100% 6369/6369 [01:43<00:00, 61.54it/s]\n",
            "\n",
            "\u001b[91mEvaluating bias on intrasentence tasks...\u001b[0m\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "100% 6318/6318 [01:45<00:00, 59.81it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUHHkF3lyHTa",
        "outputId": "1b3ececf-54b1-46d0-9620-9b30c07ddca1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/score_evaluation.py --gold-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --predictions-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/fusion-sst-imdb/predictions.json --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/fusion-sst-imdb/summary.json"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "intrasentence\n",
            "\tgender\n",
            "\t\tCount: 765.0\n",
            "\t\tLM Score: 26.13426082773909\n",
            "\t\tSS Score: 42.58381489685837\n",
            "\t\tICAT Score: 22.25793051109316\n",
            "\tprofession\n",
            "\t\tCount: 2430.0\n",
            "\t\tLM Score: 24.7810561028714\n",
            "\t\tSS Score: 46.818668285512075\n",
            "\t\tICAT Score: 23.204320908900016\n",
            "\trace\n",
            "\t\tCount: 2886.0\n",
            "\t\tLM Score: 19.71821666181442\n",
            "\t\tSS Score: 51.174393657392756\n",
            "\t\tICAT Score: 19.255077690159798\n",
            "\treligion\n",
            "\t\tCount: 237.0\n",
            "\t\tLM Score: 25.26436781609195\n",
            "\t\tSS Score: 58.62068965517241\n",
            "\t\tICAT Score: 20.908442330558856\n",
            "\toverall\n",
            "\t\tCount: 2106.0\n",
            "\t\tLM Score: 22.66358474224213\n",
            "\t\tSS Score: 48.71567643247598\n",
            "\t\tICAT Score: 22.081437222061343\n",
            "intersentence\n",
            "\tgender\n",
            "\t\tCount: 726.0\n",
            "\t\tLM Score: 89.27522960131655\n",
            "\t\tSS Score: 57.683415618198225\n",
            "\t\tICAT Score: 75.5564557325768\n",
            "\tprofession\n",
            "\t\tCount: 2481.0\n",
            "\t\tLM Score: 82.14922838803075\n",
            "\t\tSS Score: 62.02679214659193\n",
            "\t\tICAT Score: 62.389394491515645\n",
            "\trace\n",
            "\t\tCount: 2928.0\n",
            "\t\tLM Score: 85.85695281724048\n",
            "\t\tSS Score: 60.667684364322604\n",
            "\t\tICAT Score: 67.53905535450329\n",
            "\treligion\n",
            "\t\tCount: 234.0\n",
            "\t\tLM Score: 87.93390804597702\n",
            "\t\tSS Score: 61.89080459770114\n",
            "\t\tICAT Score: 67.02180968423836\n",
            "\toverall\n",
            "\t\tCount: 2123.0\n",
            "\t\tLM Score: 84.96052117990733\n",
            "\t\tSS Score: 60.85249330998047\n",
            "\t\tICAT Score: 66.51985142555937\n",
            "overall\n",
            "\tCount: 4229.0\n",
            "\tLM Score: 53.89797026105209\n",
            "\tSS Score: 54.730734028123074\n",
            "\tICAT Score: 48.798431021837594\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL4bqdI1yxoa"
      },
      "source": [
        "Fusion pre-trained and fine tuned"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIuy9GGFyV6u"
      },
      "source": [
        "import os\n",
        "try:\n",
        "    os.makedirs('/content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/fusion-pre-fine/')\n",
        "except OSError as e:\n",
        "        pass"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZMdlNszyZ-l",
        "outputId": "401adfb1-da16-4238-e592-860a3a54f2b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/evaluation.py --load-path_model /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/fusion3/pytorch_model_adapter_fusion.bin --load-path_adapter /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/fusion3/pytorch_model_adapter_fusion.bin --input-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --output-dir /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/fusion-pre-fine/ --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/fusion-pre-fine/predictions.json\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json...\n",
            "---------------------------------------------------------------\n",
            "\u001b[96m                     ARGUMENTS                 \u001b[0m\n",
            "\u001b[96mPretrained Model:\u001b[0m /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/fusion3/pytorch_model_adapter_fusion.bin\n",
            "\u001b[96mSkip Intrasentence:\u001b[0m False\n",
            "\u001b[96mSkip Intersentence:\u001b[0m False\n",
            "\u001b[96mBatch Size:\u001b[0m 1\n",
            "\u001b[96mMax Seq Length:\u001b[0m None\n",
            "\u001b[96mCUDA:\u001b[0m True\n",
            "---------------------------------------------------------------\n",
            "\n",
            "\u001b[94mEvaluating bias on intersentence tasks...\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2160: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
            "  FutureWarning,\n",
            "Maximum sequence length found: -inf\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "dict_keys(['bert.encoder.layer.0.output.adapter_fusion_layer.sst-2,sst-2.query.weight', 'bert.encoder.layer.0.output.adapter_fusion_layer.sst-2,sst-2.query.bias', 'bert.encoder.layer.0.output.adapter_fusion_layer.sst-2,sst-2.key.weight', 'bert.encoder.layer.0.output.adapter_fusion_layer.sst-2,sst-2.key.bias', 'bert.encoder.layer.0.output.adapter_fusion_layer.sst-2,sst-2.value.weight', 'bert.encoder.layer.1.output.adapter_fusion_layer.sst-2,sst-2.query.weight', 'bert.encoder.layer.1.output.adapter_fusion_layer.sst-2,sst-2.query.bias', 'bert.encoder.layer.1.output.adapter_fusion_layer.sst-2,sst-2.key.weight', 'bert.encoder.layer.1.output.adapter_fusion_layer.sst-2,sst-2.key.bias', 'bert.encoder.layer.1.output.adapter_fusion_layer.sst-2,sst-2.value.weight', 'bert.encoder.layer.2.output.adapter_fusion_layer.sst-2,sst-2.query.weight', 'bert.encoder.layer.2.output.adapter_fusion_layer.sst-2,sst-2.query.bias', 'bert.encoder.layer.2.output.adapter_fusion_layer.sst-2,sst-2.key.weight', 'bert.encoder.layer.2.output.adapter_fusion_layer.sst-2,sst-2.key.bias', 'bert.encoder.layer.2.output.adapter_fusion_layer.sst-2,sst-2.value.weight', 'bert.encoder.layer.3.output.adapter_fusion_layer.sst-2,sst-2.query.weight', 'bert.encoder.layer.3.output.adapter_fusion_layer.sst-2,sst-2.query.bias', 'bert.encoder.layer.3.output.adapter_fusion_layer.sst-2,sst-2.key.weight', 'bert.encoder.layer.3.output.adapter_fusion_layer.sst-2,sst-2.key.bias', 'bert.encoder.layer.3.output.adapter_fusion_layer.sst-2,sst-2.value.weight', 'bert.encoder.layer.4.output.adapter_fusion_layer.sst-2,sst-2.query.weight', 'bert.encoder.layer.4.output.adapter_fusion_layer.sst-2,sst-2.query.bias', 'bert.encoder.layer.4.output.adapter_fusion_layer.sst-2,sst-2.key.weight', 'bert.encoder.layer.4.output.adapter_fusion_layer.sst-2,sst-2.key.bias', 'bert.encoder.layer.4.output.adapter_fusion_layer.sst-2,sst-2.value.weight', 'bert.encoder.layer.5.output.adapter_fusion_layer.sst-2,sst-2.query.weight', 'bert.encoder.layer.5.output.adapter_fusion_layer.sst-2,sst-2.query.bias', 'bert.encoder.layer.5.output.adapter_fusion_layer.sst-2,sst-2.key.weight', 'bert.encoder.layer.5.output.adapter_fusion_layer.sst-2,sst-2.key.bias', 'bert.encoder.layer.5.output.adapter_fusion_layer.sst-2,sst-2.value.weight', 'bert.encoder.layer.6.output.adapter_fusion_layer.sst-2,sst-2.query.weight', 'bert.encoder.layer.6.output.adapter_fusion_layer.sst-2,sst-2.query.bias', 'bert.encoder.layer.6.output.adapter_fusion_layer.sst-2,sst-2.key.weight', 'bert.encoder.layer.6.output.adapter_fusion_layer.sst-2,sst-2.key.bias', 'bert.encoder.layer.6.output.adapter_fusion_layer.sst-2,sst-2.value.weight', 'bert.encoder.layer.7.output.adapter_fusion_layer.sst-2,sst-2.query.weight', 'bert.encoder.layer.7.output.adapter_fusion_layer.sst-2,sst-2.query.bias', 'bert.encoder.layer.7.output.adapter_fusion_layer.sst-2,sst-2.key.weight', 'bert.encoder.layer.7.output.adapter_fusion_layer.sst-2,sst-2.key.bias', 'bert.encoder.layer.7.output.adapter_fusion_layer.sst-2,sst-2.value.weight', 'bert.encoder.layer.8.output.adapter_fusion_layer.sst-2,sst-2.query.weight', 'bert.encoder.layer.8.output.adapter_fusion_layer.sst-2,sst-2.query.bias', 'bert.encoder.layer.8.output.adapter_fusion_layer.sst-2,sst-2.key.weight', 'bert.encoder.layer.8.output.adapter_fusion_layer.sst-2,sst-2.key.bias', 'bert.encoder.layer.8.output.adapter_fusion_layer.sst-2,sst-2.value.weight', 'bert.encoder.layer.9.output.adapter_fusion_layer.sst-2,sst-2.query.weight', 'bert.encoder.layer.9.output.adapter_fusion_layer.sst-2,sst-2.query.bias', 'bert.encoder.layer.9.output.adapter_fusion_layer.sst-2,sst-2.key.weight', 'bert.encoder.layer.9.output.adapter_fusion_layer.sst-2,sst-2.key.bias', 'bert.encoder.layer.9.output.adapter_fusion_layer.sst-2,sst-2.value.weight', 'bert.encoder.layer.10.output.adapter_fusion_layer.sst-2,sst-2.query.weight', 'bert.encoder.layer.10.output.adapter_fusion_layer.sst-2,sst-2.query.bias', 'bert.encoder.layer.10.output.adapter_fusion_layer.sst-2,sst-2.key.weight', 'bert.encoder.layer.10.output.adapter_fusion_layer.sst-2,sst-2.key.bias', 'bert.encoder.layer.10.output.adapter_fusion_layer.sst-2,sst-2.value.weight', 'bert.encoder.layer.11.output.adapter_fusion_layer.sst-2,sst-2.query.weight', 'bert.encoder.layer.11.output.adapter_fusion_layer.sst-2,sst-2.query.bias', 'bert.encoder.layer.11.output.adapter_fusion_layer.sst-2,sst-2.key.weight', 'bert.encoder.layer.11.output.adapter_fusion_layer.sst-2,sst-2.key.bias', 'bert.encoder.layer.11.output.adapter_fusion_layer.sst-2,sst-2.value.weight'])\n",
            "100% 6369/6369 [01:42<00:00, 61.91it/s]\n",
            "\n",
            "\u001b[91mEvaluating bias on intrasentence tasks...\u001b[0m\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "100% 6318/6318 [01:43<00:00, 60.76it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TxQY_h5ylAg",
        "outputId": "b597ecde-548e-4427-e0de-e53cab98b1a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/score_evaluation.py --gold-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --predictions-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/fusion-pre-fine/predictions.json --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/fusion-pre-fine/summary.json"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "intrasentence\n",
            "\tgender\n",
            "\t\tCount: 765.0\n",
            "\t\tLM Score: 57.22502642285251\n",
            "\t\tSS Score: 50.4376454463411\n",
            "\t\tICAT Score: 56.724140978238296\n",
            "\tprofession\n",
            "\t\tCount: 2430.0\n",
            "\t\tLM Score: 61.59333662616648\n",
            "\t\tSS Score: 51.24620023667777\n",
            "\t\tICAT Score: 60.05818401254043\n",
            "\trace\n",
            "\t\tCount: 2886.0\n",
            "\t\tLM Score: 62.71292921930271\n",
            "\t\tSS Score: 59.53688167137289\n",
            "\t\tICAT Score: 50.75121351470924\n",
            "\treligion\n",
            "\t\tCount: 237.0\n",
            "\t\tLM Score: 62.22988505747126\n",
            "\t\tSS Score: 50.98850574712643\n",
            "\t\tICAT Score: 60.999593077024706\n",
            "\toverall\n",
            "\t\tCount: 2106.0\n",
            "\t\tLM Score: 61.574752785833304\n",
            "\t\tSS Score: 54.91209770853859\n",
            "\t\tICAT Score: 55.52552874457087\n",
            "intersentence\n",
            "\tgender\n",
            "\t\tCount: 726.0\n",
            "\t\tLM Score: 10.150291536161102\n",
            "\t\tSS Score: 39.927182318486665\n",
            "\t\tICAT Score: 8.105450815001928\n",
            "\tprofession\n",
            "\t\tCount: 2481.0\n",
            "\t\tLM Score: 18.652360200395055\n",
            "\t\tSS Score: 38.71645915298121\n",
            "\t\tICAT Score: 14.44306683610575\n",
            "\trace\n",
            "\t\tCount: 2928.0\n",
            "\t\tLM Score: 15.029834831327232\n",
            "\t\tSS Score: 38.63747938457492\n",
            "\t\tICAT Score: 11.614298668979439\n",
            "\treligion\n",
            "\t\tCount: 234.0\n",
            "\t\tLM Score: 14.038314176245208\n",
            "\t\tSS Score: 36.775862068965516\n",
            "\t\tICAT Score: 10.32542211652794\n",
            "\toverall\n",
            "\t\tCount: 2123.0\n",
            "\t\tLM Score: 15.750160985189606\n",
            "\t\tSS Score: 38.76003090918856\n",
            "\t\tICAT Score: 12.209534532212897\n",
            "overall\n",
            "\tCount: 4229.0\n",
            "\tLM Score: 38.58387720589704\n",
            "\tSS Score: 46.73842480257085\n",
            "\tICAT Score: 36.06699286758892\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zthE4Z8F7ZPJ"
      },
      "source": [
        "single imdb pretrained adapter\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmzhKw5N7X5W"
      },
      "source": [
        "import os\n",
        "try:\n",
        "    os.makedirs('/content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/imdb-pretrained/')\n",
        "except OSError as e:\n",
        "        pass"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0ELKp5M7leE",
        "outputId": "8cbf638e-23c8-4441-a019-cdca32c9f66e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/evaluation.py --load-path_model /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/imdb-pretrained/pytorch_adapter.bin --load-path_adapter /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/imdb-pretrained/pytorch_adapter.bin --input-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --output-dir /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/imdb-pretrained/ --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/imdb-pretrained/predictions.json\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json...\n",
            "---------------------------------------------------------------\n",
            "\u001b[96m                     ARGUMENTS                 \u001b[0m\n",
            "\u001b[96mPretrained Model:\u001b[0m /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/imdb-pretrained/pytorch_adapter.bin\n",
            "\u001b[96mSkip Intrasentence:\u001b[0m False\n",
            "\u001b[96mSkip Intersentence:\u001b[0m False\n",
            "\u001b[96mBatch Size:\u001b[0m 1\n",
            "\u001b[96mMax Seq Length:\u001b[0m None\n",
            "\u001b[96mCUDA:\u001b[0m True\n",
            "---------------------------------------------------------------\n",
            "\n",
            "\u001b[94mEvaluating bias on intersentence tasks...\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2160: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
            "  FutureWarning,\n",
            "Maximum sequence length found: -inf\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "dict_keys(['bert.encoder.layer.0.output.adapters.imdb.adapter_down.0.weight', 'bert.encoder.layer.0.output.adapters.imdb.adapter_down.0.bias', 'bert.encoder.layer.0.output.adapters.imdb.adapter_up.weight', 'bert.encoder.layer.0.output.adapters.imdb.adapter_up.bias', 'bert.encoder.layer.1.output.adapters.imdb.adapter_down.0.weight', 'bert.encoder.layer.1.output.adapters.imdb.adapter_down.0.bias', 'bert.encoder.layer.1.output.adapters.imdb.adapter_up.weight', 'bert.encoder.layer.1.output.adapters.imdb.adapter_up.bias', 'bert.encoder.layer.2.output.adapters.imdb.adapter_down.0.weight', 'bert.encoder.layer.2.output.adapters.imdb.adapter_down.0.bias', 'bert.encoder.layer.2.output.adapters.imdb.adapter_up.weight', 'bert.encoder.layer.2.output.adapters.imdb.adapter_up.bias', 'bert.encoder.layer.3.output.adapters.imdb.adapter_down.0.weight', 'bert.encoder.layer.3.output.adapters.imdb.adapter_down.0.bias', 'bert.encoder.layer.3.output.adapters.imdb.adapter_up.weight', 'bert.encoder.layer.3.output.adapters.imdb.adapter_up.bias', 'bert.encoder.layer.4.output.adapters.imdb.adapter_down.0.weight', 'bert.encoder.layer.4.output.adapters.imdb.adapter_down.0.bias', 'bert.encoder.layer.4.output.adapters.imdb.adapter_up.weight', 'bert.encoder.layer.4.output.adapters.imdb.adapter_up.bias', 'bert.encoder.layer.5.output.adapters.imdb.adapter_down.0.weight', 'bert.encoder.layer.5.output.adapters.imdb.adapter_down.0.bias', 'bert.encoder.layer.5.output.adapters.imdb.adapter_up.weight', 'bert.encoder.layer.5.output.adapters.imdb.adapter_up.bias', 'bert.encoder.layer.6.output.adapters.imdb.adapter_down.0.weight', 'bert.encoder.layer.6.output.adapters.imdb.adapter_down.0.bias', 'bert.encoder.layer.6.output.adapters.imdb.adapter_up.weight', 'bert.encoder.layer.6.output.adapters.imdb.adapter_up.bias', 'bert.encoder.layer.7.output.adapters.imdb.adapter_down.0.weight', 'bert.encoder.layer.7.output.adapters.imdb.adapter_down.0.bias', 'bert.encoder.layer.7.output.adapters.imdb.adapter_up.weight', 'bert.encoder.layer.7.output.adapters.imdb.adapter_up.bias', 'bert.encoder.layer.8.output.adapters.imdb.adapter_down.0.weight', 'bert.encoder.layer.8.output.adapters.imdb.adapter_down.0.bias', 'bert.encoder.layer.8.output.adapters.imdb.adapter_up.weight', 'bert.encoder.layer.8.output.adapters.imdb.adapter_up.bias', 'bert.encoder.layer.9.output.adapters.imdb.adapter_down.0.weight', 'bert.encoder.layer.9.output.adapters.imdb.adapter_down.0.bias', 'bert.encoder.layer.9.output.adapters.imdb.adapter_up.weight', 'bert.encoder.layer.9.output.adapters.imdb.adapter_up.bias', 'bert.encoder.layer.10.output.adapters.imdb.adapter_down.0.weight', 'bert.encoder.layer.10.output.adapters.imdb.adapter_down.0.bias', 'bert.encoder.layer.10.output.adapters.imdb.adapter_up.weight', 'bert.encoder.layer.10.output.adapters.imdb.adapter_up.bias', 'bert.encoder.layer.11.output.adapters.imdb.adapter_down.0.weight', 'bert.encoder.layer.11.output.adapters.imdb.adapter_down.0.bias', 'bert.encoder.layer.11.output.adapters.imdb.adapter_up.weight', 'bert.encoder.layer.11.output.adapters.imdb.adapter_up.bias'])\n",
            "100% 6369/6369 [01:44<00:00, 61.19it/s]\n",
            "\n",
            "\u001b[91mEvaluating bias on intrasentence tasks...\u001b[0m\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "100% 6318/6318 [01:45<00:00, 60.00it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHYVNTG3792Z",
        "outputId": "3c096df8-8b87-4d41-ac2d-eb08cb65daeb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/score_evaluation.py --gold-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --predictions-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/imdb-pretrained/predictions.json --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/imdb-pretrained/summary.json"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "intrasentence\n",
            "\tgender\n",
            "\t\tCount: 765.0\n",
            "\t\tLM Score: 57.21176005958614\n",
            "\t\tSS Score: 50.82091338613078\n",
            "\t\tICAT Score: 56.272442066045805\n",
            "\tprofession\n",
            "\t\tCount: 2430.0\n",
            "\t\tLM Score: 59.30420153003146\n",
            "\t\tSS Score: 51.455441935838834\n",
            "\t\tICAT Score: 57.57792509246656\n",
            "\trace\n",
            "\t\tCount: 2886.0\n",
            "\t\tLM Score: 62.42487605777936\n",
            "\t\tSS Score: 42.010674150203855\n",
            "\t\tICAT Score: 52.45022253860461\n",
            "\treligion\n",
            "\t\tCount: 237.0\n",
            "\t\tLM Score: 58.71264367816092\n",
            "\t\tSS Score: 39.67816091954023\n",
            "\t\tICAT Score: 46.59219447747392\n",
            "\toverall\n",
            "\t\tCount: 2106.0\n",
            "\t\tLM Score: 60.43895083052336\n",
            "\t\tSS Score: 46.623938532942184\n",
            "\t\tICAT Score: 56.35803857035672\n",
            "intersentence\n",
            "\tgender\n",
            "\t\tCount: 726.0\n",
            "\t\tLM Score: 69.5604174408522\n",
            "\t\tSS Score: 49.86717187804144\n",
            "\t\tICAT Score: 69.37562584862576\n",
            "\tprofession\n",
            "\t\tCount: 2481.0\n",
            "\t\tLM Score: 67.70305501293636\n",
            "\t\tSS Score: 55.71365404120318\n",
            "\t\tICAT Score: 59.96641833540706\n",
            "\trace\n",
            "\t\tCount: 2928.0\n",
            "\t\tLM Score: 65.85713535258553\n",
            "\t\tSS Score: 52.38923356748718\n",
            "\t\tICAT Score: 62.710173783726646\n",
            "\treligion\n",
            "\t\tCount: 234.0\n",
            "\t\tLM Score: 69.95210727969348\n",
            "\t\tSS Score: 50.63409961685824\n",
            "\t\tICAT Score: 69.06497519120389\n",
            "\toverall\n",
            "\t\tCount: 2123.0\n",
            "\t\tLM Score: 67.18239264973131\n",
            "\t\tSS Score: 53.26577275059015\n",
            "\t\tICAT Score: 62.7943441050325\n",
            "overall\n",
            "\tCount: 4229.0\n",
            "\tLM Score: 63.805361720844395\n",
            "\tSS Score: 50.021530869833974\n",
            "\tICAT Score: 63.77788602208597\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJxHVxTa8Wc5"
      },
      "source": [
        "sst pretrained adapter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7yU1c9ugamN"
      },
      "source": [
        "## SST Adapter\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nsl9kUigiOxz"
      },
      "source": [
        "import os\n",
        "try:\n",
        "    os.makedirs('/content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sst/')\n",
        "except OSError as e:\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYyEJ2uFW40w"
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/evaluation.py --load-path_model /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/sst/pytorch_model.bin --load-path_adapter /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/sst/pytorch_adapter.bin --input-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --output-dir /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sst/ --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sst/predictions.json\n",
        "#!python3 evaluation.py --no-cuda False --output-dir /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/ -- skip_intrasentence False --skip_intersentence False --batch_size 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDw_upEajSLL"
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/score_evaluation.py --gold-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --predictions-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sst/predictions.json --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sst/summary.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDFKHWqKlWRE"
      },
      "source": [
        "# RoBERTa\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDjpLNYqlU2x"
      },
      "source": [
        "import os\n",
        "try:\n",
        "    os.makedirs('/content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/roberta/')\n",
        "except OSError as e:\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFZPbqQnfV-v"
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/evaluation.py --load-path_model /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/roberta/pytorch_model_head.bin --load-path_adapter /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/roberta/pytorch_adapter.bin --input-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --output-dir /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/roberta/ --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/roberta/predictions.json\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAxTHS7ylu5C"
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/score_evaluation.py --gold-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --predictions-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/roberta/predictions.json --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/roberta/summary.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0YbepOXJsoO"
      },
      "source": [
        "#Language Poem (pre-trained)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4SnA2w_JyUO"
      },
      "source": [
        "import os\n",
        "try:\n",
        "    os.makedirs('/content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/lmpoem/')\n",
        "except OSError as e:\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3B6lKsCQJ53J"
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/evaluation.py --load-path_model /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/lmpoem/pytorch_adapter.bin --load-path_adapter /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/lmpoem/pytorch_adapter.bin --input-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --output-dir /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/lmpoem/ --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/lmpoem/predictions.json\n",
        "#!python3 evaluation.py --no-cuda False --output-dir /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/ -- skip_intrasentence False --skip_intersentence False --batch_size 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPnSIwspJ8wX"
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/score_evaluation.py --gold-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --predictions-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/lmpoem/predictions.json --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/lmpoem/summary.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjiwCTcAQJib"
      },
      "source": [
        "#Common Sense Winogrande (pre-trained)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iyxrzgq4QQtK"
      },
      "source": [
        "import os\n",
        "try:\n",
        "    os.makedirs('/content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/winogrande/')\n",
        "except OSError as e:\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VOY-ZXwQVoO"
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/evaluation.py --load-path_model /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/winogrande/pytorch_adapter.bin --load-path_adapter /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/winogrande/pytorch_adapter.bin --input-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --output-dir /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/winogrande/ --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/winogrande/predictions.json\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHR_-RszQjwA"
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/score_evaluation.py --gold-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --predictions-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/winogrande/predictions.json --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/winogrande/summary.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmtvxUp-T6FJ"
      },
      "source": [
        "#SST Pretrained\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zinc5psBT8yh"
      },
      "source": [
        "import os\n",
        "try:\n",
        "    os.makedirs('/content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sst-pretrained/')\n",
        "except OSError as e:\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAbECqkiUTS1"
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/evaluation.py --load-path_model /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/sst_pretrained/pytorch_adapter.bin --load-path_adapter /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/sst_pretrained/pytorch_adapter.bin --input-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --output-dir /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sst-pretrained/ --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sst-pretrained/predictions.json\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K60WFJrzUquL"
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/score_evaluation.py --gold-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --predictions-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sst-pretrained/predictions.json --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sst-pretrained/summary.json"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}