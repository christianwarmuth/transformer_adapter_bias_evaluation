{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "adapter_evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqFl7uUVgEz7"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mthG05Ijv6SZ",
        "outputId": "cfff8903-8c18-4fcb-b80d-68d29a628f24"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Aug 10 13:41:50 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SyR5EK-hsOv",
        "outputId": "71bbaa38-b4f2-4187-f12f-9f6093fb9ca9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive/\", force_remount=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "julO5QKjnRTA",
        "outputId": "ecf50d04-938c-46b1-8761-ef188c0dffd6"
      },
      "source": [
        "!pip install -U git+https://github.com/Adapter-Hub/adapter-transformers.git\n",
        "#!pip install transformers==3.0.2\n",
        "!pip3 install -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/Adapter-Hub/adapter-transformers.git\n",
            "  Cloning https://github.com/Adapter-Hub/adapter-transformers.git to /tmp/pip-req-build-xj3wm6up\n",
            "  Running command git clone -q https://github.com/Adapter-Hub/adapter-transformers.git /tmp/pip-req-build-xj3wm6up\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==2.1.2) (3.0.12)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==2.1.2) (3.13)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==2.1.2) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==2.1.2) (21.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==2.1.2) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==2.1.2) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==2.1.2) (4.6.1)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 14.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==2.1.2) (4.41.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 61.5 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.0.14\n",
            "  Downloading huggingface_hub-0.0.15-py3-none-any.whl (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.14->adapter-transformers==2.1.2) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->adapter-transformers==2.1.2) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->adapter-transformers==2.1.2) (3.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers==2.1.2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers==2.1.2) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers==2.1.2) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers==2.1.2) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->adapter-transformers==2.1.2) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->adapter-transformers==2.1.2) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->adapter-transformers==2.1.2) (1.0.1)\n",
            "Building wheels for collected packages: adapter-transformers\n",
            "  Building wheel for adapter-transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for adapter-transformers: filename=adapter_transformers-2.1.2-py3-none-any.whl size=2539220 sha256=1eb9001a93e33a8c137dc992833ea1fbd4f3a6c29e3f2b5e4ea1c18a5320a608\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7ogk2sn5/wheels/60/0e/d5/86e64fef3085f149b504f48af9ca6e8f6da427f42da9b4bbe5\n",
            "Successfully built adapter-transformers\n",
            "Installing collected packages: tokenizers, sacremoses, huggingface-hub, adapter-transformers\n",
            "Successfully installed adapter-transformers-2.1.2 huggingface-hub-0.0.15 sacremoses-0.0.45 tokenizers-0.10.3\n",
            "Requirement already satisfied: blis==0.4.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 1)) (0.4.1)\n",
            "Collecting boto3==1.12.36\n",
            "  Downloading boto3-1.12.36-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[K     |████████████████████████████████| 128 kB 12.5 MB/s \n",
            "\u001b[?25hCollecting botocore==1.15.36\n",
            "  Downloading botocore-1.15.36-py2.py3-none-any.whl (6.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.1 MB 25.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: catalogue==1.0.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 4)) (1.0.0)\n",
            "Collecting certifi==2019.11.28\n",
            "  Downloading certifi-2019.11.28-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 70.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 6)) (3.0.4)\n",
            "Collecting click==7.1.1\n",
            "  Downloading click-7.1.1-py2.py3-none-any.whl (82 kB)\n",
            "\u001b[K     |████████████████████████████████| 82 kB 1.6 MB/s \n",
            "\u001b[?25hCollecting colorama==0.4.3\n",
            "  Downloading colorama-0.4.3-py2.py3-none-any.whl (15 kB)\n",
            "Collecting cymem==2.0.3\n",
            "  Downloading cymem-2.0.3-cp37-cp37m-manylinux1_x86_64.whl (32 kB)\n",
            "Collecting docutils==0.15.2\n",
            "  Downloading docutils-0.15.2-py3-none-any.whl (547 kB)\n",
            "\u001b[K     |████████████████████████████████| 547 kB 64.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: en-core-web-sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 11)) (2.2.5)\n",
            "Requirement already satisfied: filelock==3.0.12 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 12)) (3.0.12)\n",
            "Collecting idna==2.9\n",
            "  Downloading idna-2.9-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 6.8 MB/s \n",
            "\u001b[?25hCollecting importlib-metadata==1.6.0\n",
            "  Downloading importlib_metadata-1.6.0-py2.py3-none-any.whl (30 kB)\n",
            "Collecting jmespath==0.9.5\n",
            "  Downloading jmespath-0.9.5-py2.py3-none-any.whl (24 kB)\n",
            "Collecting joblib==0.14.1\n",
            "  Downloading joblib-0.14.1-py2.py3-none-any.whl (294 kB)\n",
            "\u001b[K     |████████████████████████████████| 294 kB 59.7 MB/s \n",
            "\u001b[?25hCollecting murmurhash==1.0.2\n",
            "  Downloading murmurhash-1.0.2-cp37-cp37m-manylinux1_x86_64.whl (19 kB)\n",
            "Requirement already satisfied: plac==1.1.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 18)) (1.1.3)\n",
            "Collecting preshed==3.0.2\n",
            "  Downloading preshed-3.0.2-cp37-cp37m-manylinux1_x86_64.whl (118 kB)\n",
            "\u001b[K     |████████████████████████████████| 118 kB 77.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil==2.8.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 20)) (2.8.1)\n",
            "Collecting pytz==2019.3\n",
            "  Downloading pytz-2019.3-py2.py3-none-any.whl (509 kB)\n",
            "\u001b[K     |████████████████████████████████| 509 kB 56.4 MB/s \n",
            "\u001b[?25hCollecting regex==2020.2.20\n",
            "  Downloading regex-2020.2.20-cp37-cp37m-manylinux2010_x86_64.whl (689 kB)\n",
            "\u001b[K     |████████████████████████████████| 689 kB 54.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 23)) (2.23.0)\n",
            "Collecting s3transfer==0.3.3\n",
            "  Downloading s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 8.6 MB/s \n",
            "\u001b[?25hCollecting sacremoses==0.0.38\n",
            "  Downloading sacremoses-0.0.38.tar.gz (860 kB)\n",
            "\u001b[K     |████████████████████████████████| 860 kB 54.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn==0.22.2.post1 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 26)) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 27)) (1.4.1)\n",
            "Collecting sentencepiece==0.1.85\n",
            "  Downloading sentencepiece-0.1.85-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 49.0 MB/s \n",
            "\u001b[?25hCollecting six==1.14.0\n",
            "  Downloading six-1.14.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 30)) (0.0)\n",
            "Requirement already satisfied: spacy==2.2.4 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 31)) (2.2.4)\n",
            "Collecting srsly==1.0.2\n",
            "  Downloading srsly-1.0.2-cp37-cp37m-manylinux1_x86_64.whl (185 kB)\n",
            "\u001b[K     |████████████████████████████████| 185 kB 76.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 33)) (7.4.0)\n",
            "Collecting torch==1.6.0\n",
            "  Downloading torch-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (748.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 748.8 MB 18 kB/s \n",
            "\u001b[?25hCollecting tqdm==4.45.0\n",
            "  Downloading tqdm-4.45.0-py2.py3-none-any.whl (60 kB)\n",
            "\u001b[K     |████████████████████████████████| 60 kB 9.3 MB/s \n",
            "\u001b[?25hCollecting urllib3==1.25.8\n",
            "  Downloading urllib3-1.25.8-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 79.1 MB/s \n",
            "\u001b[?25hCollecting wasabi==0.6.0\n",
            "  Downloading wasabi-0.6.0-py3-none-any.whl (20 kB)\n",
            "Collecting zipp==3.1.0\n",
            "  Downloading zipp-3.1.0-py3-none-any.whl (4.9 kB)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from blis==0.4.1->-r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4->-r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 31)) (57.2.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0->-r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 34)) (0.16.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-py3-none-any.whl size=884621 sha256=412ce72c86f79e8742aab355f91ac4c11f72bd629b1c21d5d397e29692413869\n",
            "  Stored in directory: /root/.cache/pip/wheels/99/c9/5a/a5e36bce983040ea5061a8ec65b5852bfebad4b1afa8d5b394\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: zipp, six, murmurhash, importlib-metadata, cymem, wasabi, urllib3, tqdm, srsly, preshed, jmespath, idna, docutils, certifi, joblib, botocore, s3transfer, regex, click, torch, sentencepiece, sacremoses, pytz, colorama, boto3\n",
            "  Attempting uninstall: zipp\n",
            "    Found existing installation: zipp 3.5.0\n",
            "    Uninstalling zipp-3.5.0:\n",
            "      Successfully uninstalled zipp-3.5.0\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Attempting uninstall: murmurhash\n",
            "    Found existing installation: murmurhash 1.0.5\n",
            "    Uninstalling murmurhash-1.0.5:\n",
            "      Successfully uninstalled murmurhash-1.0.5\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.6.1\n",
            "    Uninstalling importlib-metadata-4.6.1:\n",
            "      Successfully uninstalled importlib-metadata-4.6.1\n",
            "  Attempting uninstall: cymem\n",
            "    Found existing installation: cymem 2.0.5\n",
            "    Uninstalling cymem-2.0.5:\n",
            "      Successfully uninstalled cymem-2.0.5\n",
            "  Attempting uninstall: wasabi\n",
            "    Found existing installation: wasabi 0.8.2\n",
            "    Uninstalling wasabi-0.8.2:\n",
            "      Successfully uninstalled wasabi-0.8.2\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: preshed\n",
            "    Found existing installation: preshed 3.0.5\n",
            "    Uninstalling preshed-3.0.5:\n",
            "      Successfully uninstalled preshed-3.0.5\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 2.10\n",
            "    Uninstalling idna-2.10:\n",
            "      Successfully uninstalled idna-2.10\n",
            "  Attempting uninstall: docutils\n",
            "    Found existing installation: docutils 0.17.1\n",
            "    Uninstalling docutils-0.17.1:\n",
            "      Successfully uninstalled docutils-0.17.1\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2021.5.30\n",
            "    Uninstalling certifi-2021.5.30:\n",
            "      Successfully uninstalled certifi-2021.5.30\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.0.1\n",
            "    Uninstalling joblib-1.0.1:\n",
            "      Successfully uninstalled joblib-1.0.1\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 7.1.2\n",
            "    Uninstalling click-7.1.2:\n",
            "      Successfully uninstalled click-7.1.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.9.0+cu102\n",
            "    Uninstalling torch-1.9.0+cu102:\n",
            "      Successfully uninstalled torch-1.9.0+cu102\n",
            "  Attempting uninstall: sacremoses\n",
            "    Found existing installation: sacremoses 0.0.45\n",
            "    Uninstalling sacremoses-0.0.45:\n",
            "      Successfully uninstalled sacremoses-0.0.45\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2018.9\n",
            "    Uninstalling pytz-2018.9:\n",
            "      Successfully uninstalled pytz-2018.9\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.10.0+cu102 requires torch==1.9.0, but you have torch 1.6.0 which is incompatible.\n",
            "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.6.0 which is incompatible.\n",
            "tensorflow 2.5.0 requires six~=1.15.0, but you have six 1.14.0 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.14.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.12.36 botocore-1.15.36 certifi-2019.11.28 click-7.1.1 colorama-0.4.3 cymem-2.0.3 docutils-0.15.2 idna-2.9 importlib-metadata-1.6.0 jmespath-0.9.5 joblib-0.14.1 murmurhash-1.0.2 preshed-3.0.2 pytz-2019.3 regex-2020.2.20 s3transfer-0.3.3 sacremoses-0.0.38 sentencepiece-0.1.85 six-1.14.0 srsly-1.0.2 torch-1.6.0 tqdm-4.45.0 urllib3-1.25.8 wasabi-0.6.0 zipp-3.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pytz",
                  "six"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbEF5yRZy4an",
        "outputId": "3a61aa11-8d68-470b-b86c-d3d0a6e1b77d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import sys\n",
        "from collections import defaultdict \n",
        "from collections import defaultdict \n",
        "from transformers import BertForMaskedLM, BertModel, BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "sys.path.append('/content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/')\n",
        "path = \"/content/gdrive/MyDrive/master_hpi/NLP_Project/code/\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[autoreload of six failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "AttributeError: 'NoneType' object has no attribute 'cStringIO'\n",
            "]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8yDqLezqXLT"
      },
      "source": [
        "#adapter_list = [\"sst\"]\n",
        "#for adapter in adapter_list:\n",
        "#  print(\"!python3 evaluation.py --no-cuda False --output-dir /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/ -- skip_intrasentence False --skip_intersentence False --batch_size 1\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2LRBGYNxgAZ"
      },
      "source": [
        "# Adapter Fusion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xsDHhl0G7Sl"
      },
      "source": [
        "#sst-imdb adapter fusion evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5DQkbAYxjrx"
      },
      "source": [
        "import os\n",
        "try:\n",
        "    os.makedirs('/content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/fusion-sst-imdb/')\n",
        "except OSError as e:\n",
        "        pass"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DudWrrkJxpou",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2469ce4c-11af-4847-90a9-9c0794841ce7"
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/evaluation.py --load-path_model /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/fusion2/pytorch_model_adapter_fusion.bin --load-path_adapter /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/fusion2/pytorch_model_adapter_fusion.bin --input-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --output-dir /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/fusion-sst-imdb/ --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/fusion-sst-imdb/predictions.json\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json...\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 1.42MB/s]\n",
            "Downloading: 100% 28.0/28.0 [00:00<00:00, 30.9kB/s]\n",
            "Downloading: 100% 466k/466k [00:00<00:00, 2.32MB/s]\n",
            "---------------------------------------------------------------\n",
            "\u001b[96m                     ARGUMENTS                 \u001b[0m\n",
            "\u001b[96mPretrained Model:\u001b[0m /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/fusion2/pytorch_model_adapter_fusion.bin\n",
            "\u001b[96mSkip Intrasentence:\u001b[0m False\n",
            "\u001b[96mSkip Intersentence:\u001b[0m False\n",
            "\u001b[96mBatch Size:\u001b[0m 1\n",
            "\u001b[96mMax Seq Length:\u001b[0m None\n",
            "\u001b[96mCUDA:\u001b[0m True\n",
            "---------------------------------------------------------------\n",
            "\n",
            "\u001b[94mEvaluating bias on intersentence tasks...\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2160: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
            "  FutureWarning,\n",
            "Maximum sequence length found: -inf\n",
            "Downloading: 100% 570/570 [00:00<00:00, 644kB/s]\n",
            "Downloading: 100% 440M/440M [00:07<00:00, 61.6MB/s]\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "dict_keys(['bert.encoder.layer.0.output.adapter_fusion_layer.sst-2,imdb.query.weight', 'bert.encoder.layer.0.output.adapter_fusion_layer.sst-2,imdb.query.bias', 'bert.encoder.layer.0.output.adapter_fusion_layer.sst-2,imdb.key.weight', 'bert.encoder.layer.0.output.adapter_fusion_layer.sst-2,imdb.key.bias', 'bert.encoder.layer.0.output.adapter_fusion_layer.sst-2,imdb.value.weight', 'bert.encoder.layer.1.output.adapter_fusion_layer.sst-2,imdb.query.weight', 'bert.encoder.layer.1.output.adapter_fusion_layer.sst-2,imdb.query.bias', 'bert.encoder.layer.1.output.adapter_fusion_layer.sst-2,imdb.key.weight', 'bert.encoder.layer.1.output.adapter_fusion_layer.sst-2,imdb.key.bias', 'bert.encoder.layer.1.output.adapter_fusion_layer.sst-2,imdb.value.weight', 'bert.encoder.layer.2.output.adapter_fusion_layer.sst-2,imdb.query.weight', 'bert.encoder.layer.2.output.adapter_fusion_layer.sst-2,imdb.query.bias', 'bert.encoder.layer.2.output.adapter_fusion_layer.sst-2,imdb.key.weight', 'bert.encoder.layer.2.output.adapter_fusion_layer.sst-2,imdb.key.bias', 'bert.encoder.layer.2.output.adapter_fusion_layer.sst-2,imdb.value.weight', 'bert.encoder.layer.3.output.adapter_fusion_layer.sst-2,imdb.query.weight', 'bert.encoder.layer.3.output.adapter_fusion_layer.sst-2,imdb.query.bias', 'bert.encoder.layer.3.output.adapter_fusion_layer.sst-2,imdb.key.weight', 'bert.encoder.layer.3.output.adapter_fusion_layer.sst-2,imdb.key.bias', 'bert.encoder.layer.3.output.adapter_fusion_layer.sst-2,imdb.value.weight', 'bert.encoder.layer.4.output.adapter_fusion_layer.sst-2,imdb.query.weight', 'bert.encoder.layer.4.output.adapter_fusion_layer.sst-2,imdb.query.bias', 'bert.encoder.layer.4.output.adapter_fusion_layer.sst-2,imdb.key.weight', 'bert.encoder.layer.4.output.adapter_fusion_layer.sst-2,imdb.key.bias', 'bert.encoder.layer.4.output.adapter_fusion_layer.sst-2,imdb.value.weight', 'bert.encoder.layer.5.output.adapter_fusion_layer.sst-2,imdb.query.weight', 'bert.encoder.layer.5.output.adapter_fusion_layer.sst-2,imdb.query.bias', 'bert.encoder.layer.5.output.adapter_fusion_layer.sst-2,imdb.key.weight', 'bert.encoder.layer.5.output.adapter_fusion_layer.sst-2,imdb.key.bias', 'bert.encoder.layer.5.output.adapter_fusion_layer.sst-2,imdb.value.weight', 'bert.encoder.layer.6.output.adapter_fusion_layer.sst-2,imdb.query.weight', 'bert.encoder.layer.6.output.adapter_fusion_layer.sst-2,imdb.query.bias', 'bert.encoder.layer.6.output.adapter_fusion_layer.sst-2,imdb.key.weight', 'bert.encoder.layer.6.output.adapter_fusion_layer.sst-2,imdb.key.bias', 'bert.encoder.layer.6.output.adapter_fusion_layer.sst-2,imdb.value.weight', 'bert.encoder.layer.7.output.adapter_fusion_layer.sst-2,imdb.query.weight', 'bert.encoder.layer.7.output.adapter_fusion_layer.sst-2,imdb.query.bias', 'bert.encoder.layer.7.output.adapter_fusion_layer.sst-2,imdb.key.weight', 'bert.encoder.layer.7.output.adapter_fusion_layer.sst-2,imdb.key.bias', 'bert.encoder.layer.7.output.adapter_fusion_layer.sst-2,imdb.value.weight', 'bert.encoder.layer.8.output.adapter_fusion_layer.sst-2,imdb.query.weight', 'bert.encoder.layer.8.output.adapter_fusion_layer.sst-2,imdb.query.bias', 'bert.encoder.layer.8.output.adapter_fusion_layer.sst-2,imdb.key.weight', 'bert.encoder.layer.8.output.adapter_fusion_layer.sst-2,imdb.key.bias', 'bert.encoder.layer.8.output.adapter_fusion_layer.sst-2,imdb.value.weight', 'bert.encoder.layer.9.output.adapter_fusion_layer.sst-2,imdb.query.weight', 'bert.encoder.layer.9.output.adapter_fusion_layer.sst-2,imdb.query.bias', 'bert.encoder.layer.9.output.adapter_fusion_layer.sst-2,imdb.key.weight', 'bert.encoder.layer.9.output.adapter_fusion_layer.sst-2,imdb.key.bias', 'bert.encoder.layer.9.output.adapter_fusion_layer.sst-2,imdb.value.weight', 'bert.encoder.layer.10.output.adapter_fusion_layer.sst-2,imdb.query.weight', 'bert.encoder.layer.10.output.adapter_fusion_layer.sst-2,imdb.query.bias', 'bert.encoder.layer.10.output.adapter_fusion_layer.sst-2,imdb.key.weight', 'bert.encoder.layer.10.output.adapter_fusion_layer.sst-2,imdb.key.bias', 'bert.encoder.layer.10.output.adapter_fusion_layer.sst-2,imdb.value.weight', 'bert.encoder.layer.11.output.adapter_fusion_layer.sst-2,imdb.query.weight', 'bert.encoder.layer.11.output.adapter_fusion_layer.sst-2,imdb.query.bias', 'bert.encoder.layer.11.output.adapter_fusion_layer.sst-2,imdb.key.weight', 'bert.encoder.layer.11.output.adapter_fusion_layer.sst-2,imdb.key.bias', 'bert.encoder.layer.11.output.adapter_fusion_layer.sst-2,imdb.value.weight'])\n",
            "100% 6369/6369 [01:35<00:00, 66.81it/s]\n",
            "\n",
            "\u001b[91mEvaluating bias on intrasentence tasks...\u001b[0m\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "100% 6318/6318 [01:34<00:00, 66.55it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUHHkF3lyHTa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6692847b-3076-4353-cba4-6da272001327"
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/score_evaluation.py --gold-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --predictions-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/fusion-sst-imdb/predictions.json --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/fusion-sst-imdb/summary.json"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "intrasentence\n",
            "\tgender\n",
            "\t\tCount: 765.0\n",
            "\t\tLM Score: 29.918070737635958\n",
            "\t\tSS Score: 39.75141621663361\n",
            "\t\tICAT Score: 23.785713645809068\n",
            "\tprofession\n",
            "\t\tCount: 2430.0\n",
            "\t\tLM Score: 26.089826234902684\n",
            "\t\tSS Score: 45.97220780258053\n",
            "\t\tICAT Score: 23.98813826408327\n",
            "\trace\n",
            "\t\tCount: 2886.0\n",
            "\t\tLM Score: 19.281816473601292\n",
            "\t\tSS Score: 50.843250383330464\n",
            "\t\tICAT Score: 18.956628490947853\n",
            "\treligion\n",
            "\t\tCount: 237.0\n",
            "\t\tLM Score: 26.59770114942529\n",
            "\t\tSS Score: 41.241379310344826\n",
            "\t\tICAT Score: 21.93851763773286\n",
            "\toverall\n",
            "\t\tCount: 2106.0\n",
            "\t\tLM Score: 23.491316340776745\n",
            "\t\tSS Score: 47.224829721198525\n",
            "\t\tICAT Score: 22.187468282399802\n",
            "intersentence\n",
            "\tgender\n",
            "\t\tCount: 726.0\n",
            "\t\tLM Score: 10.115851338677425\n",
            "\t\tSS Score: 44.472270885314366\n",
            "\t\tICAT Score: 8.997497619384648\n",
            "\tprofession\n",
            "\t\tCount: 2481.0\n",
            "\t\tLM Score: 16.454588893919627\n",
            "\t\tSS Score: 37.77181581241648\n",
            "\t\tICAT Score: 12.430394019403318\n",
            "\trace\n",
            "\t\tCount: 2928.0\n",
            "\t\tLM Score: 11.772209172746807\n",
            "\t\tSS Score: 40.19948206510436\n",
            "\t\tICAT Score: 9.464734230129848\n",
            "\treligion\n",
            "\t\tCount: 234.0\n",
            "\t\tLM Score: 11.343869731800766\n",
            "\t\tSS Score: 39.019157088122604\n",
            "\t\tICAT Score: 8.852564701046665\n",
            "\toverall\n",
            "\t\tCount: 2123.0\n",
            "\t\tLM Score: 13.324396450868994\n",
            "\t\tSS Score: 39.77362036498434\n",
            "\t\tICAT Score: 10.599189720588162\n",
            "overall\n",
            "\tCount: 4229.0\n",
            "\tLM Score: 18.423605348191796\n",
            "\tSS Score: 43.41186037814963\n",
            "\tICAT Score: 15.996059660756663\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL4bqdI1yxoa"
      },
      "source": [
        "#Evaluation of triple-adapter fusion (mnli,qqp,qnli)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIuy9GGFyV6u"
      },
      "source": [
        "import os\n",
        "try:\n",
        "    os.makedirs('/content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/triple-fusion/')\n",
        "except OSError as e:\n",
        "        pass"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZMdlNszyZ-l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7685171e-8a69-48c3-dee3-2c26d4cac1c5"
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/evaluation.py --load-path_model /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/fusion/pytorch_model_adapter_fusion.bin --load-path_adapter /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/fusion/pytorch_model_adapter_fusion.bin --input-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --output-dir /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/triple-fusion/ --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/triple-fusion/predictions.json\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json...\n",
            "---------------------------------------------------------------\n",
            "\u001b[96m                     ARGUMENTS                 \u001b[0m\n",
            "\u001b[96mPretrained Model:\u001b[0m /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/fusion/pytorch_model_adapter_fusion.bin\n",
            "\u001b[96mSkip Intrasentence:\u001b[0m False\n",
            "\u001b[96mSkip Intersentence:\u001b[0m False\n",
            "\u001b[96mBatch Size:\u001b[0m 1\n",
            "\u001b[96mMax Seq Length:\u001b[0m None\n",
            "\u001b[96mCUDA:\u001b[0m True\n",
            "---------------------------------------------------------------\n",
            "\n",
            "\u001b[94mEvaluating bias on intersentence tasks...\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2160: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
            "  FutureWarning,\n",
            "Maximum sequence length found: -inf\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "dict_keys(['bert.encoder.layer.0.output.adapter_fusion_layer.multinli,qqp,qnli.query.weight', 'bert.encoder.layer.0.output.adapter_fusion_layer.multinli,qqp,qnli.query.bias', 'bert.encoder.layer.0.output.adapter_fusion_layer.multinli,qqp,qnli.key.weight', 'bert.encoder.layer.0.output.adapter_fusion_layer.multinli,qqp,qnli.key.bias', 'bert.encoder.layer.0.output.adapter_fusion_layer.multinli,qqp,qnli.value.weight', 'bert.encoder.layer.1.output.adapter_fusion_layer.multinli,qqp,qnli.query.weight', 'bert.encoder.layer.1.output.adapter_fusion_layer.multinli,qqp,qnli.query.bias', 'bert.encoder.layer.1.output.adapter_fusion_layer.multinli,qqp,qnli.key.weight', 'bert.encoder.layer.1.output.adapter_fusion_layer.multinli,qqp,qnli.key.bias', 'bert.encoder.layer.1.output.adapter_fusion_layer.multinli,qqp,qnli.value.weight', 'bert.encoder.layer.2.output.adapter_fusion_layer.multinli,qqp,qnli.query.weight', 'bert.encoder.layer.2.output.adapter_fusion_layer.multinli,qqp,qnli.query.bias', 'bert.encoder.layer.2.output.adapter_fusion_layer.multinli,qqp,qnli.key.weight', 'bert.encoder.layer.2.output.adapter_fusion_layer.multinli,qqp,qnli.key.bias', 'bert.encoder.layer.2.output.adapter_fusion_layer.multinli,qqp,qnli.value.weight', 'bert.encoder.layer.3.output.adapter_fusion_layer.multinli,qqp,qnli.query.weight', 'bert.encoder.layer.3.output.adapter_fusion_layer.multinli,qqp,qnli.query.bias', 'bert.encoder.layer.3.output.adapter_fusion_layer.multinli,qqp,qnli.key.weight', 'bert.encoder.layer.3.output.adapter_fusion_layer.multinli,qqp,qnli.key.bias', 'bert.encoder.layer.3.output.adapter_fusion_layer.multinli,qqp,qnli.value.weight', 'bert.encoder.layer.4.output.adapter_fusion_layer.multinli,qqp,qnli.query.weight', 'bert.encoder.layer.4.output.adapter_fusion_layer.multinli,qqp,qnli.query.bias', 'bert.encoder.layer.4.output.adapter_fusion_layer.multinli,qqp,qnli.key.weight', 'bert.encoder.layer.4.output.adapter_fusion_layer.multinli,qqp,qnli.key.bias', 'bert.encoder.layer.4.output.adapter_fusion_layer.multinli,qqp,qnli.value.weight', 'bert.encoder.layer.5.output.adapter_fusion_layer.multinli,qqp,qnli.query.weight', 'bert.encoder.layer.5.output.adapter_fusion_layer.multinli,qqp,qnli.query.bias', 'bert.encoder.layer.5.output.adapter_fusion_layer.multinli,qqp,qnli.key.weight', 'bert.encoder.layer.5.output.adapter_fusion_layer.multinli,qqp,qnli.key.bias', 'bert.encoder.layer.5.output.adapter_fusion_layer.multinli,qqp,qnli.value.weight', 'bert.encoder.layer.6.output.adapter_fusion_layer.multinli,qqp,qnli.query.weight', 'bert.encoder.layer.6.output.adapter_fusion_layer.multinli,qqp,qnli.query.bias', 'bert.encoder.layer.6.output.adapter_fusion_layer.multinli,qqp,qnli.key.weight', 'bert.encoder.layer.6.output.adapter_fusion_layer.multinli,qqp,qnli.key.bias', 'bert.encoder.layer.6.output.adapter_fusion_layer.multinli,qqp,qnli.value.weight', 'bert.encoder.layer.7.output.adapter_fusion_layer.multinli,qqp,qnli.query.weight', 'bert.encoder.layer.7.output.adapter_fusion_layer.multinli,qqp,qnli.query.bias', 'bert.encoder.layer.7.output.adapter_fusion_layer.multinli,qqp,qnli.key.weight', 'bert.encoder.layer.7.output.adapter_fusion_layer.multinli,qqp,qnli.key.bias', 'bert.encoder.layer.7.output.adapter_fusion_layer.multinli,qqp,qnli.value.weight', 'bert.encoder.layer.8.output.adapter_fusion_layer.multinli,qqp,qnli.query.weight', 'bert.encoder.layer.8.output.adapter_fusion_layer.multinli,qqp,qnli.query.bias', 'bert.encoder.layer.8.output.adapter_fusion_layer.multinli,qqp,qnli.key.weight', 'bert.encoder.layer.8.output.adapter_fusion_layer.multinli,qqp,qnli.key.bias', 'bert.encoder.layer.8.output.adapter_fusion_layer.multinli,qqp,qnli.value.weight', 'bert.encoder.layer.9.output.adapter_fusion_layer.multinli,qqp,qnli.query.weight', 'bert.encoder.layer.9.output.adapter_fusion_layer.multinli,qqp,qnli.query.bias', 'bert.encoder.layer.9.output.adapter_fusion_layer.multinli,qqp,qnli.key.weight', 'bert.encoder.layer.9.output.adapter_fusion_layer.multinli,qqp,qnli.key.bias', 'bert.encoder.layer.9.output.adapter_fusion_layer.multinli,qqp,qnli.value.weight', 'bert.encoder.layer.10.output.adapter_fusion_layer.multinli,qqp,qnli.query.weight', 'bert.encoder.layer.10.output.adapter_fusion_layer.multinli,qqp,qnli.query.bias', 'bert.encoder.layer.10.output.adapter_fusion_layer.multinli,qqp,qnli.key.weight', 'bert.encoder.layer.10.output.adapter_fusion_layer.multinli,qqp,qnli.key.bias', 'bert.encoder.layer.10.output.adapter_fusion_layer.multinli,qqp,qnli.value.weight', 'bert.encoder.layer.11.output.adapter_fusion_layer.multinli,qqp,qnli.query.weight', 'bert.encoder.layer.11.output.adapter_fusion_layer.multinli,qqp,qnli.query.bias', 'bert.encoder.layer.11.output.adapter_fusion_layer.multinli,qqp,qnli.key.weight', 'bert.encoder.layer.11.output.adapter_fusion_layer.multinli,qqp,qnli.key.bias', 'bert.encoder.layer.11.output.adapter_fusion_layer.multinli,qqp,qnli.value.weight'])\n",
            "100% 6369/6369 [01:34<00:00, 67.58it/s]\n",
            "\n",
            "\u001b[91mEvaluating bias on intrasentence tasks...\u001b[0m\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "100% 6318/6318 [01:36<00:00, 65.33it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TxQY_h5ylAg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32a6841a-5434-4d76-d6e8-4dd9db57f538"
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/score_evaluation.py --gold-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --predictions-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/triple-fusion/predictions.json --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/triple-fusion/summary.json"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "intrasentence\n",
            "\tgender\n",
            "\t\tCount: 765.0\n",
            "\t\tLM Score: 45.71702870398523\n",
            "\t\tSS Score: 41.39144285666025\n",
            "\t\tICAT Score: 37.84587562354602\n",
            "\tprofession\n",
            "\t\tCount: 2430.0\n",
            "\t\tLM Score: 50.29811289566069\n",
            "\t\tSS Score: 50.42432984498666\n",
            "\t\tICAT Score: 49.87125308669794\n",
            "\trace\n",
            "\t\tCount: 2886.0\n",
            "\t\tLM Score: 53.272455484458355\n",
            "\t\tSS Score: 50.75311247285475\n",
            "\t\tICAT Score: 52.470052470759455\n",
            "\treligion\n",
            "\t\tCount: 237.0\n",
            "\t\tLM Score: 59.264367816091955\n",
            "\t\tSS Score: 54.206896551724135\n",
            "\t\tICAT Score: 54.27798652397939\n",
            "\toverall\n",
            "\t\tCount: 2106.0\n",
            "\t\tLM Score: 51.41411613668924\n",
            "\t\tSS Score: 49.574393197394244\n",
            "\t\tICAT Score: 50.97647218513449\n",
            "intersentence\n",
            "\tgender\n",
            "\t\tCount: 726.0\n",
            "\t\tLM Score: 10.299255897081984\n",
            "\t\tSS Score: 40.67840774362513\n",
            "\t\tICAT Score: 8.37914661674873\n",
            "\tprofession\n",
            "\t\tCount: 2481.0\n",
            "\t\tLM Score: 17.621532571782847\n",
            "\t\tSS Score: 39.378893537421526\n",
            "\t\tICAT Score: 13.87832910220885\n",
            "\trace\n",
            "\t\tCount: 2928.0\n",
            "\t\tLM Score: 14.994180843017514\n",
            "\t\tSS Score: 42.98952192712235\n",
            "\t\tICAT Score: 12.891853322602785\n",
            "\treligion\n",
            "\t\tCount: 234.0\n",
            "\t\tLM Score: 11.399425287356323\n",
            "\t\tSS Score: 37.81417624521073\n",
            "\t\tICAT Score: 8.621197538204079\n",
            "\toverall\n",
            "\t\tCount: 2123.0\n",
            "\t\tLM Score: 15.261105346012714\n",
            "\t\tSS Score: 41.129319008492836\n",
            "\t\tICAT Score: 12.553577403967447\n",
            "overall\n",
            "\tCount: 4229.0\n",
            "\tLM Score: 33.29487278253223\n",
            "\tSS Score: 45.3448113998609\n",
            "\tICAT Score: 30.19499453812572\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zg3mDFb8HKf3"
      },
      "source": [
        "import os\n",
        "try:\n",
        "    os.makedirs('/content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/multinlipretrained_fusion/')\n",
        "except OSError as e:\n",
        "        pass"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKT4QQBUHX5D",
        "outputId": "2cfd5d88-b07f-4608-e715-8cc97b6e1890",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/evaluation.py --load-path_model /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/multinlipretrained_fusion/pytorch_adapter.bin --load-path_adapter /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/multinlipretrained_fusion/pytorch_adapter.bin --input-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --output-dir /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/multinlipretrained_fusion/ --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/multinlipretrained_fusion/predictions.json\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json...\n",
            "---------------------------------------------------------------\n",
            "\u001b[96m                     ARGUMENTS                 \u001b[0m\n",
            "\u001b[96mPretrained Model:\u001b[0m /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/multinlipretrained_fusion/pytorch_adapter.bin\n",
            "\u001b[96mSkip Intrasentence:\u001b[0m False\n",
            "\u001b[96mSkip Intersentence:\u001b[0m False\n",
            "\u001b[96mBatch Size:\u001b[0m 1\n",
            "\u001b[96mMax Seq Length:\u001b[0m None\n",
            "\u001b[96mCUDA:\u001b[0m True\n",
            "---------------------------------------------------------------\n",
            "\n",
            "\u001b[94mEvaluating bias on intersentence tasks...\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2160: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
            "  FutureWarning,\n",
            "Maximum sequence length found: -inf\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "dict_keys(['bert.encoder.layer.0.output.adapters.mnli.adapter_down.0.weight', 'bert.encoder.layer.0.output.adapters.mnli.adapter_down.0.bias', 'bert.encoder.layer.0.output.adapters.mnli.adapter_up.weight', 'bert.encoder.layer.0.output.adapters.mnli.adapter_up.bias', 'bert.encoder.layer.1.output.adapters.mnli.adapter_down.0.weight', 'bert.encoder.layer.1.output.adapters.mnli.adapter_down.0.bias', 'bert.encoder.layer.1.output.adapters.mnli.adapter_up.weight', 'bert.encoder.layer.1.output.adapters.mnli.adapter_up.bias', 'bert.encoder.layer.2.output.adapters.mnli.adapter_down.0.weight', 'bert.encoder.layer.2.output.adapters.mnli.adapter_down.0.bias', 'bert.encoder.layer.2.output.adapters.mnli.adapter_up.weight', 'bert.encoder.layer.2.output.adapters.mnli.adapter_up.bias', 'bert.encoder.layer.3.output.adapters.mnli.adapter_down.0.weight', 'bert.encoder.layer.3.output.adapters.mnli.adapter_down.0.bias', 'bert.encoder.layer.3.output.adapters.mnli.adapter_up.weight', 'bert.encoder.layer.3.output.adapters.mnli.adapter_up.bias', 'bert.encoder.layer.4.output.adapters.mnli.adapter_down.0.weight', 'bert.encoder.layer.4.output.adapters.mnli.adapter_down.0.bias', 'bert.encoder.layer.4.output.adapters.mnli.adapter_up.weight', 'bert.encoder.layer.4.output.adapters.mnli.adapter_up.bias', 'bert.encoder.layer.5.output.adapters.mnli.adapter_down.0.weight', 'bert.encoder.layer.5.output.adapters.mnli.adapter_down.0.bias', 'bert.encoder.layer.5.output.adapters.mnli.adapter_up.weight', 'bert.encoder.layer.5.output.adapters.mnli.adapter_up.bias', 'bert.encoder.layer.6.output.adapters.mnli.adapter_down.0.weight', 'bert.encoder.layer.6.output.adapters.mnli.adapter_down.0.bias', 'bert.encoder.layer.6.output.adapters.mnli.adapter_up.weight', 'bert.encoder.layer.6.output.adapters.mnli.adapter_up.bias', 'bert.encoder.layer.7.output.adapters.mnli.adapter_down.0.weight', 'bert.encoder.layer.7.output.adapters.mnli.adapter_down.0.bias', 'bert.encoder.layer.7.output.adapters.mnli.adapter_up.weight', 'bert.encoder.layer.7.output.adapters.mnli.adapter_up.bias', 'bert.encoder.layer.8.output.adapters.mnli.adapter_down.0.weight', 'bert.encoder.layer.8.output.adapters.mnli.adapter_down.0.bias', 'bert.encoder.layer.8.output.adapters.mnli.adapter_up.weight', 'bert.encoder.layer.8.output.adapters.mnli.adapter_up.bias', 'bert.encoder.layer.9.output.adapters.mnli.adapter_down.0.weight', 'bert.encoder.layer.9.output.adapters.mnli.adapter_down.0.bias', 'bert.encoder.layer.9.output.adapters.mnli.adapter_up.weight', 'bert.encoder.layer.9.output.adapters.mnli.adapter_up.bias', 'bert.encoder.layer.10.output.adapters.mnli.adapter_down.0.weight', 'bert.encoder.layer.10.output.adapters.mnli.adapter_down.0.bias', 'bert.encoder.layer.10.output.adapters.mnli.adapter_up.weight', 'bert.encoder.layer.10.output.adapters.mnli.adapter_up.bias', 'bert.encoder.layer.11.output.adapters.mnli.adapter_down.0.weight', 'bert.encoder.layer.11.output.adapters.mnli.adapter_down.0.bias', 'bert.encoder.layer.11.output.adapters.mnli.adapter_up.weight', 'bert.encoder.layer.11.output.adapters.mnli.adapter_up.bias'])\n",
            "100% 6369/6369 [01:35<00:00, 66.44it/s]\n",
            "\n",
            "\u001b[91mEvaluating bias on intrasentence tasks...\u001b[0m\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "100% 6318/6318 [01:37<00:00, 64.56it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFtbbi1_HyYU",
        "outputId": "797bdd7a-5328-424f-9d54-aebde6628078",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/score_evaluation.py --gold-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --predictions-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/multinlipretrained_fusion/predictions.json --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/multinlipretrained_fusion/summary.json"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "intrasentence\n",
            "\tgender\n",
            "\t\tCount: 765.0\n",
            "\t\tLM Score: 57.30508324203977\n",
            "\t\tSS Score: 51.42322846235891\n",
            "\t\tICAT Score: 55.67391873188142\n",
            "\tprofession\n",
            "\t\tCount: 2430.0\n",
            "\t\tLM Score: 59.01707570954731\n",
            "\t\tSS Score: 53.013611223521586\n",
            "\t\tICAT Score: 55.45998527479302\n",
            "\trace\n",
            "\t\tCount: 2886.0\n",
            "\t\tLM Score: 64.10160411427552\n",
            "\t\tSS Score: 48.51923987046882\n",
            "\t\tICAT Score: 62.2032221220473\n",
            "\treligion\n",
            "\t\tCount: 237.0\n",
            "\t\tLM Score: 59.6551724137931\n",
            "\t\tSS Score: 47.49425287356322\n",
            "\t\tICAT Score: 56.66555687673405\n",
            "\toverall\n",
            "\t\tCount: 2106.0\n",
            "\t\tLM Score: 61.14159960838121\n",
            "\t\tSS Score: 50.55463310489626\n",
            "\t\tICAT Score: 60.4633765037988\n",
            "intersentence\n",
            "\tgender\n",
            "\t\tCount: 726.0\n",
            "\t\tLM Score: 28.32007264072481\n",
            "\t\tSS Score: 42.79726513422166\n",
            "\t\tICAT Score: 24.240433148510334\n",
            "\tprofession\n",
            "\t\tCount: 2481.0\n",
            "\t\tLM Score: 44.75754989802079\n",
            "\t\tSS Score: 50.32140540833533\n",
            "\t\tICAT Score: 44.46984352599954\n",
            "\trace\n",
            "\t\tCount: 2928.0\n",
            "\t\tLM Score: 41.30489885691654\n",
            "\t\tSS Score: 47.68674657554276\n",
            "\t\tICAT Score: 39.3939248823641\n",
            "\treligion\n",
            "\t\tCount: 234.0\n",
            "\t\tLM Score: 48.32758620689655\n",
            "\t\tSS Score: 49.66858237547893\n",
            "\t\tICAT Score: 48.007253930506\n",
            "\toverall\n",
            "\t\tCount: 2123.0\n",
            "\t\tLM Score: 41.23906760528552\n",
            "\t\tSS Score: 48.14358781567409\n",
            "\t\tICAT Score: 39.70793345383168\n",
            "overall\n",
            "\tCount: 4229.0\n",
            "\tLM Score: 51.14012603815035\n",
            "\tSS Score: 49.35328166089801\n",
            "\tICAT Score: 50.47866089069317\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbBfXsaHIAgj"
      },
      "source": [
        "Single Adapter qqp\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmuxqcbyH9td"
      },
      "source": [
        "import os\n",
        "try:\n",
        "    os.makedirs('/content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/qqppretrained_fusion/')\n",
        "except OSError as e:\n",
        "        pass"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vo2ev_J2IVEI",
        "outputId": "bbe67cdd-de40-4ff2-c3e9-3b7f013bbdaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/evaluation.py --load-path_model /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/qqppretrained/pytorch_adapter.bin --load-path_adapter /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/qqppretrained/pytorch_adapter.bin --input-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --output-dir /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/qqppretrained_fusion/ --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/qqppretrained_fusion/predictions.json\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json...\n",
            "---------------------------------------------------------------\n",
            "\u001b[96m                     ARGUMENTS                 \u001b[0m\n",
            "\u001b[96mPretrained Model:\u001b[0m /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/qqppretrained/pytorch_adapter.bin\n",
            "\u001b[96mSkip Intrasentence:\u001b[0m False\n",
            "\u001b[96mSkip Intersentence:\u001b[0m False\n",
            "\u001b[96mBatch Size:\u001b[0m 1\n",
            "\u001b[96mMax Seq Length:\u001b[0m None\n",
            "\u001b[96mCUDA:\u001b[0m True\n",
            "---------------------------------------------------------------\n",
            "\n",
            "\u001b[94mEvaluating bias on intersentence tasks...\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2160: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
            "  FutureWarning,\n",
            "Maximum sequence length found: -inf\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "dict_keys(['bert.encoder.layer.0.output.adapters.qqp.adapter_down.0.weight', 'bert.encoder.layer.0.output.adapters.qqp.adapter_down.0.bias', 'bert.encoder.layer.0.output.adapters.qqp.adapter_up.weight', 'bert.encoder.layer.0.output.adapters.qqp.adapter_up.bias', 'bert.encoder.layer.1.output.adapters.qqp.adapter_down.0.weight', 'bert.encoder.layer.1.output.adapters.qqp.adapter_down.0.bias', 'bert.encoder.layer.1.output.adapters.qqp.adapter_up.weight', 'bert.encoder.layer.1.output.adapters.qqp.adapter_up.bias', 'bert.encoder.layer.2.output.adapters.qqp.adapter_down.0.weight', 'bert.encoder.layer.2.output.adapters.qqp.adapter_down.0.bias', 'bert.encoder.layer.2.output.adapters.qqp.adapter_up.weight', 'bert.encoder.layer.2.output.adapters.qqp.adapter_up.bias', 'bert.encoder.layer.3.output.adapters.qqp.adapter_down.0.weight', 'bert.encoder.layer.3.output.adapters.qqp.adapter_down.0.bias', 'bert.encoder.layer.3.output.adapters.qqp.adapter_up.weight', 'bert.encoder.layer.3.output.adapters.qqp.adapter_up.bias', 'bert.encoder.layer.4.output.adapters.qqp.adapter_down.0.weight', 'bert.encoder.layer.4.output.adapters.qqp.adapter_down.0.bias', 'bert.encoder.layer.4.output.adapters.qqp.adapter_up.weight', 'bert.encoder.layer.4.output.adapters.qqp.adapter_up.bias', 'bert.encoder.layer.5.output.adapters.qqp.adapter_down.0.weight', 'bert.encoder.layer.5.output.adapters.qqp.adapter_down.0.bias', 'bert.encoder.layer.5.output.adapters.qqp.adapter_up.weight', 'bert.encoder.layer.5.output.adapters.qqp.adapter_up.bias', 'bert.encoder.layer.6.output.adapters.qqp.adapter_down.0.weight', 'bert.encoder.layer.6.output.adapters.qqp.adapter_down.0.bias', 'bert.encoder.layer.6.output.adapters.qqp.adapter_up.weight', 'bert.encoder.layer.6.output.adapters.qqp.adapter_up.bias', 'bert.encoder.layer.7.output.adapters.qqp.adapter_down.0.weight', 'bert.encoder.layer.7.output.adapters.qqp.adapter_down.0.bias', 'bert.encoder.layer.7.output.adapters.qqp.adapter_up.weight', 'bert.encoder.layer.7.output.adapters.qqp.adapter_up.bias', 'bert.encoder.layer.8.output.adapters.qqp.adapter_down.0.weight', 'bert.encoder.layer.8.output.adapters.qqp.adapter_down.0.bias', 'bert.encoder.layer.8.output.adapters.qqp.adapter_up.weight', 'bert.encoder.layer.8.output.adapters.qqp.adapter_up.bias', 'bert.encoder.layer.9.output.adapters.qqp.adapter_down.0.weight', 'bert.encoder.layer.9.output.adapters.qqp.adapter_down.0.bias', 'bert.encoder.layer.9.output.adapters.qqp.adapter_up.weight', 'bert.encoder.layer.9.output.adapters.qqp.adapter_up.bias', 'bert.encoder.layer.10.output.adapters.qqp.adapter_down.0.weight', 'bert.encoder.layer.10.output.adapters.qqp.adapter_down.0.bias', 'bert.encoder.layer.10.output.adapters.qqp.adapter_up.weight', 'bert.encoder.layer.10.output.adapters.qqp.adapter_up.bias', 'bert.encoder.layer.11.output.adapters.qqp.adapter_down.0.weight', 'bert.encoder.layer.11.output.adapters.qqp.adapter_down.0.bias', 'bert.encoder.layer.11.output.adapters.qqp.adapter_up.weight', 'bert.encoder.layer.11.output.adapters.qqp.adapter_up.bias'])\n",
            "100% 6369/6369 [01:33<00:00, 68.29it/s]\n",
            "\n",
            "\u001b[91mEvaluating bias on intrasentence tasks...\u001b[0m\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "  0% 0/6318 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "100% 6318/6318 [01:36<00:00, 65.72it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLGD_HgeIqEa",
        "outputId": "c0dfec92-d0e2-4ffe-99c6-51df5d99816b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/score_evaluation.py --gold-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --predictions-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/qqppretrained_fusion/predictions.json --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/qqppretrained_fusion/summary.json"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "intrasentence\n",
            "\tgender\n",
            "\t\tCount: 765.0\n",
            "\t\tLM Score: 72.088713219148\n",
            "\t\tSS Score: 54.17778421256683\n",
            "\t\tICAT Score: 66.06529145932373\n",
            "\tprofession\n",
            "\t\tCount: 2430.0\n",
            "\t\tLM Score: 67.22804650620027\n",
            "\t\tSS Score: 52.06571198535922\n",
            "\t\tICAT Score: 64.45057087779738\n",
            "\trace\n",
            "\t\tCount: 2886.0\n",
            "\t\tLM Score: 70.97110562941259\n",
            "\t\tSS Score: 49.046168039671294\n",
            "\t\tICAT Score: 69.61721545322862\n",
            "\treligion\n",
            "\t\tCount: 237.0\n",
            "\t\tLM Score: 70.32183908045977\n",
            "\t\tSS Score: 39.08045977011494\n",
            "\t\tICAT Score: 54.96419606288809\n",
            "\toverall\n",
            "\t\tCount: 2106.0\n",
            "\t\tLM Score: 69.6665043959205\n",
            "\t\tSS Score: 50.463957347151336\n",
            "\t\tICAT Score: 69.02005866462373\n",
            "intersentence\n",
            "\tgender\n",
            "\t\tCount: 726.0\n",
            "\t\tLM Score: 16.897296544035676\n",
            "\t\tSS Score: 43.106673921891314\n",
            "\t\tICAT Score: 14.567725045704938\n",
            "\tprofession\n",
            "\t\tCount: 2481.0\n",
            "\t\tLM Score: 28.569382673109036\n",
            "\t\tSS Score: 43.65588796597323\n",
            "\t\tICAT Score: 24.944435384685296\n",
            "\trace\n",
            "\t\tCount: 2928.0\n",
            "\t\tLM Score: 25.61839390256767\n",
            "\t\tSS Score: 45.72668508588048\n",
            "\t\tICAT Score: 23.428884607775053\n",
            "\treligion\n",
            "\t\tCount: 234.0\n",
            "\t\tLM Score: 27.364942528735636\n",
            "\t\tSS Score: 45.13409961685824\n",
            "\t\tICAT Score: 24.701840842031093\n",
            "\toverall\n",
            "\t\tCount: 2123.0\n",
            "\t\tLM Score: 25.701410806484446\n",
            "\t\tSS Score: 44.586156204308644\n",
            "\t\tICAT Score: 22.918542337780433\n",
            "overall\n",
            "\tCount: 4229.0\n",
            "\tLM Score: 47.62006391934997\n",
            "\tSS Score: 47.519252273518354\n",
            "\tICAT Score: 45.2573966132932\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "For2sjt8I3LJ"
      },
      "source": [
        "# Single adapter qnli"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rdmUoAjI19i"
      },
      "source": [
        "import os\n",
        "try:\n",
        "    os.makedirs('/content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/qnliretrained_fusion/')\n",
        "except OSError as e:\n",
        "        pass"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyiOmBDLJFaQ",
        "outputId": "f8cc30cd-8a8d-4612-c509-2ba3003d3b2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/evaluation.py --load-path_model /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/qnlipretrained/pytorch_adapter.bin --load-path_adapter /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/qnlipretrained/pytorch_adapter.bin --input-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --output-dir /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/qqppretrained_fusion/ --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/qqppretrained_fusion/predictions.json\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json...\n",
            "---------------------------------------------------------------\n",
            "\u001b[96m                     ARGUMENTS                 \u001b[0m\n",
            "\u001b[96mPretrained Model:\u001b[0m /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/qnlipretrained/pytorch_adapter.bin\n",
            "\u001b[96mSkip Intrasentence:\u001b[0m False\n",
            "\u001b[96mSkip Intersentence:\u001b[0m False\n",
            "\u001b[96mBatch Size:\u001b[0m 1\n",
            "\u001b[96mMax Seq Length:\u001b[0m None\n",
            "\u001b[96mCUDA:\u001b[0m True\n",
            "---------------------------------------------------------------\n",
            "\n",
            "\u001b[94mEvaluating bias on intersentence tasks...\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2160: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
            "  FutureWarning,\n",
            "Maximum sequence length found: -inf\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "dict_keys(['bert.encoder.layer.0.output.adapters.qnli.adapter_down.0.weight', 'bert.encoder.layer.0.output.adapters.qnli.adapter_down.0.bias', 'bert.encoder.layer.0.output.adapters.qnli.adapter_up.weight', 'bert.encoder.layer.0.output.adapters.qnli.adapter_up.bias', 'bert.encoder.layer.1.output.adapters.qnli.adapter_down.0.weight', 'bert.encoder.layer.1.output.adapters.qnli.adapter_down.0.bias', 'bert.encoder.layer.1.output.adapters.qnli.adapter_up.weight', 'bert.encoder.layer.1.output.adapters.qnli.adapter_up.bias', 'bert.encoder.layer.2.output.adapters.qnli.adapter_down.0.weight', 'bert.encoder.layer.2.output.adapters.qnli.adapter_down.0.bias', 'bert.encoder.layer.2.output.adapters.qnli.adapter_up.weight', 'bert.encoder.layer.2.output.adapters.qnli.adapter_up.bias', 'bert.encoder.layer.3.output.adapters.qnli.adapter_down.0.weight', 'bert.encoder.layer.3.output.adapters.qnli.adapter_down.0.bias', 'bert.encoder.layer.3.output.adapters.qnli.adapter_up.weight', 'bert.encoder.layer.3.output.adapters.qnli.adapter_up.bias', 'bert.encoder.layer.4.output.adapters.qnli.adapter_down.0.weight', 'bert.encoder.layer.4.output.adapters.qnli.adapter_down.0.bias', 'bert.encoder.layer.4.output.adapters.qnli.adapter_up.weight', 'bert.encoder.layer.4.output.adapters.qnli.adapter_up.bias', 'bert.encoder.layer.5.output.adapters.qnli.adapter_down.0.weight', 'bert.encoder.layer.5.output.adapters.qnli.adapter_down.0.bias', 'bert.encoder.layer.5.output.adapters.qnli.adapter_up.weight', 'bert.encoder.layer.5.output.adapters.qnli.adapter_up.bias', 'bert.encoder.layer.6.output.adapters.qnli.adapter_down.0.weight', 'bert.encoder.layer.6.output.adapters.qnli.adapter_down.0.bias', 'bert.encoder.layer.6.output.adapters.qnli.adapter_up.weight', 'bert.encoder.layer.6.output.adapters.qnli.adapter_up.bias', 'bert.encoder.layer.7.output.adapters.qnli.adapter_down.0.weight', 'bert.encoder.layer.7.output.adapters.qnli.adapter_down.0.bias', 'bert.encoder.layer.7.output.adapters.qnli.adapter_up.weight', 'bert.encoder.layer.7.output.adapters.qnli.adapter_up.bias', 'bert.encoder.layer.8.output.adapters.qnli.adapter_down.0.weight', 'bert.encoder.layer.8.output.adapters.qnli.adapter_down.0.bias', 'bert.encoder.layer.8.output.adapters.qnli.adapter_up.weight', 'bert.encoder.layer.8.output.adapters.qnli.adapter_up.bias', 'bert.encoder.layer.9.output.adapters.qnli.adapter_down.0.weight', 'bert.encoder.layer.9.output.adapters.qnli.adapter_down.0.bias', 'bert.encoder.layer.9.output.adapters.qnli.adapter_up.weight', 'bert.encoder.layer.9.output.adapters.qnli.adapter_up.bias', 'bert.encoder.layer.10.output.adapters.qnli.adapter_down.0.weight', 'bert.encoder.layer.10.output.adapters.qnli.adapter_down.0.bias', 'bert.encoder.layer.10.output.adapters.qnli.adapter_up.weight', 'bert.encoder.layer.10.output.adapters.qnli.adapter_up.bias', 'bert.encoder.layer.11.output.adapters.qnli.adapter_down.0.weight', 'bert.encoder.layer.11.output.adapters.qnli.adapter_down.0.bias', 'bert.encoder.layer.11.output.adapters.qnli.adapter_up.weight', 'bert.encoder.layer.11.output.adapters.qnli.adapter_up.bias'])\n",
            "100% 6369/6369 [01:34<00:00, 67.21it/s]\n",
            "\n",
            "\u001b[91mEvaluating bias on intrasentence tasks...\u001b[0m\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "  0% 0/6318 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "100% 6318/6318 [01:35<00:00, 65.85it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "td8MLuySJSB_",
        "outputId": "0e93d1d9-e23e-418f-9366-d43b48350842",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/score_evaluation.py --gold-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --predictions-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/qnlipretrained_fusion/predictions.json --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/qnlipretrained_fusion/summary.json"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "intrasentence\n",
            "\tgender\n",
            "\t\tCount: 765.0\n",
            "\t\tLM Score: 50.05058387667084\n",
            "\t\tSS Score: 53.366799705930134\n",
            "\t\tICAT Score: 46.6803780551187\n",
            "\tprofession\n",
            "\t\tCount: 2430.0\n",
            "\t\tLM Score: 45.353249089552214\n",
            "\t\tSS Score: 54.11633853083108\n",
            "\t\tICAT Score: 41.61946255503815\n",
            "\trace\n",
            "\t\tCount: 2886.0\n",
            "\t\tLM Score: 45.765297565591645\n",
            "\t\tSS Score: 52.34861033738409\n",
            "\t\tICAT Score: 43.6156005464715\n",
            "\treligion\n",
            "\t\tCount: 237.0\n",
            "\t\tLM Score: 40.735632183908045\n",
            "\t\tSS Score: 64.6896551724138\n",
            "\t\tICAT Score: 28.767784383670232\n",
            "\toverall\n",
            "\t\tCount: 2106.0\n",
            "\t\tLM Score: 45.96026481476326\n",
            "\t\tSS Score: 53.617431527181054\n",
            "\t\tICAT Score: 42.63510259599297\n",
            "intersentence\n",
            "\tgender\n",
            "\t\tCount: 726.0\n",
            "\t\tLM Score: 84.77837279467713\n",
            "\t\tSS Score: 53.56700465396117\n",
            "\t\tICAT Score: 78.73027578839975\n",
            "\tprofession\n",
            "\t\tCount: 2481.0\n",
            "\t\tLM Score: 78.92132085940948\n",
            "\t\tSS Score: 53.75636981634386\n",
            "\t\tICAT Score: 72.99216750856398\n",
            "\trace\n",
            "\t\tCount: 2928.0\n",
            "\t\tLM Score: 84.85946695940827\n",
            "\t\tSS Score: 54.275050186016074\n",
            "\t\tICAT Score: 77.60389735920741\n",
            "\treligion\n",
            "\t\tCount: 234.0\n",
            "\t\tLM Score: 84.50862068965517\n",
            "\t\tSS Score: 58.019157088122604\n",
            "\t\tICAT Score: 70.95486259743691\n",
            "\toverall\n",
            "\t\tCount: 2123.0\n",
            "\t\tLM Score: 82.58088640932554\n",
            "\t\tSS Score: 54.13063821507435\n",
            "\t\tICAT Score: 75.75865110458406\n",
            "overall\n",
            "\tCount: 4229.0\n",
            "\tLM Score: 64.34010845981645\n",
            "\tSS Score: 53.847847999084905\n",
            "\tICAT Score: 59.38868930785623\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zthE4Z8F7ZPJ"
      },
      "source": [
        "# single imdb pretrained adapter\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmzhKw5N7X5W"
      },
      "source": [
        "import os\n",
        "try:\n",
        "    os.makedirs('/content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/imdb-pretrained/')\n",
        "except OSError as e:\n",
        "        pass"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0ELKp5M7leE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c29f3e13-c699-4716-d4bf-3f19eb98e4e0"
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/evaluation.py --load-path_model /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/imdb-pretrained/pytorch_adapter.bin --load-path_adapter /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/imdb-pretrained/pytorch_adapter.bin --input-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --output-dir /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/imdb-pretrained/ --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/imdb-pretrained/predictions.json\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json...\n",
            "---------------------------------------------------------------\n",
            "\u001b[96m                     ARGUMENTS                 \u001b[0m\n",
            "\u001b[96mPretrained Model:\u001b[0m /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/imdb-pretrained/pytorch_adapter.bin\n",
            "\u001b[96mSkip Intrasentence:\u001b[0m False\n",
            "\u001b[96mSkip Intersentence:\u001b[0m False\n",
            "\u001b[96mBatch Size:\u001b[0m 1\n",
            "\u001b[96mMax Seq Length:\u001b[0m None\n",
            "\u001b[96mCUDA:\u001b[0m True\n",
            "---------------------------------------------------------------\n",
            "\n",
            "\u001b[94mEvaluating bias on intersentence tasks...\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2160: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
            "  FutureWarning,\n",
            "Maximum sequence length found: -inf\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "dict_keys(['bert.encoder.layer.0.output.adapters.imdb.adapter_down.0.weight', 'bert.encoder.layer.0.output.adapters.imdb.adapter_down.0.bias', 'bert.encoder.layer.0.output.adapters.imdb.adapter_up.weight', 'bert.encoder.layer.0.output.adapters.imdb.adapter_up.bias', 'bert.encoder.layer.1.output.adapters.imdb.adapter_down.0.weight', 'bert.encoder.layer.1.output.adapters.imdb.adapter_down.0.bias', 'bert.encoder.layer.1.output.adapters.imdb.adapter_up.weight', 'bert.encoder.layer.1.output.adapters.imdb.adapter_up.bias', 'bert.encoder.layer.2.output.adapters.imdb.adapter_down.0.weight', 'bert.encoder.layer.2.output.adapters.imdb.adapter_down.0.bias', 'bert.encoder.layer.2.output.adapters.imdb.adapter_up.weight', 'bert.encoder.layer.2.output.adapters.imdb.adapter_up.bias', 'bert.encoder.layer.3.output.adapters.imdb.adapter_down.0.weight', 'bert.encoder.layer.3.output.adapters.imdb.adapter_down.0.bias', 'bert.encoder.layer.3.output.adapters.imdb.adapter_up.weight', 'bert.encoder.layer.3.output.adapters.imdb.adapter_up.bias', 'bert.encoder.layer.4.output.adapters.imdb.adapter_down.0.weight', 'bert.encoder.layer.4.output.adapters.imdb.adapter_down.0.bias', 'bert.encoder.layer.4.output.adapters.imdb.adapter_up.weight', 'bert.encoder.layer.4.output.adapters.imdb.adapter_up.bias', 'bert.encoder.layer.5.output.adapters.imdb.adapter_down.0.weight', 'bert.encoder.layer.5.output.adapters.imdb.adapter_down.0.bias', 'bert.encoder.layer.5.output.adapters.imdb.adapter_up.weight', 'bert.encoder.layer.5.output.adapters.imdb.adapter_up.bias', 'bert.encoder.layer.6.output.adapters.imdb.adapter_down.0.weight', 'bert.encoder.layer.6.output.adapters.imdb.adapter_down.0.bias', 'bert.encoder.layer.6.output.adapters.imdb.adapter_up.weight', 'bert.encoder.layer.6.output.adapters.imdb.adapter_up.bias', 'bert.encoder.layer.7.output.adapters.imdb.adapter_down.0.weight', 'bert.encoder.layer.7.output.adapters.imdb.adapter_down.0.bias', 'bert.encoder.layer.7.output.adapters.imdb.adapter_up.weight', 'bert.encoder.layer.7.output.adapters.imdb.adapter_up.bias', 'bert.encoder.layer.8.output.adapters.imdb.adapter_down.0.weight', 'bert.encoder.layer.8.output.adapters.imdb.adapter_down.0.bias', 'bert.encoder.layer.8.output.adapters.imdb.adapter_up.weight', 'bert.encoder.layer.8.output.adapters.imdb.adapter_up.bias', 'bert.encoder.layer.9.output.adapters.imdb.adapter_down.0.weight', 'bert.encoder.layer.9.output.adapters.imdb.adapter_down.0.bias', 'bert.encoder.layer.9.output.adapters.imdb.adapter_up.weight', 'bert.encoder.layer.9.output.adapters.imdb.adapter_up.bias', 'bert.encoder.layer.10.output.adapters.imdb.adapter_down.0.weight', 'bert.encoder.layer.10.output.adapters.imdb.adapter_down.0.bias', 'bert.encoder.layer.10.output.adapters.imdb.adapter_up.weight', 'bert.encoder.layer.10.output.adapters.imdb.adapter_up.bias', 'bert.encoder.layer.11.output.adapters.imdb.adapter_down.0.weight', 'bert.encoder.layer.11.output.adapters.imdb.adapter_down.0.bias', 'bert.encoder.layer.11.output.adapters.imdb.adapter_up.weight', 'bert.encoder.layer.11.output.adapters.imdb.adapter_up.bias'])\n",
            "100% 6369/6369 [01:33<00:00, 67.82it/s]\n",
            "\n",
            "\u001b[91mEvaluating bias on intrasentence tasks...\u001b[0m\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "100% 6318/6318 [01:38<00:00, 64.43it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHYVNTG3792Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efcb75bc-1c7d-4580-bd6a-78aadc3f4736"
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/score_evaluation.py --gold-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --predictions-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/imdb-pretrained/predictions.json --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/imdb-pretrained/summary.json"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "intrasentence\n",
            "\tgender\n",
            "\t\tCount: 765.0\n",
            "\t\tLM Score: 67.99342774777558\n",
            "\t\tSS Score: 56.46688544949414\n",
            "\t\tICAT Score: 59.19931357650916\n",
            "\tprofession\n",
            "\t\tCount: 2430.0\n",
            "\t\tLM Score: 70.50531021841213\n",
            "\t\tSS Score: 49.83146437296077\n",
            "\t\tICAT Score: 70.26765708506701\n",
            "\trace\n",
            "\t\tCount: 2886.0\n",
            "\t\tLM Score: 75.32071601331964\n",
            "\t\tSS Score: 52.48366641662132\n",
            "\t\tICAT Score: 71.57928535655657\n",
            "\treligion\n",
            "\t\tCount: 237.0\n",
            "\t\tLM Score: 70.62068965517241\n",
            "\t\tSS Score: 48.64367816091954\n",
            "\t\tICAT Score: 68.70500198176774\n",
            "\toverall\n",
            "\t\tCount: 2106.0\n",
            "\t\tLM Score: 72.38609404398916\n",
            "\t\tSS Score: 51.834883685631524\n",
            "\t\tICAT Score: 69.72969278343106\n",
            "intersentence\n",
            "\tgender\n",
            "\t\tCount: 726.0\n",
            "\t\tLM Score: 88.74591893613633\n",
            "\t\tSS Score: 56.66713117800074\n",
            "\t\tICAT Score: 76.91230527494753\n",
            "\tprofession\n",
            "\t\tCount: 2481.0\n",
            "\t\tLM Score: 83.93196027389577\n",
            "\t\tSS Score: 62.27854931840842\n",
            "\t\tICAT Score: 63.320706001621254\n",
            "\trace\n",
            "\t\tCount: 2928.0\n",
            "\t\tLM Score: 86.86347300139631\n",
            "\t\tSS Score: 56.931279149561796\n",
            "\t\tICAT Score: 74.82197341593427\n",
            "\treligion\n",
            "\t\tCount: 234.0\n",
            "\t\tLM Score: 90.61973180076627\n",
            "\t\tSS Score: 60.90804597701149\n",
            "\t\tICAT Score: 70.8500477826221\n",
            "\toverall\n",
            "\t\tCount: 2123.0\n",
            "\t\tLM Score: 86.13116735482029\n",
            "\t\tSS Score: 59.0794680841458\n",
            "\t\tICAT Score: 70.49066365385406\n",
            "overall\n",
            "\tCount: 4229.0\n",
            "\tLM Score: 79.24478677589163\n",
            "\tSS Score: 55.43269380956585\n",
            "\tICAT Score: 70.63453352473658\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7yU1c9ugamN"
      },
      "source": [
        "## SST Adapter\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nsl9kUigiOxz"
      },
      "source": [
        "import os\n",
        "try:\n",
        "    os.makedirs('/content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sst/')\n",
        "except OSError as e:\n",
        "        pass"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYyEJ2uFW40w",
        "outputId": "b658232c-55d7-4244-a5e9-931276035e6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/evaluation.py --load-path_model /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/sst/pytorch_model.bin --load-path_adapter /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/sst/pytorch_adapter.bin --input-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --output-dir /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sst/ --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sst/predictions.json\n",
        "#!python3 evaluation.py --no-cuda False --output-dir /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/ -- skip_intrasentence False --skip_intersentence False --batch_size 1"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json...\n",
            "---------------------------------------------------------------\n",
            "\u001b[96m                     ARGUMENTS                 \u001b[0m\n",
            "\u001b[96mPretrained Model:\u001b[0m /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/sst/pytorch_model.bin\n",
            "\u001b[96mSkip Intrasentence:\u001b[0m False\n",
            "\u001b[96mSkip Intersentence:\u001b[0m False\n",
            "\u001b[96mBatch Size:\u001b[0m 1\n",
            "\u001b[96mMax Seq Length:\u001b[0m None\n",
            "\u001b[96mCUDA:\u001b[0m True\n",
            "---------------------------------------------------------------\n",
            "\n",
            "\u001b[94mEvaluating bias on intersentence tasks...\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2160: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
            "  FutureWarning,\n",
            "Maximum sequence length found: -inf\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "odict_keys(['bert.embeddings.position_ids', 'bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.0.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.0.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.0.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.0.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.1.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.1.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.1.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.1.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.2.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.2.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.2.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.2.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.3.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.3.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.3.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.3.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.4.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.4.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.4.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.4.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.5.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.5.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.5.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.5.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.6.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.6.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.6.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.6.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.7.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.7.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.7.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.7.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.8.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.8.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.8.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.8.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.9.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.9.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.9.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.9.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.10.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.10.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.10.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.10.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.11.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.11.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.11.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.11.output.adapters.sst-2.adapter_up.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'classifier.weight', 'classifier.bias'])\n",
            "100% 6369/6369 [01:34<00:00, 67.18it/s]\n",
            "\n",
            "\u001b[91mEvaluating bias on intrasentence tasks...\u001b[0m\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "100% 6318/6318 [01:36<00:00, 65.41it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDw_upEajSLL",
        "outputId": "99e977ed-2af3-4760-9024-1a8a1a2d197c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/score_evaluation.py --gold-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --predictions-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sst/predictions.json --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sst/summary.json"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "intrasentence\n",
            "\tgender\n",
            "\t\tCount: 765.0\n",
            "\t\tLM Score: 38.30568827307958\n",
            "\t\tSS Score: 44.625063020715196\n",
            "\t\tICAT Score: 34.187875064760945\n",
            "\tprofession\n",
            "\t\tCount: 2430.0\n",
            "\t\tLM Score: 39.84723501106721\n",
            "\t\tSS Score: 46.79135853335165\n",
            "\t\tICAT Score: 37.29012519931137\n",
            "\trace\n",
            "\t\tCount: 2886.0\n",
            "\t\tLM Score: 28.93242416090031\n",
            "\t\tSS Score: 40.62323208716679\n",
            "\t\tICAT Score: 23.506571630652104\n",
            "\treligion\n",
            "\t\tCount: 237.0\n",
            "\t\tLM Score: 36.55172413793104\n",
            "\t\tSS Score: 41.97701149425287\n",
            "\t\tICAT Score: 30.686642885453825\n",
            "\toverall\n",
            "\t\tCount: 2106.0\n",
            "\t\tLM Score: 34.55311867429135\n",
            "\t\tSS Score: 43.523528807955245\n",
            "\t\tICAT Score: 30.07747312050432\n",
            "intersentence\n",
            "\tgender\n",
            "\t\tCount: 726.0\n",
            "\t\tLM Score: 65.55938224416487\n",
            "\t\tSS Score: 52.93838810143158\n",
            "\t\tICAT Score: 61.70660406969569\n",
            "\tprofession\n",
            "\t\tCount: 2481.0\n",
            "\t\tLM Score: 59.47469206729497\n",
            "\t\tSS Score: 54.136704893453135\n",
            "\t\tICAT Score: 54.554107073067016\n",
            "\trace\n",
            "\t\tCount: 2928.0\n",
            "\t\tLM Score: 59.99015062107541\n",
            "\t\tSS Score: 51.47457000627189\n",
            "\t\tICAT Score: 58.22095708552399\n",
            "\treligion\n",
            "\t\tCount: 234.0\n",
            "\t\tLM Score: 55.81992337164752\n",
            "\t\tSS Score: 52.5191570881226\n",
            "\t\tICAT Score: 53.0075402592446\n",
            "\toverall\n",
            "\t\tCount: 2123.0\n",
            "\t\tLM Score: 60.34100983460956\n",
            "\t\tSS Score: 52.71046859883627\n",
            "\t\tICAT Score: 57.06996158703396\n",
            "overall\n",
            "\tCount: 4229.0\n",
            "\tLM Score: 47.5741541681351\n",
            "\tSS Score: 48.09523658399092\n",
            "\tICAT Score: 45.76180399999431\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDFKHWqKlWRE"
      },
      "source": [
        "# RoBERTa\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDjpLNYqlU2x"
      },
      "source": [
        "import os\n",
        "try:\n",
        "    os.makedirs('/content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/roberta/')\n",
        "except OSError as e:\n",
        "        pass"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFZPbqQnfV-v",
        "outputId": "3de18adc-502d-4515-af1d-2f6aa2c98547",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/evaluation.py --load-path_model /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/roberta/pytorch_model_head.bin --load-path_adapter /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/roberta/pytorch_adapter.bin --input-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --output-dir /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/roberta/ --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/roberta/predictions.json\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json...\n",
            "---------------------------------------------------------------\n",
            "\u001b[96m                     ARGUMENTS                 \u001b[0m\n",
            "\u001b[96mPretrained Model:\u001b[0m /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/roberta/pytorch_model_head.bin\n",
            "\u001b[96mSkip Intrasentence:\u001b[0m False\n",
            "\u001b[96mSkip Intersentence:\u001b[0m False\n",
            "\u001b[96mBatch Size:\u001b[0m 1\n",
            "\u001b[96mMax Seq Length:\u001b[0m None\n",
            "\u001b[96mCUDA:\u001b[0m True\n",
            "---------------------------------------------------------------\n",
            "\n",
            "\u001b[94mEvaluating bias on intersentence tasks...\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2160: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
            "  FutureWarning,\n",
            "Maximum sequence length found: -inf\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "dict_keys(['heads.rotten_tomatoes.1.weight', 'heads.rotten_tomatoes.1.bias', 'heads.rotten_tomatoes.4.weight', 'heads.rotten_tomatoes.4.bias'])\n",
            "100% 6369/6369 [01:36<00:00, 66.19it/s]\n",
            "\n",
            "\u001b[91mEvaluating bias on intrasentence tasks...\u001b[0m\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "100% 6318/6318 [01:36<00:00, 65.70it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAxTHS7ylu5C",
        "outputId": "5e080470-5d85-4c38-c230-9512b1034a34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/score_evaluation.py --gold-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --predictions-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/roberta/predictions.json --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/roberta/summary.json"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "intrasentence\n",
            "\tgender\n",
            "\t\tCount: 765.0\n",
            "\t\tLM Score: 68.40744915310133\n",
            "\t\tSS Score: 49.01477202346767\n",
            "\t\tICAT Score: 67.05951049892437\n",
            "\tprofession\n",
            "\t\tCount: 2430.0\n",
            "\t\tLM Score: 59.50442556364201\n",
            "\t\tSS Score: 47.25144596464158\n",
            "\t\tICAT Score: 56.233402983549354\n",
            "\trace\n",
            "\t\tCount: 2886.0\n",
            "\t\tLM Score: 51.46264703216101\n",
            "\t\tSS Score: 58.34372445700384\n",
            "\t\tICAT Score: 42.874844098873055\n",
            "\treligion\n",
            "\t\tCount: 237.0\n",
            "\t\tLM Score: 54.27586206896552\n",
            "\t\tSS Score: 55.356321839080465\n",
            "\t\tICAT Score: 48.46148236226714\n",
            "\toverall\n",
            "\t\tCount: 2106.0\n",
            "\t\tLM Score: 56.768229592467925\n",
            "\t\tSS Score: 52.837141077763334\n",
            "\t\tICAT Score: 53.54704007069411\n",
            "intersentence\n",
            "\tgender\n",
            "\t\tCount: 726.0\n",
            "\t\tLM Score: 89.437720089894\n",
            "\t\tSS Score: 59.89026472722124\n",
            "\t\tICAT Score: 71.7464655241307\n",
            "\tprofession\n",
            "\t\tCount: 2481.0\n",
            "\t\tLM Score: 85.65251738298642\n",
            "\t\tSS Score: 58.03529475509454\n",
            "\t\tICAT Score: 71.88765290922332\n",
            "\trace\n",
            "\t\tCount: 2928.0\n",
            "\t\tLM Score: 87.07947275405716\n",
            "\t\tSS Score: 55.666699804244615\n",
            "\t\tICAT Score: 77.21040812987437\n",
            "\treligion\n",
            "\t\tCount: 234.0\n",
            "\t\tLM Score: 86.75670498084291\n",
            "\t\tSS Score: 52.45593869731801\n",
            "\t\tICAT Score: 82.49532200055782\n",
            "\toverall\n",
            "\t\tCount: 2123.0\n",
            "\t\tLM Score: 86.8238462845205\n",
            "\t\tSS Score: 56.9788670755672\n",
            "\t\tICAT Score: 74.70520464033754\n",
            "overall\n",
            "\tCount: 4229.0\n",
            "\tLM Score: 71.8745127184105\n",
            "\tSS Score: 54.85009245982011\n",
            "\tICAT Score: 64.90255207463436\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0YbepOXJsoO"
      },
      "source": [
        "#Language Poem (pre-trained)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4SnA2w_JyUO"
      },
      "source": [
        "import os\n",
        "try:\n",
        "    os.makedirs('/content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/lmpoem/')\n",
        "except OSError as e:\n",
        "        pass"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3B6lKsCQJ53J",
        "outputId": "27e6dd61-a4d0-43c9-8182-18ab278aa465",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/evaluation.py --load-path_model /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/lmpoem/pytorch_adapter.bin --load-path_adapter /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/lmpoem/pytorch_adapter.bin --input-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --output-dir /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/lmpoem/ --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/lmpoem/predictions.json\n",
        "#!python3 evaluation.py --no-cuda False --output-dir /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/ -- skip_intrasentence False --skip_intersentence False --batch_size 1"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json...\n",
            "---------------------------------------------------------------\n",
            "\u001b[96m                     ARGUMENTS                 \u001b[0m\n",
            "\u001b[96mPretrained Model:\u001b[0m /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/lmpoem/pytorch_adapter.bin\n",
            "\u001b[96mSkip Intrasentence:\u001b[0m False\n",
            "\u001b[96mSkip Intersentence:\u001b[0m False\n",
            "\u001b[96mBatch Size:\u001b[0m 1\n",
            "\u001b[96mMax Seq Length:\u001b[0m None\n",
            "\u001b[96mCUDA:\u001b[0m True\n",
            "---------------------------------------------------------------\n",
            "\n",
            "\u001b[94mEvaluating bias on intersentence tasks...\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2160: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
            "  FutureWarning,\n",
            "Maximum sequence length found: -inf\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "dict_keys(['transformer.h.0.output_adapters.adapters.poem.adapter_down.0.weight', 'transformer.h.0.output_adapters.adapters.poem.adapter_down.0.bias', 'transformer.h.0.output_adapters.adapters.poem.adapter_up.weight', 'transformer.h.0.output_adapters.adapters.poem.adapter_up.bias', 'transformer.h.1.output_adapters.adapters.poem.adapter_down.0.weight', 'transformer.h.1.output_adapters.adapters.poem.adapter_down.0.bias', 'transformer.h.1.output_adapters.adapters.poem.adapter_up.weight', 'transformer.h.1.output_adapters.adapters.poem.adapter_up.bias', 'transformer.h.2.output_adapters.adapters.poem.adapter_down.0.weight', 'transformer.h.2.output_adapters.adapters.poem.adapter_down.0.bias', 'transformer.h.2.output_adapters.adapters.poem.adapter_up.weight', 'transformer.h.2.output_adapters.adapters.poem.adapter_up.bias', 'transformer.h.3.output_adapters.adapters.poem.adapter_down.0.weight', 'transformer.h.3.output_adapters.adapters.poem.adapter_down.0.bias', 'transformer.h.3.output_adapters.adapters.poem.adapter_up.weight', 'transformer.h.3.output_adapters.adapters.poem.adapter_up.bias', 'transformer.h.4.output_adapters.adapters.poem.adapter_down.0.weight', 'transformer.h.4.output_adapters.adapters.poem.adapter_down.0.bias', 'transformer.h.4.output_adapters.adapters.poem.adapter_up.weight', 'transformer.h.4.output_adapters.adapters.poem.adapter_up.bias', 'transformer.h.5.output_adapters.adapters.poem.adapter_down.0.weight', 'transformer.h.5.output_adapters.adapters.poem.adapter_down.0.bias', 'transformer.h.5.output_adapters.adapters.poem.adapter_up.weight', 'transformer.h.5.output_adapters.adapters.poem.adapter_up.bias', 'transformer.h.6.output_adapters.adapters.poem.adapter_down.0.weight', 'transformer.h.6.output_adapters.adapters.poem.adapter_down.0.bias', 'transformer.h.6.output_adapters.adapters.poem.adapter_up.weight', 'transformer.h.6.output_adapters.adapters.poem.adapter_up.bias', 'transformer.h.7.output_adapters.adapters.poem.adapter_down.0.weight', 'transformer.h.7.output_adapters.adapters.poem.adapter_down.0.bias', 'transformer.h.7.output_adapters.adapters.poem.adapter_up.weight', 'transformer.h.7.output_adapters.adapters.poem.adapter_up.bias', 'transformer.h.8.output_adapters.adapters.poem.adapter_down.0.weight', 'transformer.h.8.output_adapters.adapters.poem.adapter_down.0.bias', 'transformer.h.8.output_adapters.adapters.poem.adapter_up.weight', 'transformer.h.8.output_adapters.adapters.poem.adapter_up.bias', 'transformer.h.9.output_adapters.adapters.poem.adapter_down.0.weight', 'transformer.h.9.output_adapters.adapters.poem.adapter_down.0.bias', 'transformer.h.9.output_adapters.adapters.poem.adapter_up.weight', 'transformer.h.9.output_adapters.adapters.poem.adapter_up.bias', 'transformer.h.10.output_adapters.adapters.poem.adapter_down.0.weight', 'transformer.h.10.output_adapters.adapters.poem.adapter_down.0.bias', 'transformer.h.10.output_adapters.adapters.poem.adapter_up.weight', 'transformer.h.10.output_adapters.adapters.poem.adapter_up.bias', 'transformer.h.11.output_adapters.adapters.poem.adapter_down.0.weight', 'transformer.h.11.output_adapters.adapters.poem.adapter_down.0.bias', 'transformer.h.11.output_adapters.adapters.poem.adapter_up.weight', 'transformer.h.11.output_adapters.adapters.poem.adapter_up.bias'])\n",
            "100% 6369/6369 [01:36<00:00, 65.96it/s]\n",
            "\n",
            "\u001b[91mEvaluating bias on intrasentence tasks...\u001b[0m\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "100% 6318/6318 [01:37<00:00, 64.92it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPnSIwspJ8wX",
        "outputId": "7c35ab71-7ba8-4054-eeff-c73cb8252641",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/score_evaluation.py --gold-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --predictions-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/lmpoem/predictions.json --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/lmpoem/summary.json"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "intrasentence\n",
            "\tgender\n",
            "\t\tCount: 765.0\n",
            "\t\tLM Score: 28.55930872670003\n",
            "\t\tSS Score: 44.416736081953474\n",
            "\t\tICAT Score: 25.370225567937318\n",
            "\tprofession\n",
            "\t\tCount: 2430.0\n",
            "\t\tLM Score: 29.12896589790824\n",
            "\t\tSS Score: 53.34127320628487\n",
            "\t\tICAT Score: 27.182409232278914\n",
            "\trace\n",
            "\t\tCount: 2886.0\n",
            "\t\tLM Score: 24.321422012348723\n",
            "\t\tSS Score: 55.55803745663722\n",
            "\t\tICAT Score: 21.61783452148242\n",
            "\treligion\n",
            "\t\tCount: 237.0\n",
            "\t\tLM Score: 31.28735632183908\n",
            "\t\tSS Score: 56.137931034482754\n",
            "\t\tICAT Score: 27.446563614744356\n",
            "\toverall\n",
            "\t\tCount: 2106.0\n",
            "\t\tLM Score: 26.94804209638379\n",
            "\t\tSS Score: 53.3279582094996\n",
            "\t\tICAT Score: 25.154402937891767\n",
            "intersentence\n",
            "\tgender\n",
            "\t\tCount: 726.0\n",
            "\t\tLM Score: 20.834527791049528\n",
            "\t\tSS Score: 41.53830670135018\n",
            "\t\tICAT Score: 17.308620107248384\n",
            "\tprofession\n",
            "\t\tCount: 2481.0\n",
            "\t\tLM Score: 26.82208008795884\n",
            "\t\tSS Score: 36.63794485153402\n",
            "\t\tICAT Score: 19.654117821321297\n",
            "\trace\n",
            "\t\tCount: 2928.0\n",
            "\t\tLM Score: 19.403143426093308\n",
            "\t\tSS Score: 38.96159141615513\n",
            "\t\tICAT Score: 15.119546927130077\n",
            "\treligion\n",
            "\t\tCount: 234.0\n",
            "\t\tLM Score: 19.335249042145595\n",
            "\t\tSS Score: 44.22413793103448\n",
            "\t\tICAT Score: 17.101694411414982\n",
            "\toverall\n",
            "\t\tCount: 2123.0\n",
            "\t\tLM Score: 22.399070772342483\n",
            "\t\tSS Score: 38.60520401688875\n",
            "\t\tICAT Score: 17.294413939100227\n",
            "overall\n",
            "\tCount: 4229.0\n",
            "\tLM Score: 24.686781169464883\n",
            "\tSS Score: 45.88530031274773\n",
            "\tICAT Score: 22.655207354319636\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjiwCTcAQJib"
      },
      "source": [
        "#Common Sense Winogrande (pre-trained)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iyxrzgq4QQtK"
      },
      "source": [
        "import os\n",
        "try:\n",
        "    os.makedirs('/content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/winogrande/')\n",
        "except OSError as e:\n",
        "        pass"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VOY-ZXwQVoO",
        "outputId": "b23595b0-1547-464d-e4c6-bb45ae227966",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/evaluation.py --load-path_model /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/winogrande/pytorch_adapter.bin --load-path_adapter /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/winogrande/pytorch_adapter.bin --input-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --output-dir /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/winogrande/ --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/winogrande/predictions.json\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json...\n",
            "---------------------------------------------------------------\n",
            "\u001b[96m                     ARGUMENTS                 \u001b[0m\n",
            "\u001b[96mPretrained Model:\u001b[0m /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/winogrande/pytorch_adapter.bin\n",
            "\u001b[96mSkip Intrasentence:\u001b[0m False\n",
            "\u001b[96mSkip Intersentence:\u001b[0m False\n",
            "\u001b[96mBatch Size:\u001b[0m 1\n",
            "\u001b[96mMax Seq Length:\u001b[0m None\n",
            "\u001b[96mCUDA:\u001b[0m True\n",
            "---------------------------------------------------------------\n",
            "\n",
            "\u001b[94mEvaluating bias on intersentence tasks...\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2160: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
            "  FutureWarning,\n",
            "Maximum sequence length found: -inf\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "dict_keys(['roberta.encoder.layer.0.output.adapters.winogrande.adapter_down.0.weight', 'roberta.encoder.layer.0.output.adapters.winogrande.adapter_down.0.bias', 'roberta.encoder.layer.0.output.adapters.winogrande.adapter_up.weight', 'roberta.encoder.layer.0.output.adapters.winogrande.adapter_up.bias', 'roberta.encoder.layer.1.output.adapters.winogrande.adapter_down.0.weight', 'roberta.encoder.layer.1.output.adapters.winogrande.adapter_down.0.bias', 'roberta.encoder.layer.1.output.adapters.winogrande.adapter_up.weight', 'roberta.encoder.layer.1.output.adapters.winogrande.adapter_up.bias', 'roberta.encoder.layer.2.output.adapters.winogrande.adapter_down.0.weight', 'roberta.encoder.layer.2.output.adapters.winogrande.adapter_down.0.bias', 'roberta.encoder.layer.2.output.adapters.winogrande.adapter_up.weight', 'roberta.encoder.layer.2.output.adapters.winogrande.adapter_up.bias', 'roberta.encoder.layer.3.output.adapters.winogrande.adapter_down.0.weight', 'roberta.encoder.layer.3.output.adapters.winogrande.adapter_down.0.bias', 'roberta.encoder.layer.3.output.adapters.winogrande.adapter_up.weight', 'roberta.encoder.layer.3.output.adapters.winogrande.adapter_up.bias', 'roberta.encoder.layer.4.output.adapters.winogrande.adapter_down.0.weight', 'roberta.encoder.layer.4.output.adapters.winogrande.adapter_down.0.bias', 'roberta.encoder.layer.4.output.adapters.winogrande.adapter_up.weight', 'roberta.encoder.layer.4.output.adapters.winogrande.adapter_up.bias', 'roberta.encoder.layer.5.output.adapters.winogrande.adapter_down.0.weight', 'roberta.encoder.layer.5.output.adapters.winogrande.adapter_down.0.bias', 'roberta.encoder.layer.5.output.adapters.winogrande.adapter_up.weight', 'roberta.encoder.layer.5.output.adapters.winogrande.adapter_up.bias', 'roberta.encoder.layer.6.output.adapters.winogrande.adapter_down.0.weight', 'roberta.encoder.layer.6.output.adapters.winogrande.adapter_down.0.bias', 'roberta.encoder.layer.6.output.adapters.winogrande.adapter_up.weight', 'roberta.encoder.layer.6.output.adapters.winogrande.adapter_up.bias', 'roberta.encoder.layer.7.output.adapters.winogrande.adapter_down.0.weight', 'roberta.encoder.layer.7.output.adapters.winogrande.adapter_down.0.bias', 'roberta.encoder.layer.7.output.adapters.winogrande.adapter_up.weight', 'roberta.encoder.layer.7.output.adapters.winogrande.adapter_up.bias', 'roberta.encoder.layer.8.output.adapters.winogrande.adapter_down.0.weight', 'roberta.encoder.layer.8.output.adapters.winogrande.adapter_down.0.bias', 'roberta.encoder.layer.8.output.adapters.winogrande.adapter_up.weight', 'roberta.encoder.layer.8.output.adapters.winogrande.adapter_up.bias', 'roberta.encoder.layer.9.output.adapters.winogrande.adapter_down.0.weight', 'roberta.encoder.layer.9.output.adapters.winogrande.adapter_down.0.bias', 'roberta.encoder.layer.9.output.adapters.winogrande.adapter_up.weight', 'roberta.encoder.layer.9.output.adapters.winogrande.adapter_up.bias', 'roberta.encoder.layer.10.output.adapters.winogrande.adapter_down.0.weight', 'roberta.encoder.layer.10.output.adapters.winogrande.adapter_down.0.bias', 'roberta.encoder.layer.10.output.adapters.winogrande.adapter_up.weight', 'roberta.encoder.layer.10.output.adapters.winogrande.adapter_up.bias', 'roberta.encoder.layer.11.output.adapters.winogrande.adapter_down.0.weight', 'roberta.encoder.layer.11.output.adapters.winogrande.adapter_down.0.bias', 'roberta.encoder.layer.11.output.adapters.winogrande.adapter_up.weight', 'roberta.encoder.layer.11.output.adapters.winogrande.adapter_up.bias'])\n",
            "100% 6369/6369 [01:36<00:00, 66.00it/s]\n",
            "\n",
            "\u001b[91mEvaluating bias on intrasentence tasks...\u001b[0m\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "100% 6318/6318 [01:37<00:00, 65.03it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHR_-RszQjwA",
        "outputId": "9d1f5923-ff0c-4b7d-bdc6-0d56417e3875",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/score_evaluation.py --gold-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --predictions-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/winogrande/predictions.json --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/winogrande/summary.json"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "intrasentence\n",
            "\tgender\n",
            "\t\tCount: 765.0\n",
            "\t\tLM Score: 49.095568119481165\n",
            "\t\tSS Score: 48.27848496109366\n",
            "\t\tICAT Score: 47.405192942254416\n",
            "\tprofession\n",
            "\t\tCount: 2430.0\n",
            "\t\tLM Score: 47.83155222672086\n",
            "\t\tSS Score: 47.99771756524287\n",
            "\t\tICAT Score: 45.91610668970623\n",
            "\trace\n",
            "\t\tCount: 2886.0\n",
            "\t\tLM Score: 48.51937358136241\n",
            "\t\tSS Score: 55.25993278805582\n",
            "\t\tICAT Score: 43.41520070223166\n",
            "\treligion\n",
            "\t\tCount: 237.0\n",
            "\t\tLM Score: 57.333333333333336\n",
            "\t\tSS Score: 49.05747126436782\n",
            "\t\tICAT Score: 56.252567049808434\n",
            "\toverall\n",
            "\t\tCount: 2106.0\n",
            "\t\tLM Score: 48.665818948423855\n",
            "\t\tSS Score: 51.382865452295384\n",
            "\t\tICAT Score: 47.319853353795104\n",
            "intersentence\n",
            "\tgender\n",
            "\t\tCount: 726.0\n",
            "\t\tLM Score: 71.51696129957\n",
            "\t\tSS Score: 47.03179469483817\n",
            "\t\tICAT Score: 67.27142082080125\n",
            "\tprofession\n",
            "\t\tCount: 2481.0\n",
            "\t\tLM Score: 67.17638154324287\n",
            "\t\tSS Score: 48.87386700488295\n",
            "\t\tICAT Score: 65.66339074827452\n",
            "\trace\n",
            "\t\tCount: 2928.0\n",
            "\t\tLM Score: 67.76441487269877\n",
            "\t\tSS Score: 50.66974389014311\n",
            "\t\tICAT Score: 66.8567188160965\n",
            "\treligion\n",
            "\t\tCount: 234.0\n",
            "\t\tLM Score: 68.71934865900383\n",
            "\t\tSS Score: 45.3103448275862\n",
            "\t\tICAT Score: 62.27394768133174\n",
            "\toverall\n",
            "\t\tCount: 2123.0\n",
            "\t\tLM Score: 68.05238026186272\n",
            "\t\tSS Score: 49.323743944592145\n",
            "\t\tICAT Score: 67.13196357712268\n",
            "overall\n",
            "\tCount: 4229.0\n",
            "\tLM Score: 58.40851716954809\n",
            "\tSS Score: 50.365187080320375\n",
            "\tICAT Score: 57.981916452528296\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmtvxUp-T6FJ"
      },
      "source": [
        "#SST Pretrained\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zinc5psBT8yh"
      },
      "source": [
        "import os\n",
        "try:\n",
        "    os.makedirs('/content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sst-pretrained/')\n",
        "except OSError as e:\n",
        "        pass"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAbECqkiUTS1",
        "outputId": "233fa049-190a-4cb7-c117-062d1fedc359",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/evaluation.py --load-path_model /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/sst_pretrained/pytorch_adapter.bin --load-path_adapter /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/sst_pretrained/pytorch_adapter.bin --input-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --output-dir /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sst-pretrained/ --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sst-pretrained/predictions.json\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json...\n",
            "---------------------------------------------------------------\n",
            "\u001b[96m                     ARGUMENTS                 \u001b[0m\n",
            "\u001b[96mPretrained Model:\u001b[0m /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/sst_pretrained/pytorch_adapter.bin\n",
            "\u001b[96mSkip Intrasentence:\u001b[0m False\n",
            "\u001b[96mSkip Intersentence:\u001b[0m False\n",
            "\u001b[96mBatch Size:\u001b[0m 1\n",
            "\u001b[96mMax Seq Length:\u001b[0m None\n",
            "\u001b[96mCUDA:\u001b[0m True\n",
            "---------------------------------------------------------------\n",
            "\n",
            "\u001b[94mEvaluating bias on intersentence tasks...\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2160: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
            "  FutureWarning,\n",
            "Maximum sequence length found: -inf\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "dict_keys(['bert.encoder.layer.0.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.0.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.0.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.0.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.1.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.1.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.1.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.1.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.2.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.2.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.2.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.2.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.3.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.3.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.3.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.3.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.4.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.4.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.4.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.4.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.5.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.5.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.5.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.5.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.6.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.6.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.6.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.6.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.7.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.7.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.7.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.7.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.8.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.8.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.8.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.8.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.9.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.9.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.9.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.9.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.10.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.10.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.10.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.10.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.11.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.11.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.11.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.11.output.adapters.sst-2.adapter_up.bias'])\n",
            "100% 6369/6369 [01:37<00:00, 65.64it/s]\n",
            "\n",
            "\u001b[91mEvaluating bias on intrasentence tasks...\u001b[0m\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "100% 6318/6318 [01:38<00:00, 64.10it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K60WFJrzUquL",
        "outputId": "99eefa15-174a-4607-deed-149e28d2908e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/score_evaluation.py --gold-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --predictions-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sst-pretrained/predictions.json --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sst-pretrained/summary.json"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "intrasentence\n",
            "\tgender\n",
            "\t\tCount: 765.0\n",
            "\t\tLM Score: 41.5208789600094\n",
            "\t\tSS Score: 48.71096069791722\n",
            "\t\tICAT Score: 40.45043806327992\n",
            "\tprofession\n",
            "\t\tCount: 2430.0\n",
            "\t\tLM Score: 35.43666711658744\n",
            "\t\tSS Score: 49.25408607271588\n",
            "\t\tICAT Score: 34.908013045811565\n",
            "\trace\n",
            "\t\tCount: 2886.0\n",
            "\t\tLM Score: 25.146770606552142\n",
            "\t\tSS Score: 48.42535603276806\n",
            "\t\tICAT Score: 24.354826393932683\n",
            "\treligion\n",
            "\t\tCount: 237.0\n",
            "\t\tLM Score: 35.17241379310345\n",
            "\t\tSS Score: 61.28735632183908\n",
            "\t\tICAT Score: 27.23234244946493\n",
            "\toverall\n",
            "\t\tCount: 2106.0\n",
            "\t\tLM Score: 31.50771881408739\n",
            "\t\tSS Score: 49.26464652285842\n",
            "\t\tICAT Score: 31.044332602352622\n",
            "intersentence\n",
            "\tgender\n",
            "\t\tCount: 726.0\n",
            "\t\tLM Score: 30.685519190953972\n",
            "\t\tSS Score: 49.9751154642459\n",
            "\t\tICAT Score: 30.670247292965165\n",
            "\tprofession\n",
            "\t\tCount: 2481.0\n",
            "\t\tLM Score: 25.79289660716246\n",
            "\t\tSS Score: 42.90734707553394\n",
            "\t\tICAT Score: 22.13409533613763\n",
            "\trace\n",
            "\t\tCount: 2928.0\n",
            "\t\tLM Score: 24.659931814501707\n",
            "\t\tSS Score: 45.35554758495371\n",
            "\t\tICAT Score: 22.36929421708692\n",
            "\treligion\n",
            "\t\tCount: 234.0\n",
            "\t\tLM Score: 20.6360153256705\n",
            "\t\tSS Score: 44.40804597701149\n",
            "\t\tICAT Score: 18.328102347293786\n",
            "\toverall\n",
            "\t\tCount: 2123.0\n",
            "\t\tLM Score: 25.700097233208687\n",
            "\t\tSS Score: 44.9746255430107\n",
            "\t\tICAT Score: 23.117044989650523\n",
            "overall\n",
            "\tCount: 4229.0\n",
            "\tLM Score: 28.653826024062486\n",
            "\tSS Score: 47.13431233680318\n",
            "\tICAT Score: 27.011567709251604\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}