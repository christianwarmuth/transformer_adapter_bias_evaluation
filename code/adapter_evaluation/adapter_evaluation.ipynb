{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "adapter_evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqFl7uUVgEz7"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mthG05Ijv6SZ",
        "outputId": "c37bad5d-3513-41ad-d2d4-862405753524"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Aug 11 10:53:30 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SyR5EK-hsOv",
        "outputId": "45db4725-757c-4ce4-da32-f2e31a509f4d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive/\", force_remount=True)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtXvNuOU6jjS"
      },
      "source": [
        "#!pip3 install models==0.9.3\n",
        "#!python3 -m pip install models\n",
        "\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "julO5QKjnRTA",
        "outputId": "2f29ed8d-6c43-4746-90b2-8a5170b37ae7"
      },
      "source": [
        "!pip install -U git+https://github.com/Adapter-Hub/adapter-transformers.git\n",
        "#!pip install transformers==3.0.2\n",
        "!pip3 install -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/Adapter-Hub/adapter-transformers.git\n",
            "  Cloning https://github.com/Adapter-Hub/adapter-transformers.git to /tmp/pip-req-build-71ka6wcy\n",
            "  Running command git clone -q https://github.com/Adapter-Hub/adapter-transformers.git /tmp/pip-req-build-71ka6wcy\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==2.1.2) (21.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==2.1.2) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==2.1.2) (2.23.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==2.1.2) (3.13)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 4.2 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.0.14\n",
            "  Downloading huggingface_hub-0.0.15-py3-none-any.whl (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.9 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 59.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==2.1.2) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==2.1.2) (4.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==2.1.2) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==2.1.2) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.14->adapter-transformers==2.1.2) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->adapter-transformers==2.1.2) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->adapter-transformers==2.1.2) (3.5.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers==2.1.2) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers==2.1.2) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers==2.1.2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers==2.1.2) (2021.5.30)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->adapter-transformers==2.1.2) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->adapter-transformers==2.1.2) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->adapter-transformers==2.1.2) (1.0.1)\n",
            "Building wheels for collected packages: adapter-transformers\n",
            "  Building wheel for adapter-transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for adapter-transformers: filename=adapter_transformers-2.1.2-py3-none-any.whl size=2539220 sha256=21a8ae8293eaf1656a430228918b9c00619c612674e36a94eac2853e771972c9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ty9xhe8i/wheels/60/0e/d5/86e64fef3085f149b504f48af9ca6e8f6da427f42da9b4bbe5\n",
            "Successfully built adapter-transformers\n",
            "Installing collected packages: tokenizers, sacremoses, huggingface-hub, adapter-transformers\n",
            "Successfully installed adapter-transformers-2.1.2 huggingface-hub-0.0.15 sacremoses-0.0.45 tokenizers-0.10.3\n",
            "Requirement already satisfied: blis==0.4.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 1)) (0.4.1)\n",
            "Collecting boto3==1.12.36\n",
            "  Downloading boto3-1.12.36-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[K     |████████████████████████████████| 128 kB 4.1 MB/s \n",
            "\u001b[?25hCollecting botocore==1.15.36\n",
            "  Downloading botocore-1.15.36-py2.py3-none-any.whl (6.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.1 MB 17.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: catalogue==1.0.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 4)) (1.0.0)\n",
            "Collecting certifi==2019.11.28\n",
            "  Downloading certifi-2019.11.28-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 70.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 6)) (3.0.4)\n",
            "Collecting click==7.1.1\n",
            "  Downloading click-7.1.1-py2.py3-none-any.whl (82 kB)\n",
            "\u001b[K     |████████████████████████████████| 82 kB 1.6 MB/s \n",
            "\u001b[?25hCollecting colorama==0.4.3\n",
            "  Downloading colorama-0.4.3-py2.py3-none-any.whl (15 kB)\n",
            "Collecting cymem==2.0.3\n",
            "  Downloading cymem-2.0.3-cp37-cp37m-manylinux1_x86_64.whl (32 kB)\n",
            "Collecting docutils==0.15.2\n",
            "  Downloading docutils-0.15.2-py3-none-any.whl (547 kB)\n",
            "\u001b[K     |████████████████████████████████| 547 kB 81.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: en-core-web-sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 11)) (2.2.5)\n",
            "Requirement already satisfied: filelock==3.0.12 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 12)) (3.0.12)\n",
            "Collecting idna==2.9\n",
            "  Downloading idna-2.9-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 9.2 MB/s \n",
            "\u001b[?25hCollecting importlib-metadata==1.6.0\n",
            "  Downloading importlib_metadata-1.6.0-py2.py3-none-any.whl (30 kB)\n",
            "Collecting jmespath==0.9.5\n",
            "  Downloading jmespath-0.9.5-py2.py3-none-any.whl (24 kB)\n",
            "Collecting joblib==0.14.1\n",
            "  Downloading joblib-0.14.1-py2.py3-none-any.whl (294 kB)\n",
            "\u001b[K     |████████████████████████████████| 294 kB 50.5 MB/s \n",
            "\u001b[?25hCollecting murmurhash==1.0.2\n",
            "  Downloading murmurhash-1.0.2-cp37-cp37m-manylinux1_x86_64.whl (19 kB)\n",
            "Requirement already satisfied: plac==1.1.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 18)) (1.1.3)\n",
            "Collecting preshed==3.0.2\n",
            "  Downloading preshed-3.0.2-cp37-cp37m-manylinux1_x86_64.whl (118 kB)\n",
            "\u001b[K     |████████████████████████████████| 118 kB 71.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil==2.8.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 20)) (2.8.1)\n",
            "Collecting pytz==2019.3\n",
            "  Downloading pytz-2019.3-py2.py3-none-any.whl (509 kB)\n",
            "\u001b[K     |████████████████████████████████| 509 kB 71.9 MB/s \n",
            "\u001b[?25hCollecting regex==2020.2.20\n",
            "  Downloading regex-2020.2.20-cp37-cp37m-manylinux2010_x86_64.whl (689 kB)\n",
            "\u001b[K     |████████████████████████████████| 689 kB 60.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 23)) (2.23.0)\n",
            "Collecting s3transfer==0.3.3\n",
            "  Downloading s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 10.9 MB/s \n",
            "\u001b[?25hCollecting sacremoses==0.0.38\n",
            "  Downloading sacremoses-0.0.38.tar.gz (860 kB)\n",
            "\u001b[K     |████████████████████████████████| 860 kB 46.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn==0.22.2.post1 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 26)) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 27)) (1.4.1)\n",
            "Collecting sentencepiece==0.1.85\n",
            "  Downloading sentencepiece-0.1.85-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 58.1 MB/s \n",
            "\u001b[?25hCollecting six==1.14.0\n",
            "  Downloading six-1.14.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 30)) (0.0)\n",
            "Requirement already satisfied: spacy==2.2.4 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 31)) (2.2.4)\n",
            "Collecting srsly==1.0.2\n",
            "  Downloading srsly-1.0.2-cp37-cp37m-manylinux1_x86_64.whl (185 kB)\n",
            "\u001b[K     |████████████████████████████████| 185 kB 70.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 33)) (7.4.0)\n",
            "Collecting torch==1.6.0\n",
            "  Downloading torch-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (748.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 748.8 MB 18 kB/s \n",
            "\u001b[?25hCollecting tqdm==4.45.0\n",
            "  Downloading tqdm-4.45.0-py2.py3-none-any.whl (60 kB)\n",
            "\u001b[K     |████████████████████████████████| 60 kB 11.3 MB/s \n",
            "\u001b[?25hCollecting urllib3==1.25.8\n",
            "  Downloading urllib3-1.25.8-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 69.6 MB/s \n",
            "\u001b[?25hCollecting wasabi==0.6.0\n",
            "  Downloading wasabi-0.6.0-py3-none-any.whl (20 kB)\n",
            "Collecting zipp==3.1.0\n",
            "  Downloading zipp-3.1.0-py3-none-any.whl (4.9 kB)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from blis==0.4.1->-r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4->-r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 31)) (57.2.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0->-r /content/gdrive/MyDrive/master_hpi/NLP_Project/requirements.txt (line 34)) (0.16.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-py3-none-any.whl size=884621 sha256=64722e1b8c9a21cd5cb7061b09a2f4673eab9ef077c513cbcbb627d938460d11\n",
            "  Stored in directory: /root/.cache/pip/wheels/99/c9/5a/a5e36bce983040ea5061a8ec65b5852bfebad4b1afa8d5b394\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: zipp, six, murmurhash, importlib-metadata, cymem, wasabi, urllib3, tqdm, srsly, preshed, jmespath, idna, docutils, certifi, joblib, botocore, s3transfer, regex, click, torch, sentencepiece, sacremoses, pytz, colorama, boto3\n",
            "  Attempting uninstall: zipp\n",
            "    Found existing installation: zipp 3.5.0\n",
            "    Uninstalling zipp-3.5.0:\n",
            "      Successfully uninstalled zipp-3.5.0\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Attempting uninstall: murmurhash\n",
            "    Found existing installation: murmurhash 1.0.5\n",
            "    Uninstalling murmurhash-1.0.5:\n",
            "      Successfully uninstalled murmurhash-1.0.5\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.6.1\n",
            "    Uninstalling importlib-metadata-4.6.1:\n",
            "      Successfully uninstalled importlib-metadata-4.6.1\n",
            "  Attempting uninstall: cymem\n",
            "    Found existing installation: cymem 2.0.5\n",
            "    Uninstalling cymem-2.0.5:\n",
            "      Successfully uninstalled cymem-2.0.5\n",
            "  Attempting uninstall: wasabi\n",
            "    Found existing installation: wasabi 0.8.2\n",
            "    Uninstalling wasabi-0.8.2:\n",
            "      Successfully uninstalled wasabi-0.8.2\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: preshed\n",
            "    Found existing installation: preshed 3.0.5\n",
            "    Uninstalling preshed-3.0.5:\n",
            "      Successfully uninstalled preshed-3.0.5\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 2.10\n",
            "    Uninstalling idna-2.10:\n",
            "      Successfully uninstalled idna-2.10\n",
            "  Attempting uninstall: docutils\n",
            "    Found existing installation: docutils 0.17.1\n",
            "    Uninstalling docutils-0.17.1:\n",
            "      Successfully uninstalled docutils-0.17.1\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2021.5.30\n",
            "    Uninstalling certifi-2021.5.30:\n",
            "      Successfully uninstalled certifi-2021.5.30\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.0.1\n",
            "    Uninstalling joblib-1.0.1:\n",
            "      Successfully uninstalled joblib-1.0.1\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 7.1.2\n",
            "    Uninstalling click-7.1.2:\n",
            "      Successfully uninstalled click-7.1.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.9.0+cu102\n",
            "    Uninstalling torch-1.9.0+cu102:\n",
            "      Successfully uninstalled torch-1.9.0+cu102\n",
            "  Attempting uninstall: sacremoses\n",
            "    Found existing installation: sacremoses 0.0.45\n",
            "    Uninstalling sacremoses-0.0.45:\n",
            "      Successfully uninstalled sacremoses-0.0.45\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2018.9\n",
            "    Uninstalling pytz-2018.9:\n",
            "      Successfully uninstalled pytz-2018.9\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.10.0+cu102 requires torch==1.9.0, but you have torch 1.6.0 which is incompatible.\n",
            "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.6.0 which is incompatible.\n",
            "tensorflow 2.5.0 requires six~=1.15.0, but you have six 1.14.0 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.14.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.12.36 botocore-1.15.36 certifi-2019.11.28 click-7.1.1 colorama-0.4.3 cymem-2.0.3 docutils-0.15.2 idna-2.9 importlib-metadata-1.6.0 jmespath-0.9.5 joblib-0.14.1 murmurhash-1.0.2 preshed-3.0.2 pytz-2019.3 regex-2020.2.20 s3transfer-0.3.3 sacremoses-0.0.38 sentencepiece-0.1.85 six-1.14.0 srsly-1.0.2 torch-1.6.0 tqdm-4.45.0 urllib3-1.25.8 wasabi-0.6.0 zipp-3.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pytz",
                  "six"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbEF5yRZy4an",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f02db3b5-5f5a-4920-bc18-c33b3382eb90"
      },
      "source": [
        "import sys\n",
        "from collections import defaultdict \n",
        "from collections import defaultdict \n",
        "from transformers import BertForMaskedLM, BertModel, BertTokenizer, BertForSequenceClassification\n",
        "#from models import models\n",
        "\n",
        "sys.path.append('/content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/')\n",
        "path = \"/content/gdrive/MyDrive/master_hpi/NLP_Project/code/\""
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[autoreload of six failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "AttributeError: 'NoneType' object has no attribute 'cStringIO'\n",
            "]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8yDqLezqXLT"
      },
      "source": [
        "#adapter_list = [\"sst\"]\n",
        "#https://github.com/christianwarmuth/transformer_adapter_bias_evaluation/blob/main/code/adapter_evaluation/evaluation_generativemodels.py\n",
        "#for adapter in adapter_list:\n",
        "#  print(\"!python3 evaluation.py --no-cuda False --output-dir /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/ -- skip_intrasentence False --skip_intersentence False --batch_size 1\")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2LRBGYNxgAZ"
      },
      "source": [
        "# Adapter Fusion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xsDHhl0G7Sl"
      },
      "source": [
        "#sst-imdb adapter fusion evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5DQkbAYxjrx"
      },
      "source": [
        "import os\n",
        "try:\n",
        "    os.makedirs('/content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/fusion-sst-imdb/')\n",
        "except OSError as e:\n",
        "        pass"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DudWrrkJxpou",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fffa06ef-46dd-4ad5-db25-be1dc37add6a"
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/evaluation.py --load-path_model /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/fusion2/pytorch_model_adapter_fusion.bin --load-path_adapter /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/fusion2/pytorch_model_adapter_fusion.bin --input-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --output-dir /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/fusion-sst-imdb/ --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/fusion-sst-imdb/predictions.json\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json...\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 679kB/s]\n",
            "Downloading: 100% 28.0/28.0 [00:00<00:00, 23.9kB/s]\n",
            "Downloading: 100% 466k/466k [00:00<00:00, 905kB/s]\n",
            "---------------------------------------------------------------\n",
            "\u001b[96m                     ARGUMENTS                 \u001b[0m\n",
            "\u001b[96mPretrained Model:\u001b[0m /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/fusion2/pytorch_model_adapter_fusion.bin\n",
            "\u001b[96mSkip Intrasentence:\u001b[0m False\n",
            "\u001b[96mSkip Intersentence:\u001b[0m False\n",
            "\u001b[96mBatch Size:\u001b[0m 1\n",
            "\u001b[96mMax Seq Length:\u001b[0m None\n",
            "\u001b[96mCUDA:\u001b[0m True\n",
            "---------------------------------------------------------------\n",
            "\n",
            "\u001b[94mEvaluating bias on intersentence tasks...\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2160: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
            "  FutureWarning,\n",
            "Maximum sequence length found: -inf\n",
            "Downloading: 100% 570/570 [00:00<00:00, 515kB/s]\n",
            "Downloading: 100% 440M/440M [00:07<00:00, 59.4MB/s]\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "dict_keys(['bert.encoder.layer.0.output.adapter_fusion_layer.sst-2,imdb.query.weight', 'bert.encoder.layer.0.output.adapter_fusion_layer.sst-2,imdb.query.bias', 'bert.encoder.layer.0.output.adapter_fusion_layer.sst-2,imdb.key.weight', 'bert.encoder.layer.0.output.adapter_fusion_layer.sst-2,imdb.key.bias', 'bert.encoder.layer.0.output.adapter_fusion_layer.sst-2,imdb.value.weight', 'bert.encoder.layer.1.output.adapter_fusion_layer.sst-2,imdb.query.weight', 'bert.encoder.layer.1.output.adapter_fusion_layer.sst-2,imdb.query.bias', 'bert.encoder.layer.1.output.adapter_fusion_layer.sst-2,imdb.key.weight', 'bert.encoder.layer.1.output.adapter_fusion_layer.sst-2,imdb.key.bias', 'bert.encoder.layer.1.output.adapter_fusion_layer.sst-2,imdb.value.weight', 'bert.encoder.layer.2.output.adapter_fusion_layer.sst-2,imdb.query.weight', 'bert.encoder.layer.2.output.adapter_fusion_layer.sst-2,imdb.query.bias', 'bert.encoder.layer.2.output.adapter_fusion_layer.sst-2,imdb.key.weight', 'bert.encoder.layer.2.output.adapter_fusion_layer.sst-2,imdb.key.bias', 'bert.encoder.layer.2.output.adapter_fusion_layer.sst-2,imdb.value.weight', 'bert.encoder.layer.3.output.adapter_fusion_layer.sst-2,imdb.query.weight', 'bert.encoder.layer.3.output.adapter_fusion_layer.sst-2,imdb.query.bias', 'bert.encoder.layer.3.output.adapter_fusion_layer.sst-2,imdb.key.weight', 'bert.encoder.layer.3.output.adapter_fusion_layer.sst-2,imdb.key.bias', 'bert.encoder.layer.3.output.adapter_fusion_layer.sst-2,imdb.value.weight', 'bert.encoder.layer.4.output.adapter_fusion_layer.sst-2,imdb.query.weight', 'bert.encoder.layer.4.output.adapter_fusion_layer.sst-2,imdb.query.bias', 'bert.encoder.layer.4.output.adapter_fusion_layer.sst-2,imdb.key.weight', 'bert.encoder.layer.4.output.adapter_fusion_layer.sst-2,imdb.key.bias', 'bert.encoder.layer.4.output.adapter_fusion_layer.sst-2,imdb.value.weight', 'bert.encoder.layer.5.output.adapter_fusion_layer.sst-2,imdb.query.weight', 'bert.encoder.layer.5.output.adapter_fusion_layer.sst-2,imdb.query.bias', 'bert.encoder.layer.5.output.adapter_fusion_layer.sst-2,imdb.key.weight', 'bert.encoder.layer.5.output.adapter_fusion_layer.sst-2,imdb.key.bias', 'bert.encoder.layer.5.output.adapter_fusion_layer.sst-2,imdb.value.weight', 'bert.encoder.layer.6.output.adapter_fusion_layer.sst-2,imdb.query.weight', 'bert.encoder.layer.6.output.adapter_fusion_layer.sst-2,imdb.query.bias', 'bert.encoder.layer.6.output.adapter_fusion_layer.sst-2,imdb.key.weight', 'bert.encoder.layer.6.output.adapter_fusion_layer.sst-2,imdb.key.bias', 'bert.encoder.layer.6.output.adapter_fusion_layer.sst-2,imdb.value.weight', 'bert.encoder.layer.7.output.adapter_fusion_layer.sst-2,imdb.query.weight', 'bert.encoder.layer.7.output.adapter_fusion_layer.sst-2,imdb.query.bias', 'bert.encoder.layer.7.output.adapter_fusion_layer.sst-2,imdb.key.weight', 'bert.encoder.layer.7.output.adapter_fusion_layer.sst-2,imdb.key.bias', 'bert.encoder.layer.7.output.adapter_fusion_layer.sst-2,imdb.value.weight', 'bert.encoder.layer.8.output.adapter_fusion_layer.sst-2,imdb.query.weight', 'bert.encoder.layer.8.output.adapter_fusion_layer.sst-2,imdb.query.bias', 'bert.encoder.layer.8.output.adapter_fusion_layer.sst-2,imdb.key.weight', 'bert.encoder.layer.8.output.adapter_fusion_layer.sst-2,imdb.key.bias', 'bert.encoder.layer.8.output.adapter_fusion_layer.sst-2,imdb.value.weight', 'bert.encoder.layer.9.output.adapter_fusion_layer.sst-2,imdb.query.weight', 'bert.encoder.layer.9.output.adapter_fusion_layer.sst-2,imdb.query.bias', 'bert.encoder.layer.9.output.adapter_fusion_layer.sst-2,imdb.key.weight', 'bert.encoder.layer.9.output.adapter_fusion_layer.sst-2,imdb.key.bias', 'bert.encoder.layer.9.output.adapter_fusion_layer.sst-2,imdb.value.weight', 'bert.encoder.layer.10.output.adapter_fusion_layer.sst-2,imdb.query.weight', 'bert.encoder.layer.10.output.adapter_fusion_layer.sst-2,imdb.query.bias', 'bert.encoder.layer.10.output.adapter_fusion_layer.sst-2,imdb.key.weight', 'bert.encoder.layer.10.output.adapter_fusion_layer.sst-2,imdb.key.bias', 'bert.encoder.layer.10.output.adapter_fusion_layer.sst-2,imdb.value.weight', 'bert.encoder.layer.11.output.adapter_fusion_layer.sst-2,imdb.query.weight', 'bert.encoder.layer.11.output.adapter_fusion_layer.sst-2,imdb.query.bias', 'bert.encoder.layer.11.output.adapter_fusion_layer.sst-2,imdb.key.weight', 'bert.encoder.layer.11.output.adapter_fusion_layer.sst-2,imdb.key.bias', 'bert.encoder.layer.11.output.adapter_fusion_layer.sst-2,imdb.value.weight'])\n",
            "100% 6369/6369 [01:34<00:00, 67.44it/s]\n",
            "\n",
            "\u001b[91mEvaluating bias on intrasentence tasks...\u001b[0m\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "100% 6318/6318 [01:36<00:00, 65.55it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUHHkF3lyHTa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ddf68c9-dbb7-4c2f-ef16-5910bc0b3923"
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/score_evaluation.py --gold-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --predictions-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/fusion-sst-imdb/predictions.json --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/fusion-sst-imdb/summary.json"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "intrasentence\n",
            "\tgender\n",
            "\t\tCount: 765.0\n",
            "\t\tLM Score: 36.98808566634654\n",
            "\t\tSS Score: 36.76060815191249\n",
            "\t\tICAT Score: 27.194090469398727\n",
            "\tprofession\n",
            "\t\tCount: 2430.0\n",
            "\t\tLM Score: 39.94509000402677\n",
            "\t\tSS Score: 48.60533031106777\n",
            "\t\tICAT Score: 38.83088587902105\n",
            "\trace\n",
            "\t\tCount: 2886.0\n",
            "\t\tLM Score: 32.51170172882939\n",
            "\t\tSS Score: 49.5950645899267\n",
            "\t\tICAT Score: 32.2483989433945\n",
            "\treligion\n",
            "\t\tCount: 237.0\n",
            "\t\tLM Score: 41.333333333333336\n",
            "\t\tSS Score: 46.94252873563219\n",
            "\t\tICAT Score: 38.80582375478928\n",
            "\toverall\n",
            "\t\tCount: 2106.0\n",
            "\t\tLM Score: 36.23613694964717\n",
            "\t\tSS Score: 47.493872180954625\n",
            "\t\tICAT Score: 34.419889132362194\n",
            "intersentence\n",
            "\tgender\n",
            "\t\tCount: 726.0\n",
            "\t\tLM Score: 87.6498712640017\n",
            "\t\tSS Score: 61.16123409601671\n",
            "\t\tICAT Score: 68.08425663073668\n",
            "\tprofession\n",
            "\t\tCount: 2481.0\n",
            "\t\tLM Score: 80.3657797910764\n",
            "\t\tSS Score: 56.20253450733615\n",
            "\t\tICAT Score: 70.39634934381381\n",
            "\trace\n",
            "\t\tCount: 2928.0\n",
            "\t\tLM Score: 83.9499715830593\n",
            "\t\tSS Score: 57.78506309009337\n",
            "\t\tICAT Score: 70.87885507934605\n",
            "\treligion\n",
            "\t\tCount: 234.0\n",
            "\t\tLM Score: 86.62835249042148\n",
            "\t\tSS Score: 57.83524904214559\n",
            "\t\tICAT Score: 73.05325817295696\n",
            "\toverall\n",
            "\t\tCount: 2123.0\n",
            "\t\tLM Score: 83.15893849156592\n",
            "\t\tSS Score: 57.61337208291201\n",
            "\t\tICAT Score: 70.49653967644022\n",
            "overall\n",
            "\tCount: 4229.0\n",
            "\tLM Score: 59.764825210774504\n",
            "\tSS Score: 52.52368770542892\n",
            "\tICAT Score: 56.748270118743704\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYjHVv6rsCwV"
      },
      "source": [
        "# Evaluation of Pre-Trained Sentiment Adapters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_56ULJcssIhe"
      },
      "source": [
        "Rotten Tomatoes (distilbert-base-uncased)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ar5oqxFJsazq"
      },
      "source": [
        "import os\n",
        "try:\n",
        "    os.makedirs('/content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/rotten-tomatoes/')\n",
        "except OSError as e:\n",
        "        pass"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8u9-IOKEsdY1",
        "outputId": "806529e5-8cb1-468d-e179-83f502d44bd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/evaluation.py --load-path_model /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/rottentomatoes-pretrained/pytorch_adapter.bin --load-path_adapter /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/rottentomatoes-pretrained/pytorch_adapter.bin --input-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --output-dir /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/rotten-tomatoes/ --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/rotten-tomatoes/predictions.json\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json...\n",
            "---------------------------------------------------------------\n",
            "\u001b[96m                     ARGUMENTS                 \u001b[0m\n",
            "\u001b[96mPretrained Model:\u001b[0m /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/rottentomatoes-pretrained/pytorch_adapter.bin\n",
            "\u001b[96mSkip Intrasentence:\u001b[0m False\n",
            "\u001b[96mSkip Intersentence:\u001b[0m False\n",
            "\u001b[96mBatch Size:\u001b[0m 1\n",
            "\u001b[96mMax Seq Length:\u001b[0m None\n",
            "\u001b[96mCUDA:\u001b[0m True\n",
            "---------------------------------------------------------------\n",
            "\n",
            "\u001b[94mEvaluating bias on intersentence tasks...\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2160: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
            "  FutureWarning,\n",
            "Maximum sequence length found: -inf\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "dict_keys(['distilbert.transformer.layer.0.output_adapters.adapters.rotten_tomatoes.adapter_down.0.weight', 'distilbert.transformer.layer.0.output_adapters.adapters.rotten_tomatoes.adapter_down.0.bias', 'distilbert.transformer.layer.0.output_adapters.adapters.rotten_tomatoes.adapter_up.weight', 'distilbert.transformer.layer.0.output_adapters.adapters.rotten_tomatoes.adapter_up.bias', 'distilbert.transformer.layer.1.output_adapters.adapters.rotten_tomatoes.adapter_down.0.weight', 'distilbert.transformer.layer.1.output_adapters.adapters.rotten_tomatoes.adapter_down.0.bias', 'distilbert.transformer.layer.1.output_adapters.adapters.rotten_tomatoes.adapter_up.weight', 'distilbert.transformer.layer.1.output_adapters.adapters.rotten_tomatoes.adapter_up.bias', 'distilbert.transformer.layer.2.output_adapters.adapters.rotten_tomatoes.adapter_down.0.weight', 'distilbert.transformer.layer.2.output_adapters.adapters.rotten_tomatoes.adapter_down.0.bias', 'distilbert.transformer.layer.2.output_adapters.adapters.rotten_tomatoes.adapter_up.weight', 'distilbert.transformer.layer.2.output_adapters.adapters.rotten_tomatoes.adapter_up.bias', 'distilbert.transformer.layer.3.output_adapters.adapters.rotten_tomatoes.adapter_down.0.weight', 'distilbert.transformer.layer.3.output_adapters.adapters.rotten_tomatoes.adapter_down.0.bias', 'distilbert.transformer.layer.3.output_adapters.adapters.rotten_tomatoes.adapter_up.weight', 'distilbert.transformer.layer.3.output_adapters.adapters.rotten_tomatoes.adapter_up.bias', 'distilbert.transformer.layer.4.output_adapters.adapters.rotten_tomatoes.adapter_down.0.weight', 'distilbert.transformer.layer.4.output_adapters.adapters.rotten_tomatoes.adapter_down.0.bias', 'distilbert.transformer.layer.4.output_adapters.adapters.rotten_tomatoes.adapter_up.weight', 'distilbert.transformer.layer.4.output_adapters.adapters.rotten_tomatoes.adapter_up.bias', 'distilbert.transformer.layer.5.output_adapters.adapters.rotten_tomatoes.adapter_down.0.weight', 'distilbert.transformer.layer.5.output_adapters.adapters.rotten_tomatoes.adapter_down.0.bias', 'distilbert.transformer.layer.5.output_adapters.adapters.rotten_tomatoes.adapter_up.weight', 'distilbert.transformer.layer.5.output_adapters.adapters.rotten_tomatoes.adapter_up.bias'])\n",
            "100% 6369/6369 [01:34<00:00, 67.51it/s]\n",
            "\n",
            "\u001b[91mEvaluating bias on intrasentence tasks...\u001b[0m\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "100% 6318/6318 [01:36<00:00, 65.58it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmtJyBTSsgBm",
        "outputId": "cf5af6b4-f9a9-444c-bf32-40719f66df05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/score_evaluation.py --gold-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --predictions-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/rotten-tomatoes/predictions.json --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/rotten-tomatoes/summary.json"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "intrasentence\n",
            "\tgender\n",
            "\t\tCount: 765.0\n",
            "\t\tLM Score: 60.50091567048089\n",
            "\t\tSS Score: 51.36622652709609\n",
            "\t\tICAT Score: 58.847756552428606\n",
            "\tprofession\n",
            "\t\tCount: 2430.0\n",
            "\t\tLM Score: 67.20160866478645\n",
            "\t\tSS Score: 51.567489522779006\n",
            "\t\tICAT Score: 65.0948523148675\n",
            "\trace\n",
            "\t\tCount: 2886.0\n",
            "\t\tLM Score: 73.28357326699319\n",
            "\t\tSS Score: 56.970920411227816\n",
            "\t\tICAT Score: 63.06649413310135\n",
            "\treligion\n",
            "\t\tCount: 237.0\n",
            "\t\tLM Score: 62.11494252873563\n",
            "\t\tSS Score: 38.94252873563219\n",
            "\t\tICAT Score: 48.37825868674858\n",
            "\toverall\n",
            "\t\tCount: 2106.0\n",
            "\t\tLM Score: 68.93178331451092\n",
            "\t\tSS Score: 53.52490724006871\n",
            "\t\tICAT Score: 64.07222047298758\n",
            "intersentence\n",
            "\tgender\n",
            "\t\tCount: 726.0\n",
            "\t\tLM Score: 10.715413813239902\n",
            "\t\tSS Score: 41.91506963246094\n",
            "\t\tICAT Score: 8.982746322451687\n",
            "\tprofession\n",
            "\t\tCount: 2481.0\n",
            "\t\tLM Score: 16.130537652311844\n",
            "\t\tSS Score: 40.539772090198475\n",
            "\t\tICAT Score: 13.078566402341748\n",
            "\trace\n",
            "\t\tCount: 2928.0\n",
            "\t\tLM Score: 13.278327677737517\n",
            "\t\tSS Score: 40.63405861945907\n",
            "\t\tICAT Score: 10.791046904511441\n",
            "\treligion\n",
            "\t\tCount: 234.0\n",
            "\t\tLM Score: 10.03831417624521\n",
            "\t\tSS Score: 41.79693486590038\n",
            "\t\tICAT Score: 8.391415275759309\n",
            "\toverall\n",
            "\t\tCount: 2123.0\n",
            "\t\tLM Score: 13.913987425684057\n",
            "\t\tSS Score: 40.8045667585923\n",
            "\t\tICAT Score: 11.35508457579078\n",
            "overall\n",
            "\tCount: 4229.0\n",
            "\tLM Score: 41.30693245428182\n",
            "\tSS Score: 47.14770884484046\n",
            "\tICAT Score: 38.95054449255941\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NDqdUT-tuj0"
      },
      "source": [
        "SST-2-Houlsby (bert-base-uncased), 20 epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjW6l4oit7u7"
      },
      "source": [
        "import os\n",
        "try:\n",
        "    os.makedirs('/content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sst-2-houlsby/')\n",
        "except OSError as e:\n",
        "        pass"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tb7YLjtFuAhe",
        "outputId": "3d73d32a-35c3-48af-ca6d-2db592161d7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/evaluation.py --load-path_model /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/sst-2-houlsby-pretrained/pytorch_adapter.bin --load-path_adapter /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/sst-2-houlsby-pretrained/pytorch_adapter.bin --input-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --output-dir /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sst-2-houlsby/ --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sst-2-houlsby/predictions.json\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json...\n",
            "---------------------------------------------------------------\n",
            "\u001b[96m                     ARGUMENTS                 \u001b[0m\n",
            "\u001b[96mPretrained Model:\u001b[0m /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/sst-2-houlsby-pretrained/pytorch_adapter.bin\n",
            "\u001b[96mSkip Intrasentence:\u001b[0m False\n",
            "\u001b[96mSkip Intersentence:\u001b[0m False\n",
            "\u001b[96mBatch Size:\u001b[0m 1\n",
            "\u001b[96mMax Seq Length:\u001b[0m None\n",
            "\u001b[96mCUDA:\u001b[0m True\n",
            "---------------------------------------------------------------\n",
            "\n",
            "\u001b[94mEvaluating bias on intersentence tasks...\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2160: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
            "  FutureWarning,\n",
            "Maximum sequence length found: -inf\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "dict_keys(['bert.encoder.layer.0.attention.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.0.attention.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.0.attention.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.0.attention.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.0.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.0.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.0.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.0.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.1.attention.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.1.attention.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.1.attention.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.1.attention.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.1.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.1.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.1.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.1.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.2.attention.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.2.attention.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.2.attention.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.2.attention.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.2.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.2.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.2.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.2.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.3.attention.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.3.attention.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.3.attention.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.3.attention.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.3.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.3.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.3.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.3.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.4.attention.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.4.attention.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.4.attention.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.4.attention.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.4.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.4.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.4.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.4.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.5.attention.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.5.attention.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.5.attention.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.5.attention.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.5.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.5.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.5.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.5.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.6.attention.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.6.attention.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.6.attention.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.6.attention.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.6.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.6.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.6.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.6.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.7.attention.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.7.attention.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.7.attention.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.7.attention.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.7.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.7.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.7.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.7.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.8.attention.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.8.attention.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.8.attention.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.8.attention.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.8.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.8.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.8.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.8.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.9.attention.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.9.attention.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.9.attention.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.9.attention.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.9.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.9.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.9.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.9.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.10.attention.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.10.attention.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.10.attention.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.10.attention.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.10.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.10.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.10.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.10.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.11.attention.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.11.attention.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.11.attention.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.11.attention.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.11.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.11.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.11.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.11.output.adapters.sst-2.adapter_up.bias'])\n",
            "100% 6369/6369 [01:34<00:00, 67.09it/s]\n",
            "\n",
            "\u001b[91mEvaluating bias on intrasentence tasks...\u001b[0m\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "100% 6318/6318 [01:36<00:00, 65.49it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ud7AZHbuT3k",
        "outputId": "b0f83329-df99-4dcd-8daa-03114b4722d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/score_evaluation.py --gold-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --predictions-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sst-2-houlsby/predictions.json --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sst-2-houlsby/summary.json"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "intrasentence\n",
            "\tgender\n",
            "\t\tCount: 765.0\n",
            "\t\tLM Score: 50.34941700811266\n",
            "\t\tSS Score: 48.43563891824762\n",
            "\t\tICAT Score: 48.774123638984406\n",
            "\tprofession\n",
            "\t\tCount: 2430.0\n",
            "\t\tLM Score: 52.43812186915161\n",
            "\t\tSS Score: 52.189305974006956\n",
            "\t\tICAT Score: 50.14205999967484\n",
            "\trace\n",
            "\t\tCount: 2886.0\n",
            "\t\tLM Score: 64.29798190489652\n",
            "\t\tSS Score: 52.9794240388802\n",
            "\t\tICAT Score: 60.46656284611787\n",
            "\treligion\n",
            "\t\tCount: 237.0\n",
            "\t\tLM Score: 52.09195402298851\n",
            "\t\tSS Score: 59.356321839080465\n",
            "\t\tICAT Score: 42.34417228167525\n",
            "\toverall\n",
            "\t\tCount: 2106.0\n",
            "\t\tLM Score: 57.565076415201474\n",
            "\t\tSS Score: 52.34637720657739\n",
            "\t\tICAT Score: 54.863688751291185\n",
            "intersentence\n",
            "\tgender\n",
            "\t\tCount: 726.0\n",
            "\t\tLM Score: 89.63247199660245\n",
            "\t\tSS Score: 52.20883987188334\n",
            "\t\tICAT Score: 85.6727964373712\n",
            "\tprofession\n",
            "\t\tCount: 2481.0\n",
            "\t\tLM Score: 81.10130831520338\n",
            "\t\tSS Score: 56.63635369011713\n",
            "\t\tICAT Score: 70.33696898098484\n",
            "\trace\n",
            "\t\tCount: 2928.0\n",
            "\t\tLM Score: 84.65040680039598\n",
            "\t\tSS Score: 49.715193402453934\n",
            "\t\tICAT Score: 84.16822691356175\n",
            "\treligion\n",
            "\t\tCount: 234.0\n",
            "\t\tLM Score: 84.68869731800767\n",
            "\t\tSS Score: 57.411877394636015\n",
            "\t\tICAT Score: 72.13465249335742\n",
            "\toverall\n",
            "\t\tCount: 2123.0\n",
            "\t\tLM Score: 83.93474311633425\n",
            "\t\tSS Score: 52.95141271005819\n",
            "\t\tICAT Score: 78.98022176335388\n",
            "overall\n",
            "\tCount: 4229.0\n",
            "\tLM Score: 70.71426281709769\n",
            "\tSS Score: 52.68711457405122\n",
            "\tICAT Score: 66.91391629291546\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXBCzr7aubTf"
      },
      "source": [
        "SST-2- (distilbert-base-uncased),  15 epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ig7RknP_vyxm"
      },
      "source": [
        "import os\n",
        "try:\n",
        "    os.makedirs('/content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sst-2-pretrained-distilbert15/')\n",
        "except OSError as e:\n",
        "        pass\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JprJe27rwCdl",
        "outputId": "f10214d1-a656-4a3b-a566-890211dc8a14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/evaluation.py --load-path_model /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/sst-2-pretrained-distilbert/pytorch_adapter.bin --load-path_adapter /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/sst-2-pretrained-distilbert/pytorch_adapter.bin --input-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --output-dir /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sst-2-pretrained-distilbert15/ --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sst-2-pretrained-distilbert15/predictions.json\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json...\n",
            "---------------------------------------------------------------\n",
            "\u001b[96m                     ARGUMENTS                 \u001b[0m\n",
            "\u001b[96mPretrained Model:\u001b[0m /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/sst-2-pretrained-distilbert/pytorch_adapter.bin\n",
            "\u001b[96mSkip Intrasentence:\u001b[0m False\n",
            "\u001b[96mSkip Intersentence:\u001b[0m False\n",
            "\u001b[96mBatch Size:\u001b[0m 1\n",
            "\u001b[96mMax Seq Length:\u001b[0m None\n",
            "\u001b[96mCUDA:\u001b[0m True\n",
            "---------------------------------------------------------------\n",
            "\n",
            "\u001b[94mEvaluating bias on intersentence tasks...\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2160: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
            "  FutureWarning,\n",
            "Maximum sequence length found: -inf\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "dict_keys(['distilbert.transformer.layer.0.attention_adapters.adapters.sst-2.adapter_down.0.weight', 'distilbert.transformer.layer.0.attention_adapters.adapters.sst-2.adapter_down.0.bias', 'distilbert.transformer.layer.0.attention_adapters.adapters.sst-2.adapter_up.weight', 'distilbert.transformer.layer.0.attention_adapters.adapters.sst-2.adapter_up.bias', 'distilbert.transformer.layer.0.output_adapters.adapters.sst-2.adapter_down.0.weight', 'distilbert.transformer.layer.0.output_adapters.adapters.sst-2.adapter_down.0.bias', 'distilbert.transformer.layer.0.output_adapters.adapters.sst-2.adapter_up.weight', 'distilbert.transformer.layer.0.output_adapters.adapters.sst-2.adapter_up.bias', 'distilbert.transformer.layer.1.attention_adapters.adapters.sst-2.adapter_down.0.weight', 'distilbert.transformer.layer.1.attention_adapters.adapters.sst-2.adapter_down.0.bias', 'distilbert.transformer.layer.1.attention_adapters.adapters.sst-2.adapter_up.weight', 'distilbert.transformer.layer.1.attention_adapters.adapters.sst-2.adapter_up.bias', 'distilbert.transformer.layer.1.output_adapters.adapters.sst-2.adapter_down.0.weight', 'distilbert.transformer.layer.1.output_adapters.adapters.sst-2.adapter_down.0.bias', 'distilbert.transformer.layer.1.output_adapters.adapters.sst-2.adapter_up.weight', 'distilbert.transformer.layer.1.output_adapters.adapters.sst-2.adapter_up.bias', 'distilbert.transformer.layer.2.attention_adapters.adapters.sst-2.adapter_down.0.weight', 'distilbert.transformer.layer.2.attention_adapters.adapters.sst-2.adapter_down.0.bias', 'distilbert.transformer.layer.2.attention_adapters.adapters.sst-2.adapter_up.weight', 'distilbert.transformer.layer.2.attention_adapters.adapters.sst-2.adapter_up.bias', 'distilbert.transformer.layer.2.output_adapters.adapters.sst-2.adapter_down.0.weight', 'distilbert.transformer.layer.2.output_adapters.adapters.sst-2.adapter_down.0.bias', 'distilbert.transformer.layer.2.output_adapters.adapters.sst-2.adapter_up.weight', 'distilbert.transformer.layer.2.output_adapters.adapters.sst-2.adapter_up.bias', 'distilbert.transformer.layer.3.attention_adapters.adapters.sst-2.adapter_down.0.weight', 'distilbert.transformer.layer.3.attention_adapters.adapters.sst-2.adapter_down.0.bias', 'distilbert.transformer.layer.3.attention_adapters.adapters.sst-2.adapter_up.weight', 'distilbert.transformer.layer.3.attention_adapters.adapters.sst-2.adapter_up.bias', 'distilbert.transformer.layer.3.output_adapters.adapters.sst-2.adapter_down.0.weight', 'distilbert.transformer.layer.3.output_adapters.adapters.sst-2.adapter_down.0.bias', 'distilbert.transformer.layer.3.output_adapters.adapters.sst-2.adapter_up.weight', 'distilbert.transformer.layer.3.output_adapters.adapters.sst-2.adapter_up.bias', 'distilbert.transformer.layer.4.attention_adapters.adapters.sst-2.adapter_down.0.weight', 'distilbert.transformer.layer.4.attention_adapters.adapters.sst-2.adapter_down.0.bias', 'distilbert.transformer.layer.4.attention_adapters.adapters.sst-2.adapter_up.weight', 'distilbert.transformer.layer.4.attention_adapters.adapters.sst-2.adapter_up.bias', 'distilbert.transformer.layer.4.output_adapters.adapters.sst-2.adapter_down.0.weight', 'distilbert.transformer.layer.4.output_adapters.adapters.sst-2.adapter_down.0.bias', 'distilbert.transformer.layer.4.output_adapters.adapters.sst-2.adapter_up.weight', 'distilbert.transformer.layer.4.output_adapters.adapters.sst-2.adapter_up.bias', 'distilbert.transformer.layer.5.attention_adapters.adapters.sst-2.adapter_down.0.weight', 'distilbert.transformer.layer.5.attention_adapters.adapters.sst-2.adapter_down.0.bias', 'distilbert.transformer.layer.5.attention_adapters.adapters.sst-2.adapter_up.weight', 'distilbert.transformer.layer.5.attention_adapters.adapters.sst-2.adapter_up.bias', 'distilbert.transformer.layer.5.output_adapters.adapters.sst-2.adapter_down.0.weight', 'distilbert.transformer.layer.5.output_adapters.adapters.sst-2.adapter_down.0.bias', 'distilbert.transformer.layer.5.output_adapters.adapters.sst-2.adapter_up.weight', 'distilbert.transformer.layer.5.output_adapters.adapters.sst-2.adapter_up.bias'])\n",
            "100% 6369/6369 [01:33<00:00, 68.45it/s]\n",
            "\n",
            "\u001b[91mEvaluating bias on intrasentence tasks...\u001b[0m\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "100% 6318/6318 [01:34<00:00, 66.68it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aEkFh-c1Vhe",
        "outputId": "7faf4483-85f5-4f7d-e759-3a05f5b5c034",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/score_evaluation.py --gold-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --predictions-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sst-2-pretrained-distilbert15/predictions.json --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sst-2-pretrained-distilbert15/summary.json"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "intrasentence\n",
            "\tgender\n",
            "\t\tCount: 765.0\n",
            "\t\tLM Score: 37.66621365751801\n",
            "\t\tSS Score: 46.417075356205785\n",
            "\t\tICAT Score: 34.967109554479215\n",
            "\tprofession\n",
            "\t\tCount: 2430.0\n",
            "\t\tLM Score: 44.78477113788118\n",
            "\t\tSS Score: 49.09586526394594\n",
            "\t\tICAT Score: 43.974941793241385\n",
            "\trace\n",
            "\t\tCount: 2886.0\n",
            "\t\tLM Score: 43.032360921603804\n",
            "\t\tSS Score: 53.01580072783584\n",
            "\t\tICAT Score: 40.43682041384646\n",
            "\treligion\n",
            "\t\tCount: 237.0\n",
            "\t\tLM Score: 47.701149425287355\n",
            "\t\tSS Score: 58.62068965517241\n",
            "\t\tICAT Score: 39.47681331747919\n",
            "\toverall\n",
            "\t\tCount: 2106.0\n",
            "\t\tLM Score: 43.195869774243214\n",
            "\t\tSS Score: 50.90477983098789\n",
            "\t\tICAT Score: 42.41421473916892\n",
            "intersentence\n",
            "\tgender\n",
            "\t\tCount: 726.0\n",
            "\t\tLM Score: 68.29987745748615\n",
            "\t\tSS Score: 56.86443789704659\n",
            "\t\tICAT Score: 58.923072113830024\n",
            "\tprofession\n",
            "\t\tCount: 2481.0\n",
            "\t\tLM Score: 64.38387587501418\n",
            "\t\tSS Score: 56.253618620767305\n",
            "\t\tICAT Score: 56.331231774031\n",
            "\trace\n",
            "\t\tCount: 2928.0\n",
            "\t\tLM Score: 72.08286433751456\n",
            "\t\tSS Score: 56.494461087944586\n",
            "\t\tICAT Score: 62.720077186563024\n",
            "\treligion\n",
            "\t\tCount: 234.0\n",
            "\t\tLM Score: 78.22701149425286\n",
            "\t\tSS Score: 59.59195402298851\n",
            "\t\tICAT Score: 63.22001354207952\n",
            "\toverall\n",
            "\t\tCount: 2123.0\n",
            "\t\tLM Score: 68.91366077795657\n",
            "\t\tSS Score: 56.56746074466401\n",
            "\t\tICAT Score: 59.86190553935013\n",
            "overall\n",
            "\tCount: 4229.0\n",
            "\tLM Score: 56.08307510354157\n",
            "\tSS Score: 53.71771801338968\n",
            "\tICAT Score: 51.91305393236711\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qnWTRZ_1gH0"
      },
      "source": [
        "SST-Adapter Pfeiffer (bert-base uncased)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkviNUuQ1V1-"
      },
      "source": [
        "import os\n",
        "try:\n",
        "    os.makedirs('/content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sst-pretrained/')\n",
        "except OSError as e:\n",
        "        pass"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-w0qI871pD6",
        "outputId": "6aceb482-7b24-4b3f-b5ed-2e4b9a3a6b6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/evaluation.py --load-path_model /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/sst_pretrained/pytorch_adapter.bin --load-path_adapter /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/sst_pretrained/pytorch_adapter.bin --input-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --output-dir /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sst-pretrained/ --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sst-pretrained/predictions.json\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json...\n",
            "---------------------------------------------------------------\n",
            "\u001b[96m                     ARGUMENTS                 \u001b[0m\n",
            "\u001b[96mPretrained Model:\u001b[0m /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/sst_pretrained/pytorch_adapter.bin\n",
            "\u001b[96mSkip Intrasentence:\u001b[0m False\n",
            "\u001b[96mSkip Intersentence:\u001b[0m False\n",
            "\u001b[96mBatch Size:\u001b[0m 1\n",
            "\u001b[96mMax Seq Length:\u001b[0m None\n",
            "\u001b[96mCUDA:\u001b[0m True\n",
            "---------------------------------------------------------------\n",
            "\n",
            "\u001b[94mEvaluating bias on intersentence tasks...\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2160: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
            "  FutureWarning,\n",
            "Maximum sequence length found: -inf\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "dict_keys(['bert.encoder.layer.0.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.0.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.0.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.0.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.1.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.1.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.1.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.1.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.2.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.2.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.2.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.2.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.3.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.3.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.3.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.3.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.4.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.4.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.4.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.4.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.5.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.5.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.5.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.5.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.6.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.6.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.6.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.6.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.7.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.7.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.7.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.7.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.8.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.8.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.8.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.8.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.9.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.9.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.9.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.9.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.10.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.10.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.10.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.10.output.adapters.sst-2.adapter_up.bias', 'bert.encoder.layer.11.output.adapters.sst-2.adapter_down.0.weight', 'bert.encoder.layer.11.output.adapters.sst-2.adapter_down.0.bias', 'bert.encoder.layer.11.output.adapters.sst-2.adapter_up.weight', 'bert.encoder.layer.11.output.adapters.sst-2.adapter_up.bias'])\n",
            "100% 6369/6369 [01:34<00:00, 67.41it/s]\n",
            "\n",
            "\u001b[91mEvaluating bias on intrasentence tasks...\u001b[0m\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "100% 6318/6318 [01:36<00:00, 65.40it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-ch14IV1pbg",
        "outputId": "cc175195-9658-47d1-d658-6256073315c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/score_evaluation.py --gold-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --predictions-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sst-pretrained/predictions.json --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sst-pretrained/summary.json"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "intrasentence\n",
            "\tgender\n",
            "\t\tCount: 765.0\n",
            "\t\tLM Score: 78.92807603024995\n",
            "\t\tSS Score: 57.23715559802516\n",
            "\t\tICAT Score: 67.50378068457637\n",
            "\tprofession\n",
            "\t\tCount: 2430.0\n",
            "\t\tLM Score: 73.73893327515842\n",
            "\t\tSS Score: 56.157609895867374\n",
            "\t\tICAT Score: 64.65782157024204\n",
            "\trace\n",
            "\t\tCount: 2886.0\n",
            "\t\tLM Score: 71.41823119198074\n",
            "\t\tSS Score: 62.32725281999703\n",
            "\t\tICAT Score: 53.810419354769856\n",
            "\treligion\n",
            "\t\tCount: 237.0\n",
            "\t\tLM Score: 70.89655172413792\n",
            "\t\tSS Score: 50.39080459770114\n",
            "\t\tICAT Score: 70.34241775663892\n",
            "\toverall\n",
            "\t\tCount: 2106.0\n",
            "\t\tLM Score: 73.23031312203761\n",
            "\t\tSS Score: 58.88675149581355\n",
            "\t\tICAT Score: 60.214721228514364\n",
            "intersentence\n",
            "\tgender\n",
            "\t\tCount: 726.0\n",
            "\t\tLM Score: 90.75630187043231\n",
            "\t\tSS Score: 55.57552954292085\n",
            "\t\tICAT Score: 80.63601302473555\n",
            "\tprofession\n",
            "\t\tCount: 2481.0\n",
            "\t\tLM Score: 84.54030118249621\n",
            "\t\tSS Score: 57.08551131231887\n",
            "\t\tICAT Score: 72.56007597498778\n",
            "\trace\n",
            "\t\tCount: 2928.0\n",
            "\t\tLM Score: 89.42154515452778\n",
            "\t\tSS Score: 52.79818120661341\n",
            "\t\tICAT Score: 84.41719141217314\n",
            "\treligion\n",
            "\t\tCount: 234.0\n",
            "\t\tLM Score: 91.95306513409962\n",
            "\t\tSS Score: 54.92145593869733\n",
            "\t\tICAT Score: 82.90220596438688\n",
            "\toverall\n",
            "\t\tCount: 2123.0\n",
            "\t\tLM Score: 87.83299841955075\n",
            "\t\tSS Score: 54.858475013328494\n",
            "\t\tICAT Score: 79.29830985620858\n",
            "overall\n",
            "\tCount: 4229.0\n",
            "\tLM Score: 80.53251297991152\n",
            "\tSS Score: 56.9166988753297\n",
            "\tICAT Score: 69.39213014079894\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL4bqdI1yxoa"
      },
      "source": [
        "#Evaluation of triple-adapter fusion (mnli,qqp,qnli)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIuy9GGFyV6u"
      },
      "source": [
        "import os\n",
        "try:\n",
        "    os.makedirs('/content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/triple-fusion/')\n",
        "except OSError as e:\n",
        "        pass"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZMdlNszyZ-l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce3efd51-62f4-4f9c-98dc-7121506c1334"
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/evaluation.py --load-path_model /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/fusion/pytorch_model_adapter_fusion.bin --load-path_adapter /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/fusion/pytorch_model_adapter_fusion.bin --input-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --output-dir /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/triple-fusion/ --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/triple-fusion/predictions.json\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json...\n",
            "---------------------------------------------------------------\n",
            "\u001b[96m                     ARGUMENTS                 \u001b[0m\n",
            "\u001b[96mPretrained Model:\u001b[0m /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/fusion/pytorch_model_adapter_fusion.bin\n",
            "\u001b[96mSkip Intrasentence:\u001b[0m False\n",
            "\u001b[96mSkip Intersentence:\u001b[0m False\n",
            "\u001b[96mBatch Size:\u001b[0m 1\n",
            "\u001b[96mMax Seq Length:\u001b[0m None\n",
            "\u001b[96mCUDA:\u001b[0m True\n",
            "---------------------------------------------------------------\n",
            "\n",
            "\u001b[94mEvaluating bias on intersentence tasks...\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2160: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
            "  FutureWarning,\n",
            "Maximum sequence length found: -inf\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "dict_keys(['bert.encoder.layer.0.output.adapter_fusion_layer.multinli,qqp,qnli.query.weight', 'bert.encoder.layer.0.output.adapter_fusion_layer.multinli,qqp,qnli.query.bias', 'bert.encoder.layer.0.output.adapter_fusion_layer.multinli,qqp,qnli.key.weight', 'bert.encoder.layer.0.output.adapter_fusion_layer.multinli,qqp,qnli.key.bias', 'bert.encoder.layer.0.output.adapter_fusion_layer.multinli,qqp,qnli.value.weight', 'bert.encoder.layer.1.output.adapter_fusion_layer.multinli,qqp,qnli.query.weight', 'bert.encoder.layer.1.output.adapter_fusion_layer.multinli,qqp,qnli.query.bias', 'bert.encoder.layer.1.output.adapter_fusion_layer.multinli,qqp,qnli.key.weight', 'bert.encoder.layer.1.output.adapter_fusion_layer.multinli,qqp,qnli.key.bias', 'bert.encoder.layer.1.output.adapter_fusion_layer.multinli,qqp,qnli.value.weight', 'bert.encoder.layer.2.output.adapter_fusion_layer.multinli,qqp,qnli.query.weight', 'bert.encoder.layer.2.output.adapter_fusion_layer.multinli,qqp,qnli.query.bias', 'bert.encoder.layer.2.output.adapter_fusion_layer.multinli,qqp,qnli.key.weight', 'bert.encoder.layer.2.output.adapter_fusion_layer.multinli,qqp,qnli.key.bias', 'bert.encoder.layer.2.output.adapter_fusion_layer.multinli,qqp,qnli.value.weight', 'bert.encoder.layer.3.output.adapter_fusion_layer.multinli,qqp,qnli.query.weight', 'bert.encoder.layer.3.output.adapter_fusion_layer.multinli,qqp,qnli.query.bias', 'bert.encoder.layer.3.output.adapter_fusion_layer.multinli,qqp,qnli.key.weight', 'bert.encoder.layer.3.output.adapter_fusion_layer.multinli,qqp,qnli.key.bias', 'bert.encoder.layer.3.output.adapter_fusion_layer.multinli,qqp,qnli.value.weight', 'bert.encoder.layer.4.output.adapter_fusion_layer.multinli,qqp,qnli.query.weight', 'bert.encoder.layer.4.output.adapter_fusion_layer.multinli,qqp,qnli.query.bias', 'bert.encoder.layer.4.output.adapter_fusion_layer.multinli,qqp,qnli.key.weight', 'bert.encoder.layer.4.output.adapter_fusion_layer.multinli,qqp,qnli.key.bias', 'bert.encoder.layer.4.output.adapter_fusion_layer.multinli,qqp,qnli.value.weight', 'bert.encoder.layer.5.output.adapter_fusion_layer.multinli,qqp,qnli.query.weight', 'bert.encoder.layer.5.output.adapter_fusion_layer.multinli,qqp,qnli.query.bias', 'bert.encoder.layer.5.output.adapter_fusion_layer.multinli,qqp,qnli.key.weight', 'bert.encoder.layer.5.output.adapter_fusion_layer.multinli,qqp,qnli.key.bias', 'bert.encoder.layer.5.output.adapter_fusion_layer.multinli,qqp,qnli.value.weight', 'bert.encoder.layer.6.output.adapter_fusion_layer.multinli,qqp,qnli.query.weight', 'bert.encoder.layer.6.output.adapter_fusion_layer.multinli,qqp,qnli.query.bias', 'bert.encoder.layer.6.output.adapter_fusion_layer.multinli,qqp,qnli.key.weight', 'bert.encoder.layer.6.output.adapter_fusion_layer.multinli,qqp,qnli.key.bias', 'bert.encoder.layer.6.output.adapter_fusion_layer.multinli,qqp,qnli.value.weight', 'bert.encoder.layer.7.output.adapter_fusion_layer.multinli,qqp,qnli.query.weight', 'bert.encoder.layer.7.output.adapter_fusion_layer.multinli,qqp,qnli.query.bias', 'bert.encoder.layer.7.output.adapter_fusion_layer.multinli,qqp,qnli.key.weight', 'bert.encoder.layer.7.output.adapter_fusion_layer.multinli,qqp,qnli.key.bias', 'bert.encoder.layer.7.output.adapter_fusion_layer.multinli,qqp,qnli.value.weight', 'bert.encoder.layer.8.output.adapter_fusion_layer.multinli,qqp,qnli.query.weight', 'bert.encoder.layer.8.output.adapter_fusion_layer.multinli,qqp,qnli.query.bias', 'bert.encoder.layer.8.output.adapter_fusion_layer.multinli,qqp,qnli.key.weight', 'bert.encoder.layer.8.output.adapter_fusion_layer.multinli,qqp,qnli.key.bias', 'bert.encoder.layer.8.output.adapter_fusion_layer.multinli,qqp,qnli.value.weight', 'bert.encoder.layer.9.output.adapter_fusion_layer.multinli,qqp,qnli.query.weight', 'bert.encoder.layer.9.output.adapter_fusion_layer.multinli,qqp,qnli.query.bias', 'bert.encoder.layer.9.output.adapter_fusion_layer.multinli,qqp,qnli.key.weight', 'bert.encoder.layer.9.output.adapter_fusion_layer.multinli,qqp,qnli.key.bias', 'bert.encoder.layer.9.output.adapter_fusion_layer.multinli,qqp,qnli.value.weight', 'bert.encoder.layer.10.output.adapter_fusion_layer.multinli,qqp,qnli.query.weight', 'bert.encoder.layer.10.output.adapter_fusion_layer.multinli,qqp,qnli.query.bias', 'bert.encoder.layer.10.output.adapter_fusion_layer.multinli,qqp,qnli.key.weight', 'bert.encoder.layer.10.output.adapter_fusion_layer.multinli,qqp,qnli.key.bias', 'bert.encoder.layer.10.output.adapter_fusion_layer.multinli,qqp,qnli.value.weight', 'bert.encoder.layer.11.output.adapter_fusion_layer.multinli,qqp,qnli.query.weight', 'bert.encoder.layer.11.output.adapter_fusion_layer.multinli,qqp,qnli.query.bias', 'bert.encoder.layer.11.output.adapter_fusion_layer.multinli,qqp,qnli.key.weight', 'bert.encoder.layer.11.output.adapter_fusion_layer.multinli,qqp,qnli.key.bias', 'bert.encoder.layer.11.output.adapter_fusion_layer.multinli,qqp,qnli.value.weight'])\n",
            "100% 6369/6369 [01:34<00:00, 67.11it/s]\n",
            "\n",
            "\u001b[91mEvaluating bias on intrasentence tasks...\u001b[0m\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "  0% 0/6318 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "100% 6318/6318 [01:35<00:00, 65.82it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TxQY_h5ylAg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "452f7796-b754-4d75-fab7-9e49c031a948"
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/score_evaluation.py --gold-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --predictions-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/triple-fusion/predictions.json --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/triple-fusion/summary.json"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "intrasentence\n",
            "\tgender\n",
            "\t\tCount: 765.0\n",
            "\t\tLM Score: 59.67342818212383\n",
            "\t\tSS Score: 46.56734843256582\n",
            "\t\tICAT Score: 55.57666644645307\n",
            "\tprofession\n",
            "\t\tCount: 2430.0\n",
            "\t\tLM Score: 63.40723430511036\n",
            "\t\tSS Score: 51.22606827026834\n",
            "\t\tICAT Score: 61.85240234337103\n",
            "\trace\n",
            "\t\tCount: 2886.0\n",
            "\t\tLM Score: 73.59928914130745\n",
            "\t\tSS Score: 49.94537236017638\n",
            "\t\tICAT Score: 73.51887803213774\n",
            "\treligion\n",
            "\t\tCount: 237.0\n",
            "\t\tLM Score: 64.52873563218391\n",
            "\t\tSS Score: 43.12643678160919\n",
            "\t\tICAT Score: 55.657888756771044\n",
            "\toverall\n",
            "\t\tCount: 2106.0\n",
            "\t\tLM Score: 67.62166996149581\n",
            "\t\tSS Score: 49.74516769297325\n",
            "\t\tICAT Score: 67.27702623827001\n",
            "intersentence\n",
            "\tgender\n",
            "\t\tCount: 726.0\n",
            "\t\tLM Score: 11.386654810567851\n",
            "\t\tSS Score: 38.768270779140344\n",
            "\t\tICAT Score: 8.828818339293909\n",
            "\tprofession\n",
            "\t\tCount: 2481.0\n",
            "\t\tLM Score: 18.136828924175983\n",
            "\t\tSS Score: 42.843049644569845\n",
            "\t\tICAT Score: 15.54074123987084\n",
            "\trace\n",
            "\t\tCount: 2928.0\n",
            "\t\tLM Score: 13.229605442187324\n",
            "\t\tSS Score: 48.515644779240134\n",
            "\t\tICAT Score: 12.836856764053246\n",
            "\treligion\n",
            "\t\tCount: 234.0\n",
            "\t\tLM Score: 12.10249042145594\n",
            "\t\tSS Score: 40.224137931034484\n",
            "\t\tICAT Score: 9.736244880433347\n",
            "\toverall\n",
            "\t\tCount: 2123.0\n",
            "\t\tLM Score: 14.817021303975563\n",
            "\t\tSS Score: 44.812782569294264\n",
            "\t\tICAT Score: 13.279839080393158\n",
            "overall\n",
            "\tCount: 4229.0\n",
            "\tLM Score: 41.12265259114933\n",
            "\tSS Score: 47.32667017153748\n",
            "\tICAT Score: 38.92396431520091\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zg3mDFb8HKf3"
      },
      "source": [
        "import os\n",
        "try:\n",
        "    os.makedirs('/content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/multinlipretrained_fusion/')\n",
        "except OSError as e:\n",
        "        pass"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKT4QQBUHX5D",
        "outputId": "78b4e072-03c6-41f9-923b-a79ec120b16d"
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/evaluation.py --load-path_model /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/multinlipretrained_fusion/pytorch_adapter.bin --load-path_adapter /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/multinlipretrained_fusion/pytorch_adapter.bin --input-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --output-dir /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/multinlipretrained_fusion/ --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/multinlipretrained_fusion/predictions.json\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json...\n",
            "---------------------------------------------------------------\n",
            "\u001b[96m                     ARGUMENTS                 \u001b[0m\n",
            "\u001b[96mPretrained Model:\u001b[0m /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/multinlipretrained_fusion/pytorch_adapter.bin\n",
            "\u001b[96mSkip Intrasentence:\u001b[0m False\n",
            "\u001b[96mSkip Intersentence:\u001b[0m False\n",
            "\u001b[96mBatch Size:\u001b[0m 1\n",
            "\u001b[96mMax Seq Length:\u001b[0m None\n",
            "\u001b[96mCUDA:\u001b[0m True\n",
            "---------------------------------------------------------------\n",
            "\n",
            "\u001b[94mEvaluating bias on intersentence tasks...\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2160: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
            "  FutureWarning,\n",
            "Maximum sequence length found: -inf\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "dict_keys(['bert.encoder.layer.0.output.adapters.mnli.adapter_down.0.weight', 'bert.encoder.layer.0.output.adapters.mnli.adapter_down.0.bias', 'bert.encoder.layer.0.output.adapters.mnli.adapter_up.weight', 'bert.encoder.layer.0.output.adapters.mnli.adapter_up.bias', 'bert.encoder.layer.1.output.adapters.mnli.adapter_down.0.weight', 'bert.encoder.layer.1.output.adapters.mnli.adapter_down.0.bias', 'bert.encoder.layer.1.output.adapters.mnli.adapter_up.weight', 'bert.encoder.layer.1.output.adapters.mnli.adapter_up.bias', 'bert.encoder.layer.2.output.adapters.mnli.adapter_down.0.weight', 'bert.encoder.layer.2.output.adapters.mnli.adapter_down.0.bias', 'bert.encoder.layer.2.output.adapters.mnli.adapter_up.weight', 'bert.encoder.layer.2.output.adapters.mnli.adapter_up.bias', 'bert.encoder.layer.3.output.adapters.mnli.adapter_down.0.weight', 'bert.encoder.layer.3.output.adapters.mnli.adapter_down.0.bias', 'bert.encoder.layer.3.output.adapters.mnli.adapter_up.weight', 'bert.encoder.layer.3.output.adapters.mnli.adapter_up.bias', 'bert.encoder.layer.4.output.adapters.mnli.adapter_down.0.weight', 'bert.encoder.layer.4.output.adapters.mnli.adapter_down.0.bias', 'bert.encoder.layer.4.output.adapters.mnli.adapter_up.weight', 'bert.encoder.layer.4.output.adapters.mnli.adapter_up.bias', 'bert.encoder.layer.5.output.adapters.mnli.adapter_down.0.weight', 'bert.encoder.layer.5.output.adapters.mnli.adapter_down.0.bias', 'bert.encoder.layer.5.output.adapters.mnli.adapter_up.weight', 'bert.encoder.layer.5.output.adapters.mnli.adapter_up.bias', 'bert.encoder.layer.6.output.adapters.mnli.adapter_down.0.weight', 'bert.encoder.layer.6.output.adapters.mnli.adapter_down.0.bias', 'bert.encoder.layer.6.output.adapters.mnli.adapter_up.weight', 'bert.encoder.layer.6.output.adapters.mnli.adapter_up.bias', 'bert.encoder.layer.7.output.adapters.mnli.adapter_down.0.weight', 'bert.encoder.layer.7.output.adapters.mnli.adapter_down.0.bias', 'bert.encoder.layer.7.output.adapters.mnli.adapter_up.weight', 'bert.encoder.layer.7.output.adapters.mnli.adapter_up.bias', 'bert.encoder.layer.8.output.adapters.mnli.adapter_down.0.weight', 'bert.encoder.layer.8.output.adapters.mnli.adapter_down.0.bias', 'bert.encoder.layer.8.output.adapters.mnli.adapter_up.weight', 'bert.encoder.layer.8.output.adapters.mnli.adapter_up.bias', 'bert.encoder.layer.9.output.adapters.mnli.adapter_down.0.weight', 'bert.encoder.layer.9.output.adapters.mnli.adapter_down.0.bias', 'bert.encoder.layer.9.output.adapters.mnli.adapter_up.weight', 'bert.encoder.layer.9.output.adapters.mnli.adapter_up.bias', 'bert.encoder.layer.10.output.adapters.mnli.adapter_down.0.weight', 'bert.encoder.layer.10.output.adapters.mnli.adapter_down.0.bias', 'bert.encoder.layer.10.output.adapters.mnli.adapter_up.weight', 'bert.encoder.layer.10.output.adapters.mnli.adapter_up.bias', 'bert.encoder.layer.11.output.adapters.mnli.adapter_down.0.weight', 'bert.encoder.layer.11.output.adapters.mnli.adapter_down.0.bias', 'bert.encoder.layer.11.output.adapters.mnli.adapter_up.weight', 'bert.encoder.layer.11.output.adapters.mnli.adapter_up.bias'])\n",
            "100% 6369/6369 [01:32<00:00, 68.57it/s]\n",
            "\n",
            "\u001b[91mEvaluating bias on intrasentence tasks...\u001b[0m\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "100% 6318/6318 [01:35<00:00, 66.44it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFtbbi1_HyYU",
        "outputId": "8e2bc0de-0b2a-42c6-957c-019f13e12261"
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/score_evaluation.py --gold-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --predictions-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/multinlipretrained_fusion/predictions.json --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/multinlipretrained_fusion/summary.json"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "intrasentence\n",
            "\tgender\n",
            "\t\tCount: 765.0\n",
            "\t\tLM Score: 41.29119270423618\n",
            "\t\tSS Score: 48.762330101460535\n",
            "\t\tICAT Score: 40.26909537853967\n",
            "\tprofession\n",
            "\t\tCount: 2430.0\n",
            "\t\tLM Score: 34.80582617357475\n",
            "\t\tSS Score: 46.91457036532544\n",
            "\t\tICAT Score: 32.65800762286917\n",
            "\trace\n",
            "\t\tCount: 2886.0\n",
            "\t\tLM Score: 23.1747892309369\n",
            "\t\tSS Score: 49.022484605487456\n",
            "\t\tICAT Score: 22.72171496618041\n",
            "\treligion\n",
            "\t\tCount: 237.0\n",
            "\t\tLM Score: 31.471264367816094\n",
            "\t\tSS Score: 51.54022988505747\n",
            "\t\tICAT Score: 30.501804729819\n",
            "\toverall\n",
            "\t\tCount: 2106.0\n",
            "\t\tLM Score: 30.19991035021242\n",
            "\t\tSS Score: 48.28469047376063\n",
            "\t\tICAT Score: 29.163866471906534\n",
            "intersentence\n",
            "\tgender\n",
            "\t\tCount: 726.0\n",
            "\t\tLM Score: 10.592141353010916\n",
            "\t\tSS Score: 43.881921218877736\n",
            "\t\tICAT Score: 9.296070247840841\n",
            "\tprofession\n",
            "\t\tCount: 2481.0\n",
            "\t\tLM Score: 14.718526579093874\n",
            "\t\tSS Score: 40.8062113451991\n",
            "\t\tICAT Score: 12.012146125528698\n",
            "\trace\n",
            "\t\tCount: 2928.0\n",
            "\t\tLM Score: 12.878575095706786\n",
            "\t\tSS Score: 42.28446403795557\n",
            "\t\tICAT Score: 10.891272909890477\n",
            "\treligion\n",
            "\t\tCount: 234.0\n",
            "\t\tLM Score: 10.796934865900383\n",
            "\t\tSS Score: 43.56130268199234\n",
            "\t\tICAT Score: 9.406570954624858\n",
            "\toverall\n",
            "\t\tCount: 2123.0\n",
            "\t\tLM Score: 13.208819227165453\n",
            "\t\tSS Score: 41.97379956907758\n",
            "\t\tICAT Score: 11.088486615704419\n",
            "overall\n",
            "\tCount: 4229.0\n",
            "\tLM Score: 21.720595293879956\n",
            "\tSS Score: 45.11441388966484\n",
            "\tICAT Score: 19.598238520360134\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbBfXsaHIAgj"
      },
      "source": [
        "Single Adapter qqp\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmuxqcbyH9td"
      },
      "source": [
        "import os\n",
        "try:\n",
        "    os.makedirs('/content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/qqppretrained_fusion/')\n",
        "except OSError as e:\n",
        "        pass"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vo2ev_J2IVEI",
        "outputId": "b8d975fc-7096-42ad-ced0-d253164034ae"
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/evaluation.py --load-path_model /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/qqppretrained/pytorch_adapter.bin --load-path_adapter /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/qqppretrained/pytorch_adapter.bin --input-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --output-dir /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/qqppretrained_fusion/ --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/qqppretrained_fusion/predictions.json\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json...\n",
            "---------------------------------------------------------------\n",
            "\u001b[96m                     ARGUMENTS                 \u001b[0m\n",
            "\u001b[96mPretrained Model:\u001b[0m /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/qqppretrained/pytorch_adapter.bin\n",
            "\u001b[96mSkip Intrasentence:\u001b[0m False\n",
            "\u001b[96mSkip Intersentence:\u001b[0m False\n",
            "\u001b[96mBatch Size:\u001b[0m 1\n",
            "\u001b[96mMax Seq Length:\u001b[0m None\n",
            "\u001b[96mCUDA:\u001b[0m True\n",
            "---------------------------------------------------------------\n",
            "\n",
            "\u001b[94mEvaluating bias on intersentence tasks...\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2160: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
            "  FutureWarning,\n",
            "Maximum sequence length found: -inf\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "dict_keys(['bert.encoder.layer.0.output.adapters.qqp.adapter_down.0.weight', 'bert.encoder.layer.0.output.adapters.qqp.adapter_down.0.bias', 'bert.encoder.layer.0.output.adapters.qqp.adapter_up.weight', 'bert.encoder.layer.0.output.adapters.qqp.adapter_up.bias', 'bert.encoder.layer.1.output.adapters.qqp.adapter_down.0.weight', 'bert.encoder.layer.1.output.adapters.qqp.adapter_down.0.bias', 'bert.encoder.layer.1.output.adapters.qqp.adapter_up.weight', 'bert.encoder.layer.1.output.adapters.qqp.adapter_up.bias', 'bert.encoder.layer.2.output.adapters.qqp.adapter_down.0.weight', 'bert.encoder.layer.2.output.adapters.qqp.adapter_down.0.bias', 'bert.encoder.layer.2.output.adapters.qqp.adapter_up.weight', 'bert.encoder.layer.2.output.adapters.qqp.adapter_up.bias', 'bert.encoder.layer.3.output.adapters.qqp.adapter_down.0.weight', 'bert.encoder.layer.3.output.adapters.qqp.adapter_down.0.bias', 'bert.encoder.layer.3.output.adapters.qqp.adapter_up.weight', 'bert.encoder.layer.3.output.adapters.qqp.adapter_up.bias', 'bert.encoder.layer.4.output.adapters.qqp.adapter_down.0.weight', 'bert.encoder.layer.4.output.adapters.qqp.adapter_down.0.bias', 'bert.encoder.layer.4.output.adapters.qqp.adapter_up.weight', 'bert.encoder.layer.4.output.adapters.qqp.adapter_up.bias', 'bert.encoder.layer.5.output.adapters.qqp.adapter_down.0.weight', 'bert.encoder.layer.5.output.adapters.qqp.adapter_down.0.bias', 'bert.encoder.layer.5.output.adapters.qqp.adapter_up.weight', 'bert.encoder.layer.5.output.adapters.qqp.adapter_up.bias', 'bert.encoder.layer.6.output.adapters.qqp.adapter_down.0.weight', 'bert.encoder.layer.6.output.adapters.qqp.adapter_down.0.bias', 'bert.encoder.layer.6.output.adapters.qqp.adapter_up.weight', 'bert.encoder.layer.6.output.adapters.qqp.adapter_up.bias', 'bert.encoder.layer.7.output.adapters.qqp.adapter_down.0.weight', 'bert.encoder.layer.7.output.adapters.qqp.adapter_down.0.bias', 'bert.encoder.layer.7.output.adapters.qqp.adapter_up.weight', 'bert.encoder.layer.7.output.adapters.qqp.adapter_up.bias', 'bert.encoder.layer.8.output.adapters.qqp.adapter_down.0.weight', 'bert.encoder.layer.8.output.adapters.qqp.adapter_down.0.bias', 'bert.encoder.layer.8.output.adapters.qqp.adapter_up.weight', 'bert.encoder.layer.8.output.adapters.qqp.adapter_up.bias', 'bert.encoder.layer.9.output.adapters.qqp.adapter_down.0.weight', 'bert.encoder.layer.9.output.adapters.qqp.adapter_down.0.bias', 'bert.encoder.layer.9.output.adapters.qqp.adapter_up.weight', 'bert.encoder.layer.9.output.adapters.qqp.adapter_up.bias', 'bert.encoder.layer.10.output.adapters.qqp.adapter_down.0.weight', 'bert.encoder.layer.10.output.adapters.qqp.adapter_down.0.bias', 'bert.encoder.layer.10.output.adapters.qqp.adapter_up.weight', 'bert.encoder.layer.10.output.adapters.qqp.adapter_up.bias', 'bert.encoder.layer.11.output.adapters.qqp.adapter_down.0.weight', 'bert.encoder.layer.11.output.adapters.qqp.adapter_down.0.bias', 'bert.encoder.layer.11.output.adapters.qqp.adapter_up.weight', 'bert.encoder.layer.11.output.adapters.qqp.adapter_up.bias'])\n",
            "100% 6369/6369 [01:33<00:00, 68.08it/s]\n",
            "\n",
            "\u001b[91mEvaluating bias on intrasentence tasks...\u001b[0m\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "100% 6318/6318 [01:35<00:00, 66.35it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLGD_HgeIqEa",
        "outputId": "e1b1375d-91f1-4a0f-9a53-cacaba76b03d"
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/score_evaluation.py --gold-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --predictions-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/qqppretrained_fusion/predictions.json --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/qqppretrained_fusion/summary.json"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "intrasentence\n",
            "\tgender\n",
            "\t\tCount: 765.0\n",
            "\t\tLM Score: 37.471496136713526\n",
            "\t\tSS Score: 43.57063773585513\n",
            "\t\tICAT Score: 32.6531396718648\n",
            "\tprofession\n",
            "\t\tCount: 2430.0\n",
            "\t\tLM Score: 38.67263472988977\n",
            "\t\tSS Score: 49.356227085149115\n",
            "\t\tICAT Score: 38.17470683418927\n",
            "\trace\n",
            "\t\tCount: 2886.0\n",
            "\t\tLM Score: 50.3732566194645\n",
            "\t\tSS Score: 41.547899035377746\n",
            "\t\tICAT Score: 41.85805960217369\n",
            "\treligion\n",
            "\t\tCount: 237.0\n",
            "\t\tLM Score: 55.356321839080465\n",
            "\t\tSS Score: 52.27586206896552\n",
            "\t\tICAT Score: 52.83665477606024\n",
            "\toverall\n",
            "\t\tCount: 2106.0\n",
            "\t\tLM Score: 44.486078570655586\n",
            "\t\tSS Score: 45.17652077713316\n",
            "\t\tICAT Score: 40.194525056808004\n",
            "intersentence\n",
            "\tgender\n",
            "\t\tCount: 726.0\n",
            "\t\tLM Score: 84.27592636831767\n",
            "\t\tSS Score: 57.33989842685494\n",
            "\t\tICAT Score: 71.90439158086653\n",
            "\tprofession\n",
            "\t\tCount: 2481.0\n",
            "\t\tLM Score: 79.97464717405391\n",
            "\t\tSS Score: 61.16316833923433\n",
            "\t\tICAT Score: 62.11923818855721\n",
            "\trace\n",
            "\t\tCount: 2928.0\n",
            "\t\tLM Score: 84.08107950815898\n",
            "\t\tSS Score: 62.14547830863799\n",
            "\t\tICAT Score: 63.65698096149475\n",
            "\treligion\n",
            "\t\tCount: 234.0\n",
            "\t\tLM Score: 84.26532567049809\n",
            "\t\tSS Score: 60.446360153256705\n",
            "\t\tICAT Score: 66.66000686278828\n",
            "\toverall\n",
            "\t\tCount: 2123.0\n",
            "\t\tLM Score: 82.55333567354445\n",
            "\t\tSS Score: 61.09962448121921\n",
            "\t\tICAT Score: 64.22711516057683\n",
            "overall\n",
            "\tCount: 4229.0\n",
            "\tLM Score: 63.52896424915696\n",
            "\tSS Score: 53.15236045944013\n",
            "\tICAT Score: 59.52364035059239\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "For2sjt8I3LJ"
      },
      "source": [
        "# Single adapter qnli"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rdmUoAjI19i"
      },
      "source": [
        "import os\n",
        "try:\n",
        "    os.makedirs('/content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/qnliretrained_fusion/')\n",
        "except OSError as e:\n",
        "        pass"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyiOmBDLJFaQ",
        "outputId": "44ddd0d8-5041-4e68-ef13-e61744237065"
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/evaluation.py --load-path_model /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/qnlipretrained/pytorch_adapter.bin --load-path_adapter /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/qnlipretrained/pytorch_adapter.bin --input-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --output-dir /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/qqppretrained_fusion/ --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/qqppretrained_fusion/predictions.json\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json...\n",
            "---------------------------------------------------------------\n",
            "\u001b[96m                     ARGUMENTS                 \u001b[0m\n",
            "\u001b[96mPretrained Model:\u001b[0m /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/qnlipretrained/pytorch_adapter.bin\n",
            "\u001b[96mSkip Intrasentence:\u001b[0m False\n",
            "\u001b[96mSkip Intersentence:\u001b[0m False\n",
            "\u001b[96mBatch Size:\u001b[0m 1\n",
            "\u001b[96mMax Seq Length:\u001b[0m None\n",
            "\u001b[96mCUDA:\u001b[0m True\n",
            "---------------------------------------------------------------\n",
            "\n",
            "\u001b[94mEvaluating bias on intersentence tasks...\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2160: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
            "  FutureWarning,\n",
            "Maximum sequence length found: -inf\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Number of parameters: 109,483,778\n",
            "dict_keys(['bert.encoder.layer.0.output.adapters.qnli.adapter_down.0.weight', 'bert.encoder.layer.0.output.adapters.qnli.adapter_down.0.bias', 'bert.encoder.layer.0.output.adapters.qnli.adapter_up.weight', 'bert.encoder.layer.0.output.adapters.qnli.adapter_up.bias', 'bert.encoder.layer.1.output.adapters.qnli.adapter_down.0.weight', 'bert.encoder.layer.1.output.adapters.qnli.adapter_down.0.bias', 'bert.encoder.layer.1.output.adapters.qnli.adapter_up.weight', 'bert.encoder.layer.1.output.adapters.qnli.adapter_up.bias', 'bert.encoder.layer.2.output.adapters.qnli.adapter_down.0.weight', 'bert.encoder.layer.2.output.adapters.qnli.adapter_down.0.bias', 'bert.encoder.layer.2.output.adapters.qnli.adapter_up.weight', 'bert.encoder.layer.2.output.adapters.qnli.adapter_up.bias', 'bert.encoder.layer.3.output.adapters.qnli.adapter_down.0.weight', 'bert.encoder.layer.3.output.adapters.qnli.adapter_down.0.bias', 'bert.encoder.layer.3.output.adapters.qnli.adapter_up.weight', 'bert.encoder.layer.3.output.adapters.qnli.adapter_up.bias', 'bert.encoder.layer.4.output.adapters.qnli.adapter_down.0.weight', 'bert.encoder.layer.4.output.adapters.qnli.adapter_down.0.bias', 'bert.encoder.layer.4.output.adapters.qnli.adapter_up.weight', 'bert.encoder.layer.4.output.adapters.qnli.adapter_up.bias', 'bert.encoder.layer.5.output.adapters.qnli.adapter_down.0.weight', 'bert.encoder.layer.5.output.adapters.qnli.adapter_down.0.bias', 'bert.encoder.layer.5.output.adapters.qnli.adapter_up.weight', 'bert.encoder.layer.5.output.adapters.qnli.adapter_up.bias', 'bert.encoder.layer.6.output.adapters.qnli.adapter_down.0.weight', 'bert.encoder.layer.6.output.adapters.qnli.adapter_down.0.bias', 'bert.encoder.layer.6.output.adapters.qnli.adapter_up.weight', 'bert.encoder.layer.6.output.adapters.qnli.adapter_up.bias', 'bert.encoder.layer.7.output.adapters.qnli.adapter_down.0.weight', 'bert.encoder.layer.7.output.adapters.qnli.adapter_down.0.bias', 'bert.encoder.layer.7.output.adapters.qnli.adapter_up.weight', 'bert.encoder.layer.7.output.adapters.qnli.adapter_up.bias', 'bert.encoder.layer.8.output.adapters.qnli.adapter_down.0.weight', 'bert.encoder.layer.8.output.adapters.qnli.adapter_down.0.bias', 'bert.encoder.layer.8.output.adapters.qnli.adapter_up.weight', 'bert.encoder.layer.8.output.adapters.qnli.adapter_up.bias', 'bert.encoder.layer.9.output.adapters.qnli.adapter_down.0.weight', 'bert.encoder.layer.9.output.adapters.qnli.adapter_down.0.bias', 'bert.encoder.layer.9.output.adapters.qnli.adapter_up.weight', 'bert.encoder.layer.9.output.adapters.qnli.adapter_up.bias', 'bert.encoder.layer.10.output.adapters.qnli.adapter_down.0.weight', 'bert.encoder.layer.10.output.adapters.qnli.adapter_down.0.bias', 'bert.encoder.layer.10.output.adapters.qnli.adapter_up.weight', 'bert.encoder.layer.10.output.adapters.qnli.adapter_up.bias', 'bert.encoder.layer.11.output.adapters.qnli.adapter_down.0.weight', 'bert.encoder.layer.11.output.adapters.qnli.adapter_down.0.bias', 'bert.encoder.layer.11.output.adapters.qnli.adapter_up.weight', 'bert.encoder.layer.11.output.adapters.qnli.adapter_up.bias'])\n",
            " 43% 2740/6369 [00:40<00:54, 66.33it/s]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "td8MLuySJSB_"
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/score_evaluation.py --gold-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --predictions-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/qnlipretrained_fusion/predictions.json --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/qnlipretrained_fusion/summary.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zthE4Z8F7ZPJ"
      },
      "source": [
        "# single imdb pretrained adapter\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmzhKw5N7X5W"
      },
      "source": [
        "import os\n",
        "try:\n",
        "    os.makedirs('/content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/imdb-pretrained/')\n",
        "except OSError as e:\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0ELKp5M7leE"
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/evaluation.py --load-path_model /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/imdb-pretrained/pytorch_adapter.bin --load-path_adapter /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/imdb-pretrained/pytorch_adapter.bin --input-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --output-dir /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/imdb-pretrained/ --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/imdb-pretrained/predictions.json\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHYVNTG3792Z"
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/score_evaluation.py --gold-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --predictions-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/imdb-pretrained/predictions.json --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/imdb-pretrained/summary.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7yU1c9ugamN"
      },
      "source": [
        "## SST Adapter\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nsl9kUigiOxz"
      },
      "source": [
        "import os\n",
        "try:\n",
        "    os.makedirs('/content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sst/')\n",
        "except OSError as e:\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYyEJ2uFW40w"
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/evaluation.py --load-path_model /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/sst/pytorch_model.bin --load-path_adapter /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/sst/pytorch_adapter.bin --input-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --output-dir /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sst/ --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sst/predictions.json\n",
        "#!python3 evaluation.py --no-cuda False --output-dir /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/ -- skip_intrasentence False --skip_intersentence False --batch_size 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDw_upEajSLL"
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/score_evaluation.py --gold-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --predictions-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sst/predictions.json --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sst/summary.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDFKHWqKlWRE"
      },
      "source": [
        "# RoBERTa\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDjpLNYqlU2x"
      },
      "source": [
        "import os\n",
        "try:\n",
        "    os.makedirs('/content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/roberta/')\n",
        "except OSError as e:\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFZPbqQnfV-v"
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/evaluation.py --load-path_model /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/roberta/pytorch_model_head.bin --load-path_adapter /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/roberta/pytorch_adapter.bin --input-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --output-dir /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/roberta/ --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/roberta/predictions.json\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAxTHS7ylu5C"
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/score_evaluation.py --gold-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --predictions-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/roberta/predictions.json --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/roberta/summary.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0YbepOXJsoO"
      },
      "source": [
        "#Language Poem (pre-trained)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4SnA2w_JyUO"
      },
      "source": [
        "import os\n",
        "try:\n",
        "    os.makedirs('/content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/lmpoem/')\n",
        "except OSError as e:\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3B6lKsCQJ53J"
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/evaluation_generativemodels.py --load-path_model /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/lmpoem/pytorch_adapter.bin --load-path_adapter /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/lmpoem/pytorch_adapter.bin --input-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --output-dir /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/lmpoem/ --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/lmpoem/predictions.json\n",
        "#!python3 evaluation.py --no-cuda False --output-dir /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/ -- skip_intrasentence False --skip_intersentence False --batch_size 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPnSIwspJ8wX"
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/score_evaluation.py --gold-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --predictions-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/lmpoem/predictions.json --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/lmpoem/summary.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjiwCTcAQJib"
      },
      "source": [
        "#Common Sense Winogrande (pre-trained)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iyxrzgq4QQtK"
      },
      "source": [
        "import os\n",
        "try:\n",
        "    os.makedirs('/content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/winogrande/')\n",
        "except OSError as e:\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VOY-ZXwQVoO"
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/evaluation.py --load-path_model /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/winogrande/pytorch_adapter.bin --load-path_adapter /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/winogrande/pytorch_adapter.bin --input-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --output-dir /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/winogrande/ --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/winogrande/predictions.json\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHR_-RszQjwA"
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/score_evaluation.py --gold-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --predictions-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/winogrande/predictions.json --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/winogrande/summary.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmtvxUp-T6FJ"
      },
      "source": [
        "#SST Pretrained\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zinc5psBT8yh"
      },
      "source": [
        "import os\n",
        "try:\n",
        "    os.makedirs('/content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sst-pretrained/')\n",
        "except OSError as e:\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAbECqkiUTS1"
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/evaluation.py --load-path_model /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/sst_pretrained/pytorch_adapter.bin --load-path_adapter /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/sst_pretrained/pytorch_adapter.bin --input-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --output-dir /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sst-pretrained/ --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sst-pretrained/predictions.json\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K60WFJrzUquL"
      },
      "source": [
        "!python3 /content/gdrive/MyDrive/master_hpi/NLP_Project/code/adapter_evaluation/score_evaluation.py --gold-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/dev.json --predictions-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sst-pretrained/predictions.json --output-file /content/gdrive/MyDrive/master_hpi/NLP_Project/code/results/sst-pretrained/summary.json"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}