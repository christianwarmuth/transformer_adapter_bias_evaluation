{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "03_Adapter_Fusion.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI6aVBQRy6LC"
      },
      "source": [
        "This notebook is taken from: https://adapterhub.ml\n",
        "\n",
        "For this guide, we select **CommitmentBank** ([De Marneffe et al., 2019](https://github.com/mcdm/CommitmentBank)), a three-class textual entailment dataset, as our target task. We will fuse [adapters from AdapterHub](https://adapterhub.ml/explore/) which were pre-trained on different tasks. During training, their represantions are kept fix while a newly introduced fusion layer is trained. As our base model, we will use BERT (`bert-base-uncased`). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3L9gYpCV28OA"
      },
      "source": [
        "## Installation\n",
        "\n",
        "Again, we install `adapter-transformers` and HuggingFace's `datasets` library first:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qL3Sq1HQynCq",
        "outputId": "678f9684-b617-4967-83bb-650e6681c07e"
      },
      "source": [
        "!pip install -U adapter-transformers\n",
        "!pip install datasets"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: adapter-transformers in /usr/local/lib/python3.7/dist-packages (2.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (4.6.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (3.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.14 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (0.0.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (21.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (1.19.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (0.0.45)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (4.62.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (0.10.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.14->adapter-transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->adapter-transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->adapter-transformers) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->adapter-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->adapter-transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->adapter-transformers) (1.15.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.11.0)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: tqdm>=4.42 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.6.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.15)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.5.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2Uo9Hy6a6zE",
        "outputId": "8a6b30cb-b697-4d87-c0dc-e534e9ee6fd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBn-2-h9a9wh",
        "outputId": "cef2c40a-d9d8-4bd7-a24f-ad54ffaf800a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive/\", force_remount=True)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQLFHqzabAoO"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/gdrive/MyDrive/master_hpi/NLP_Project/code/')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LERTW5NkbD4F"
      },
      "source": [
        "path = \"/content/gdrive/MyDrive/master_hpi/NLP_Project/code/\"\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fP9OMSUT-FtL"
      },
      "source": [
        "## Dataset Preprocessing\n",
        "\n",
        "Before setting up training, we first prepare the training data. CommimentBank is part of the SuperGLUE benchmark and can be loaded via HuggingFace `datasets` using one line of code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INW7UEhC-I6b",
        "outputId": "58894b57-0470-471c-cc89-25e7f7b2db81"
      },
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"super_glue\", \"cb\")\n",
        "dataset.num_rows"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reusing dataset super_glue (/root/.cache/huggingface/datasets/super_glue/cb/1.0.2/2fb163bca9085c1deb906aff20f00c242227ff704a4e8c9cfdfe820be3abfc83)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test': 250, 'train': 250, 'validation': 56}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epiKaEz5dDVe"
      },
      "source": [
        "Every dataset sample has a premise, a hypothesis and a three-class class label:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifu4q5IJ-hYI",
        "outputId": "663803c9-aa81-4919-fa5a-e0c44c36e6a2"
      },
      "source": [
        "dataset['train'].features"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'hypothesis': Value(dtype='string', id=None),\n",
              " 'idx': Value(dtype='int32', id=None),\n",
              " 'label': ClassLabel(num_classes=3, names=['entailment', 'contradiction', 'neutral'], names_file=None, id=None),\n",
              " 'premise': Value(dtype='string', id=None)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVa3Vk0QdNYI"
      },
      "source": [
        "Now, we need to encode all dataset samples to valid inputs for our `bert-base-uncased` model. Using `dataset.map()`, we can pass the full dataset through the tokenizer in batches:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEnRCQfE_Oi3",
        "outputId": "3279cb8b-1f61-4df2-9319-b942cb29ac0a"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def encode_batch(batch):\n",
        "  \"\"\"Encodes a batch of input data using the model tokenizer.\"\"\"\n",
        "  return tokenizer(\n",
        "      batch[\"premise\"],\n",
        "      batch[\"hypothesis\"],\n",
        "      max_length=180,\n",
        "      truncation=True,\n",
        "      padding=\"max_length\"\n",
        "  )\n",
        "\n",
        "# Encode the input data\n",
        "dataset = dataset.map(encode_batch, batched=True)\n",
        "# The transformers model expects the target class column to be named \"labels\"\n",
        "dataset.rename_column_(\"label\", \"labels\")\n",
        "# Transform to pytorch tensors and only output the required columns\n",
        "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/super_glue/cb/1.0.2/2fb163bca9085c1deb906aff20f00c242227ff704a4e8c9cfdfe820be3abfc83/cache-408ae8ee29281e21.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/super_glue/cb/1.0.2/2fb163bca9085c1deb906aff20f00c242227ff704a4e8c9cfdfe820be3abfc83/cache-38c0a8b28f724692.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/super_glue/cb/1.0.2/2fb163bca9085c1deb906aff20f00c242227ff704a4e8c9cfdfe820be3abfc83/cache-fe05e0365097e891.arrow\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsXEy1TNeAnI"
      },
      "source": [
        "New we're ready to setup AdapterFusion..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xs21MzEQ_0v4"
      },
      "source": [
        "## Fusion Training\n",
        "\n",
        "We use a pre-trained BERT model from HuggingFace and instantiate our model using `BertModelWithHeads`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnq8n_KP_3aX",
        "outputId": "a384f4fe-6795-4e4a-bf18-f4d090f8bfe7"
      },
      "source": [
        "from transformers import BertConfig, BertModelWithHeads\n",
        "\n",
        "id2label = {id: label for (id, label) in enumerate(dataset[\"train\"].features[\"labels\"].names)}\n",
        "\n",
        "config = BertConfig.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    id2label=id2label,\n",
        ")\n",
        "model = BertModelWithHeads.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    config=config,\n",
        ")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"adapters\": {\n",
            "    \"adapters\": {},\n",
            "    \"config_map\": {}\n",
            "  },\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"entailment\",\n",
            "    \"1\": \"contradiction\",\n",
            "    \"2\": \"neutral\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.8.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of BertModelWithHeads were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModelWithHeads for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDCNJzTXezcn"
      },
      "source": [
        "Now we have everything set up to load our _AdapterFusion_ setup. First, we load three adapters pre-trained on different tasks from the Hub: MultiNLI, QQP and QNLI. As we don't need their prediction heads, we pass `with_head=False` to the loading method. Next, we add a new fusion layer that combines all the adapters we've just loaded. Finally, we add a new classification head for our target task on top."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRqbBgS0BoHJ",
        "outputId": "635d9f72-5794-4a1a-ef76-32f33a5e7c6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from transformers.adapters.composition import Fuse\n",
        "\n",
        "# Load the pre-trained adapters we want to fuse\n",
        "model.load_adapter(\"nli/multinli@ukp\", load_as=\"multinli\", with_head=False)\n",
        "model.load_adapter(\"sts/qqp@ukp\", with_head=False)\n",
        "model.load_adapter(\"nli/qnli@ukp\", with_head=False)\n",
        "# Add a fusion layer for all loaded adapters\n",
        "model.add_adapter_fusion(Fuse(\"multinli\", \"qqp\", \"qnli\"))\n",
        "model.set_active_adapters(Fuse(\"multinli\", \"qqp\", \"qnli\"))\n",
        "\n",
        "# Add a classification head for our target task\n",
        "model.add_classification_head(\"cb\", num_labels=len(id2label))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No exactly matching adapter config found for this specifier, falling back to default.\n",
            "Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_task/mnli/bert-base-uncased/pfeiffer/bert-base-uncased_nli_multinli_pfeiffer.zip.\n",
            "Loading module configuration from ~/.cache/torch/adapters/2e2b596bb3b1b6db529d746b87272bda4c8892b0a26f6a960553852cc4378654-5ad6785a3c6c5d82b0a96c3612e27fccc2f710cd379f14326924e06e815c48eb-extracted/adapter_config.json\n",
            "Adding adapter 'multinli'.\n",
            "Loading module weights from ~/.cache/torch/adapters/2e2b596bb3b1b6db529d746b87272bda4c8892b0a26f6a960553852cc4378654-5ad6785a3c6c5d82b0a96c3612e27fccc2f710cd379f14326924e06e815c48eb-extracted/pytorch_adapter.bin\n",
            "Some module weights could not be found in loaded weights file: invertible_adapters.multinli.F.0.weight, invertible_adapters.multinli.F.0.bias, invertible_adapters.multinli.F.2.weight, invertible_adapters.multinli.F.2.bias, invertible_adapters.multinli.G.0.weight, invertible_adapters.multinli.G.0.bias, invertible_adapters.multinli.G.2.weight, invertible_adapters.multinli.G.2.bias\n",
            "No exactly matching adapter config found for this specifier, falling back to default.\n",
            "Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_task/qqp/bert-base-uncased/pfeiffer/bert-base-uncased_sts_qqp_pfeiffer.zip.\n",
            "Loading module configuration from ~/.cache/torch/adapters/a76702bb6d5d00153275d5b73f20c8b77c26bd593c9799b5ae47caa6cd8670c0-2efddfc7cbf25b883ac4f0a4a51ce66009b15fc62554a5c766448bff2489c65b-extracted/adapter_config.json\n",
            "Adding adapter 'qqp'.\n",
            "Loading module weights from ~/.cache/torch/adapters/a76702bb6d5d00153275d5b73f20c8b77c26bd593c9799b5ae47caa6cd8670c0-2efddfc7cbf25b883ac4f0a4a51ce66009b15fc62554a5c766448bff2489c65b-extracted/pytorch_adapter.bin\n",
            "Some module weights could not be found in loaded weights file: bert.invertible_adapters.qqp.F.0.weight, bert.invertible_adapters.qqp.F.0.bias, bert.invertible_adapters.qqp.F.2.weight, bert.invertible_adapters.qqp.F.2.bias, bert.invertible_adapters.qqp.G.0.weight, bert.invertible_adapters.qqp.G.0.bias, bert.invertible_adapters.qqp.G.2.weight, bert.invertible_adapters.qqp.G.2.bias\n",
            "No exactly matching adapter config found for this specifier, falling back to default.\n",
            "Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_task/qnli/bert-base-uncased/pfeiffer/bert-base-uncased_nli_qnli_pfeiffer.zip.\n",
            "Loading module configuration from ~/.cache/torch/adapters/8de3e0bdeb5e9027edfe5b3ab43eed653e87d73608f46c7bf3721edff9fb53f7-e1bfa5ca96af6e7cba82511a23064076fe6d4a9b92f77d708fad101eb7b00f00-extracted/adapter_config.json\n",
            "Adding adapter 'qnli'.\n",
            "Loading module weights from ~/.cache/torch/adapters/8de3e0bdeb5e9027edfe5b3ab43eed653e87d73608f46c7bf3721edff9fb53f7-e1bfa5ca96af6e7cba82511a23064076fe6d4a9b92f77d708fad101eb7b00f00-extracted/pytorch_adapter.bin\n",
            "Some module weights could not be found in loaded weights file: bert.invertible_adapters.qnli.F.0.weight, bert.invertible_adapters.qnli.F.0.bias, bert.invertible_adapters.qnli.F.2.weight, bert.invertible_adapters.qnli.F.2.bias, bert.invertible_adapters.qnli.G.0.weight, bert.invertible_adapters.qnli.G.0.bias, bert.invertible_adapters.qnli.G.2.weight, bert.invertible_adapters.qnli.G.2.bias\n",
            "Could not identify 'Fuse[multinli, qqp, qnli]' as a valid prediction head.\n",
            "Adding head 'cb' with config {'head_type': 'classification', 'num_labels': 3, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2}, 'use_pooler': False, 'bias': True}.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60qIas8-il92"
      },
      "source": [
        "The last preparation step is to define and activate our adapter setup. Similar to `train_adapter()`, `train_adapter_fusion()` does two things: It freezes all weights of the model (including adapters!) except for the fusion layer and classification head. It also activates the given adapter setup to be used in very forward pass.\n",
        "\n",
        "The syntax for the adapter setup (which is also applied to other methods such as `set_active_adapters()`) works as follows:\n",
        "\n",
        "- a single string is interpreted as a single adapter\n",
        "- a list of strings is interpreted as a __stack__ of adapters\n",
        "- a _nested_ list of strings is interpreted as a __fusion__ of adapters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgGqHJQbijgg"
      },
      "source": [
        "# Unfreeze and activate fusion setup\n",
        "adapter_setup = Fuse(\"multinli\", \"qqp\", \"qnli\")\n",
        "model.train_adapter_fusion(adapter_setup)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fb8BY5RAmzkd"
      },
      "source": [
        "For training, we make use of the `Trainer` class built-in into `transformers`. We configure the training process using a `TrainingArguments` object and define a method that will calculate the evaluation accuracy in the end. We pass both, together with the training and validation split of our dataset, to the trainer instance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0gFxQRdDkQ6",
        "outputId": "feff950e-b647-44e2-de89-4a382984f62b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "from transformers import TrainingArguments, Trainer, EvalPrediction\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    learning_rate=5e-5,\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    logging_steps=200,\n",
        "    output_dir=\"./training_output\",\n",
        "    overwrite_output_dir=True,\n",
        "    # The next line is important to ensure the dataset labels are properly passed to the model\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "def compute_accuracy(p: EvalPrediction):\n",
        "  preds = np.argmax(p.predictions, axis=1)\n",
        "  return {\"acc\": (preds == p.label_ids).mean()}\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"validation\"],\n",
        "    compute_metrics=compute_accuracy,\n",
        ")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKlYjA9rm2Kp"
      },
      "source": [
        "Start the training üöÄ (this will take a while)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "vSUs2FjXDmsx",
        "outputId": "2e612b38-69ad-4141-c8dd-ccf253616fce"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 250\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 40\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [40/40 00:51, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=40, training_loss=0.7984646320343017, metrics={'train_runtime': 52.8561, 'train_samples_per_second': 23.649, 'train_steps_per_second': 0.757, 'total_flos': 180914605650000.0, 'train_loss': 0.7984646320343017, 'epoch': 5.0})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axvDsmnJnGUG"
      },
      "source": [
        "After completed training, let's check how well our setup performs on the validation set of our target dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "id": "PqcApZ_-DpwK",
        "outputId": "421984a6-8493-4f10-8c0b-de8c583def6c"
      },
      "source": [
        "trainer.evaluate()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 56\n",
            "  Batch size = 32\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'epoch': 5.0,\n",
              " 'eval_acc': 0.7321428571428571,\n",
              " 'eval_loss': 0.7299303412437439,\n",
              " 'eval_runtime': 1.0846,\n",
              " 'eval_samples_per_second': 51.631,\n",
              " 'eval_steps_per_second': 1.844}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "op8M7AfYnWhs"
      },
      "source": [
        "We can also use our setup to make some predictions (the example is from the test set of CB):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "e9wQcaNLNPAT",
        "outputId": "49227cea-fe81-4626-81c9-8bd3b7937282"
      },
      "source": [
        "import torch\n",
        "\n",
        "def predict(premise, hypothesis):\n",
        "  encoded = tokenizer(premise, hypothesis, return_tensors=\"pt\")\n",
        "  if torch.cuda.is_available():\n",
        "    encoded.to(\"cuda\")\n",
        "  logits = model(**encoded)[0]\n",
        "  pred_class = torch.argmax(logits).item()\n",
        "  return id2label[pred_class]\n",
        "\n",
        "predict(\"\"\"\n",
        "``It doesn't happen very often.'' Karen went home\n",
        "happy at the end of the day. She didn't think that\n",
        "the work was difficult.\n",
        "\"\"\",\n",
        "\"the work was difficult\"\n",
        ")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'contradiction'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-fJK8OtnrAu"
      },
      "source": [
        "Finally, we can extract and save our fusion layer as well as all the adapters we used for training. Both can later be reloaded into the pre-trained model again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpHIRIHxJd5d",
        "outputId": "11eb5cde-ee9c-4562-8e57-fcebda2995f6"
      },
      "source": [
        "model.save_adapter_fusion(path +\"models/fusion/\", \"multinli,qqp,qnli\")\n",
        "model.save_all_adapters(path +\"models/fusion/\", \"multinli,qqp,qnli\")\n",
        "\n",
        "print(model.active_adapters)\n",
        "\n",
        "\n",
        "!ls -l saved"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Configuration saved in /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/fusion/adapter_fusion_config.json\n",
            "Module weights saved in /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/fusion/pytorch_model_adapter_fusion.bin\n",
            "Configuration saved in /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/fusion/multinli/adapter_config.json\n",
            "Module weights saved in /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/fusion/multinli/pytorch_adapter.bin\n",
            "Configuration saved in /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/fusion/qqp/adapter_config.json\n",
            "Module weights saved in /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/fusion/qqp/pytorch_adapter.bin\n",
            "Configuration saved in /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/fusion/qnli/adapter_config.json\n",
            "Module weights saved in /content/gdrive/MyDrive/master_hpi/NLP_Project/code/models/fusion/qnli/pytorch_adapter.bin\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fuse[multinli, qqp, qnli]\n",
            "ls: cannot access 'saved': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PHWpVLApDWt"
      },
      "source": [
        "That's it. Do check out [the paper on AdapterFusion](https://arxiv.org/pdf/2005.00247.pdf) for a more theoretical view on what we've just seen.\n",
        "\n",
        "‚û°Ô∏è `adapter-transformers` also enables other composition methods beyond AdapterFusion. For example, check out [the next notebook in this series](https://colab.research.google.com/github/Adapter-Hub/adapter-transformers/blob/master/notebooks/04_Cross_Lingual_Transfer.ipynb) on cross-lingual transfer."
      ]
    }
  ]
}